,article,class
0,"NLP is a multidisciplinary field that draws from linguistics and computer science, particularly artificial intelligence ,.  In terms of linguistics, a program must be able to deal with words that have multiple meanings (“wind up the clock” and “the wind is cold today”) as well as grammatical ambiguities (in the phrase “little girl’s school” is it the school that is little, the girls, or both?).  Of course each language has its own forms of ambiguity. Programs can use several strategies for dealing with these problems, including using statistical models to predict the likely meaning of a given phrase based on a “corpus” of existing text in that language ,. As formidable as the task of extracting the correct (literal) meaning from text can be, it is really only the first level of natural language processing.  If a program is to successfully summarize or draw conclusions about a news report from North Korea, for example, it would also have to have a knowledge base of facts about that country and/or a set of “frames” , about how to interpret various situations such as threat, bluff, or compromise. )",0
1,"There are a variety of emerging applications for NLP, including the following:, voice-controlled computer interfaces (such as in aircraft cockpits), programs that can assist with planning or other tasks ,, more-realistic interactions with computer-controlled game characters, robots that interact with humans in various settings such as hospitals, automatic analysis or summarization of news stories and other text, intelligence and surveillance applications (analysis of communication, etc. ), data mining, creating consumer profiles, and other ecommerce applications, search-engine improvements, such as in determining relevancy",0
2,"As each new means of communication and social interaction is introduced, social customs and etiquette evolve in response.  For example, it took time before the practice of saying “hello” and identifying oneself became the universal way to initiate a phone conversation. By the 1980s, a system of topical news postings , carried on the Internet was becoming widely used in universities, the computer industry, and scientific institutions.  Many new users did not understand the system, and posted messages that were off topic.  Others used their postings as to insult or attack (“flame”) other users, particularly in newsgroups discussing perennially controversial topics such as abortion.  When a significant number of postings in a newsgroup are devoted to flaming and counter-flaming, many users who had sought civilized, intelligent discussion leave in protest. In 1984, Chuq von Rospach wrote a document entitled “A Primer on How to Work with the Usenet Community. ” It and later guides to net etiquette or “netiquette” offered useful guidelines to new users and to more experienced users who wanted to facilitate civil discourse. ",0
3,"These suggestions include:, Learn about the purpose of a newsgroup before you post to it.  If a group is moderated, understand the moderator’s guidelines so your postings won’t be rejected. , Before posting, follow some discussions to see what sort of language, tone, and attitude seems to be appropriate for this group Do not post bulky graphics or other attachments unless the group is designed for them. , Avoid “ad hominem” (to the person) attacks when discussing disagreements. , Do not post in ALL CAPS, which is interpreted as “shouting. ”, Check your postings for proper spelling and grammar.  On the other hand, avoid “flaming” other users for their spelling or grammar errors. , When replying to an existing message, include enough of the original message to provide context for your reply, but no more. , If you know the answer to a question or problem raised by another user, send it to that user by e-mail.  That way the newsgroup doesn’t get cluttered up with dozens of versions of the same information. ",0
4,"In recent years there has been growing concern that Internet users may eventually be treated differently by service providers depending on the kind of data they download or the kind of application programs they use online.  Advocates of network (or net) neutrality , want legislation that would bar cable, DSL, or other providers , from making such distinctions, such as by charging content providers higher fees for high volumes of data or even blocking certain applications.  Advocates of net neutrality believe that, since there are rather limited choices for broadband Internet service, discrimination on the basis of Web content could lead to a loss of freedom for consumers and providers alike",0
5,"By the late 1970s, researchers at many major universities were using the UNIX operating system ,.  In 1979, a suite of utilities called UUCP was distributed with the widely used UNIX Version 7.  These utilities could be used to transfer files between UNIX computers that were linked by some form of telephone or network connection. Two Duke University graduate students, Tom Truscott and Jim Ellis, decided to set up a way in which users on different computers could share a collection of files containing text messages on various topics.  They wrote a simple set of shell scripts that could be used for distributing and viewing these message files.  The first version of the news network linked computers at Duke and at the University of North Carolina.  Soon these programs were revised and rewritten in the C language and distributed to other UNIX users as the “A” release of the News software. During the 1980s, the news system was expanded and features such as moderated newsgroups were added.  As the Internet and its TCP/IP protocol , became a more widespread standard for connecting computers, a version of News using the NNTP (Network News Transmission Protocol) over the Internet was released in 1986.  Netnews is a mature system today, with news reading software available for virtually every type of computer",0
6,"Netnews postings are simply text files that begin with a set of standard headers, similar to those used in e-mail.  (Like e-mail, news postings can have binary graphics or program files attached, using a standard called MIME, for Multipurpose Internet Mail Extensions. )The files are stored on news servers—machines that have the spare capacity to handle the hundreds of gigabytes of messages now posted each week.  The files are stored in a typical hierarchical UNIX fashion, grouped into approximately 75,000 different newsgroups. As shown in the following table, the newsgroups are broken down into 10 major categories.  The names of individual groups begin with the major category and then specify subdivisions.  For example, the newsgroup comp. sys. ibm. pc deals with IBM PC-compatible personal computers, while comp. os. linux deals with the Linux operating system. ",0
7,"The servers are linked into a branching distribution system.  Messages being posted by users are forwarded to the nearest major regional “node” site, which in turn distributes them to other major nodes.  In turn, when messages arrive at a major node from another region, they are distributed to all the smaller sites that share the newsfeed.  Due to the volume of groups and messages, many sites now choose to receive only a subset of the total newsfeed.  Sites also determine when messages will expire (and thus be removed from the site). There are dozens of different news reading programs that can be used to view the available newsgroups and postings.  On UNIX systems, programs such as elm and tin are popular, while other newsreaders cater to Windows, Macintosh, and other systems.  Major Web browsers such as Netscape and Internet Explorer offer simplified news reading features.  To use these news readers, the user accesses a newsfeed at an address provided by the Internet Service Provider (ISP).  There are also services that let users simply navigate through the news system by following the links on a Web page.  The former service called DejaNews, now Google Groups, is the best-known and most complete such site",0
8,"In the 1940s, the main objective in developing the first digital computers was to speed up the process of calculation.  In the 1950s, the machines began to be used for more general data-processing tasks by governments and business.  By the 1960s, computers were in use in most major academic, government, and business organizations.  The desire for users to share data and to communicate both within and outside their organization led to efforts to link computers together into networks. Computer manufacturers began to develop proprietary networking software to link their computers, but they were limited to a particular kind of computer, such as a DEC PDP minicomputer, or an IBM mainframe.  However, the U. S.  Defense Department, seeing the need for a robust, decentralized network that could maintain links between their computers under wartime conditions, funded the development of a protocol that, given appropriate hardware to bridge the gap, could link these disparate networks ,. ",0
9,"According to the OSI (open systems interconnection) model, a network can be considered to be a series of seven layers laid one atop another ,. The physical layer is at the bottom.  It specifies the physical connections between the computers, which can be anything from ordinary phone lines to cable, fiber optic, or wireless.  This layer specifies the required electrical characteristics (such as voltage changes and durations that constitute the physical signal that is recognized as either a 1 or 0 in the “bit stream. ”The next layer, called the data link layer, specifies how data will be grouped into chunks of bits (frames or packets) and how transmission errors will be dealt with ,. The network layer groups the data frames as parts of a properly formed data packet and routes that packet from the sending node to the specified destination node.  A variety of routing algorithms can be used to determine the most efficient route given current traffic or line conditions. The transport layer views the packets as part of a complete transmission of an object (such as a Web page) and ensures that all the packets belonging to that object are sorted into their original sequence at the destination.  This is necessary because packets belonging to the same message may be sent via different routes in keeping with traffic or line conditions. The session layer provides application programs communicating over the network with the ability to initiate, terminate, or restart an interrupted data transfer. The presentation layer ensures that data formats are consistent so that all applications know what to expect.  This layer can also provide special services ,. Finally, the application layer gives applications highlevel commands for performing tasks over the network, such as file transfer protocol (ftp)",0
10,"A network attached storage (NAS) unit can be thought of as a dedicated data storage unit that is available to all users of a network.  Unlike a traditional dedicated file storage unit ,, a NAS unit typically has an operating system and software designed specifically (and only) for providing data storage services.  The actual storage is usually provided by an array of hard drives ,.  Files on the NAS are accessed through protocols such as SMB (server message block), common on Windows networks, and NFS (network file system), used on many UNIX and some Linux networks.  In recent years smaller, lower-cost NAS devices have become available for smaller networks, including home networks, where they can store music, video, and other files ,",0
11,"Although it sounds similar, a storage area network (SAN) does not function as its own file server.  Rather, it attaches storage modules such as hard drives or tape libraries to an existing server so that it appears to the server’s operating system as though it were locally attached.  Typically the protocol used to attach the storage is SCSI ,, but the physical connection is fiber or high-speed Ethernet.  The emphasis for SAN applications is the need for fast access to data, such as in large online databases, e-mail servers, and high-volume file servers.  SANs offer great flexibility, since storage can be expanded without changing the network structure, and a replacement server can quickly be attached to the storage in case of hardware failure",0
12,"In the kind of science fiction sometimes called “cyberpunk,”people are able to “jack in” or connect their brains directly tocomputer networks.  Because of this direct input into the brain(or perhaps the optic and other sensory nerves), a personwho is jacked in experiences the virtual world as fully real,and can (depending on the world’s rules) manipulate it withhis or her mind.  This kind of all-immersive virtual reality isstill science fiction, but today people are beginning to controlcomputers and artificial limbs directly with their minds. ",0
13,"Neuroprosthetics is the creation of artificial limbs or sensoryorgans that are directly connected to the nervous system. The first (and most widely used) example is the cochlearimplant, which can restore hearing by taking sound signals from a microphone and converting them to electricalimpulses that directly stimulate auditory nerves within thecochlea, a part of the inner ear.  Similarly, experimental retinal implants that stimulate optic nerves are beginning tooffer crude but useful vision to certain blind patients. Research in connecting the brain to artificial arms orlegs is still in its early stages, but scientists using microelectrode arrays have been able to record signals from thebrain’s neurons and correlate them to different types ofmotor movements.  In a series of experiments at Duke University, researchers first trained a monkey to operate a joystick to move a shape in a video game.  They then recordedand analyzed the signals produced by the monkey’s brainwhile playing the game, and correlated them with themotor movements in the joystick.  Next, they replicatedthese movements with a robotic arm as the monkey movedthe joystick.  Finally, they were able to train the monkey tomove the robotic arm without using the joystick at all, simply by “thinking” about the movements. ",0
14,"As more is learned about the detailed functioning of neuronal networks inside the brain, “cognitive prosthetics” maybecome feasible.  One example might be computer memory modules that might act as a surrogate or extension ofhuman memory, perhaps helping compensate for loss ofmemory due to age or disease.  (Early experiments on interfacing to the hippocampus, a part of the brain important forforming memories, have been underway since 2003. )Other possibilities might include processors that couldgive a person the ability to think about a mathematicalproblem and “see” the answer, or to search databases or theWeb simply by visualizing or thinking about the information desired",0
15,"Integers (whole numbers) have the simplest representation, but there are two important considerations: the totalnumber of bits available and whether one bit is used to holdthe sign. Since all numbers are stored as binary digits, an unsignedinteger has a range from 0 to 2bits where “bits” is the totalnumber of bits available.  Thus if there are 16 bits available, the maximum value for an integer is 65535.  If negativenumbers are to be handled, a signed integer must be used(in most languages such as C, C++, and Java, an integer issigned unless unsigned is specified).  Since one bit is usedto hold the sign and each bit doubles the maximum size, itfollows that a signed integer can have only half the rangeabove or below zero.  Thus, a 16-bit signed integer can rangefrom -32,768 to 32,767. One complication is that the available sizes of integersdepend on whether the computer system’s native data sizeis 16, 32, or 64 bits.  In most cases the native size is 32 bits,so the declaration “int” in a C program on such a machineimplies a signed 32-bit integer that can range from - 231 or-2,147,483,647 to 231-1, or 2,147,483,647.  However, if one isusing large numbers in a program, it is important to checkthat the chosen type is large enough.  The long specifier isoften used to indicate an integer twice the normal size, or64 bits in this case. ",0
16,"Numbers with a fractional (decimal) part are usually storedin a format called floating point.  The “floating” means thatthe location of the decimal point can be moved as necessaryto fit the number within the specified digit range.  A floatingpoint number is actually stored in four separate parts.  Firstcomes the sign, indicating whether the number is negativeor positive.  Next comes the mantissa, which contains theactual digits of the number, both before and after the decimal point.  The radix is the “base” for the number systemused.  Finally, the exponent determines where the decimalpoint will be placed. For example, the base 10 number 247. 35 could be represented as 24735 × 10-2.  The -2 moves the decimal pointat the end two places to the left.  However, floating-pointnumbers are normalized to a form in which there is just onedigit to the left of the decimal point.  Thus, 247. 35 wouldactually be written 2. 4735 × 102.  This system is also knownas scientific notation. As noted earlier, actual data storage in modern computers is always in binary, but the same principle applies. According to IEEE Standard 754, 32-bit floating-point numbers use 1 bit for the sign, 8 bits for the exponent, and 23bits for the mantissa (also called the significand, since itexpressed the digits that are significant—that is, guaranteed not to be “lost” through overflow or underflow in processing).  The double precision float, declared as a “double”in C programs, uses 1, 11, and 52 bits respectively. ",0
17,"Many objects are more elaborate or specialized variations ofmore basic objects.  For example, in Microsoft Windows thevarious kinds of dialog boxes are specialized versions of thegeneral Window class.  Therefore, the specialized versionis created by declaring it to be derived from a “base class. ”Put another way, the specialized class inherits the basicdata and functions available in the base (parent) class.  Theprogrammer can then add new data or functions or modifythe inherited ones to create the necessary behavior for thespecialized class. Languages such as C++ allow for a class to be derivedfrom more than one base class.  This is called multipleinheritance.  For example, a Message Window class mightinherit its overall structure from the Window class andits text-display capabilities from the Message class.  However, it can sometimes be difficult to keep the relationshipsbetween multiple classes clear.  The Java language takes thealternative approach of being limited to only single inheritance of classes, but allowing interfaces (specifications ofhow a class interacts with the program) to be multiplyinherited. ",0
18,"Different kinds of objects often have analogous methods. For example, suppose there is a series of classes that represent various polygons: square, triangle, hexagon, and so forth.  Each class has a method called “perimeter” thatreturns the total distance around the edges of the object.  Ifeach of these classes is derived from a base polygon class,each class inherits the base class’s perimeter method andadapts it for its own use.  Thus, a square might calculateits perimeter simply by multiplying the length of a side byfour, while the rectangle would have to add up differentsized pairs of sides, and so on. ",0
19,"In the late 1990s “banner ads” started to appear on Websites, and other forms of advertising soon followed.  Companies rushed into the online world, either with the belief thatit had unlimited potential for finding new customers, or outof fear that the competition would get there first.  Unfortunately it was hard to measure the actual effectiveness ofads, and Web sites (such as for publications) that lookedto third-party advertising as a source of income found theoutlook bleak in the wake of the bursting of the “dot-combubble” of the early 2000 decade. Only a few years later, however, advertisers using newbusiness models and targeting techniques have made onlineadvertising not only a viable business, but a rapidly growing one.  (According to the Interactive Advertsing Bureau,Internet advertising revenue in the United States in 2007was $21. 2 billion, up 26 percent from 2005. )The effects of the online advertising revolution are rippling outward, impacting traditional advertising mediasuch as newspapers (in particular see craigslist), magazines, and even television. ",0
20,"Types of ads include the following:, Banner ads are contained in rectangles, often at the topof the Web page.  (Sometimes they can mimic dialogboxes from the operating system. ) They still accountfor about half of all online advertising, and can appearon sites of all types. , Pop-up or pop-under ads appear above or beneath thecurrent window, respectivelyFloating ads appear over the main page content, oftenmoving across the screen. , Interstitial ads are displayed before the requestedcontent (such as an article or video) is shown.  Theyrun for a specified period of time, although they cansometimes be closed by the viewer. Many ads are animated; some even contain video clips. There are also ads formatted for mobile devices, includingtext messages sent to cell phones. ",0
21,"Google and other large search enginesor portals can make money from advertisers is through“affiliate marketing”; Google’s version is called Ad Sense. Participating Web sites are indexed, and the resulting keywords are matched with ads awaiting placement.  The sitecarrying the ad generally gets a per-click payment.  However, the problem of “click fraud” has also arisen: Scammerscan set up an affiliate site and then use special software togenerate the clicks, while making them come from a varietyof sources.  Despite these problems, in 2006 about 40 percent of revenue from online advertising was attributed tosearch-related ads. ",0
22,"The dark side of online advertising is found in programsthat are surreptitiously installed on users’ PCs and thendownload and display advertising from shady Web operations ,.  While many users nowregularly run programs to block such malware, even legitimate online advertising can irritate users, particularlywhen ads are too prominent, float over (and block) text, orlurk behind the browser window.  Modern Web browsershave ad-blocking features that work with varying degreesof effectiveness.  As with TV, online advertisers increasinglyhave to cope with impatient users who do not have to lookat ads unless they actually want to. Advertisers can employ several strategies to keep userswilling to look at ads.  One is to make the ad unobtrusiveand brief, and on the way to something the user reallywants to see.  In 2007 YouTube began such advertising. Another is to provide free versions of software or servicesthat, in exchange for being free, require the user to put upwith some screen real estate being devoted to ads.  Finally,as with TV, advertising can be woven into the content itself,such as in online computer games. A sensitive area is the attempt to balance advertisers’desire to know as much as possible about consumers’ interestsand buying habits with the same consumers’ concern aboutprotecting their privacy",0
23,"A variety of other frauds and scams appear online or viae-mail with some frequency:, the “419” or “Nigerian money letter” that promises arich cut for helping facilitate a money transfer for adistressed official, fraudulent charitable solicitations, particularly aftersuch disasters as the Asian tsunami or HurricaneKatrina, adoption and marriage scams, educational fraud, such as worthless degrees offeredby unaccredited institutions, dubious employment schemes or “home businesses”involving preparing mailings or medical billing, services that offer to “repair” bad credit ratings, tax-avoidance schemes, often based on nonexistentlegal claims or loopholes",0
24,"Online casinos appeared in 1995, but at first they couldonly be played “for fun,” with no actual money changinghands.  That soon changed: In 1996, InterCasino appeared—it would be the first of hundreds of online casinos, sportsbookmakers, and other types of gambling.  Generally theseoperations are based outside of the United States—Caribbean islands such as Antigua and Curaçao are popular locations. Online casinos offer traditional table games such asblackjack, roulette, and craps.  Generally odds and payoffsare comparable to those at traditional casinos.  Assumingthe game is honest and properly programmed, the house’srevenue comes from a percentage of the amount bet—blackjack having the lowest house percentage and roulette thegreatest.  Slot machines (which give an even higher percentage to the house) can also be simulated online. ",0
25,"Online poker has become very popular, particularly gamessuch as Texas Hold’Em.  Estimated revenues from onlinepoker in the United States were $2. 4 billion in 2005. Unlike the case with casino games, online poker playersplay against each other, not the house.  The house’s revenuecomes from a “rake,” or percentage, of the pot.  Many sitesoffer organized tournaments, and some online players havegone on to win traditional tournaments.  (The aptly namedChris Moneymaker won an online tournament, qualifyinghim to enter the 2004 World Series of Poker, which he wenton to win. )Like online casinos, online poker is illegal in the UnitedStates.  Proponents argue that while any given hand is random, poker in the long run is a game of skill, not chance.  Agroup called the Poker Players Alliance has been lobbyingto exempt poker from Internet gambling laws. A third type of online gambling is sports betting, whichis legal in many countries but only in Nevada in the UnitedStates.  The Web has also given sports bettors a forum fordiscussing (or arguing about) teams and their prospects. ",0
26,"Online games today range from elaborate war games toopen-ended fantasy worlds to virtual universes that mirror“real-world” activities, including economics, politics, andeven education. The first online games appeared in the late 1970s onPLATO, an educational network, as well as on the earlyInternet of the 1980s.  These MUDs (multiuser dungeons)were generally based on pen-and-paper role-playing gamesof the time, notably Dungeons & Dragons.  These games were text based, with players typing their characters’ actionsand dialog while the changing world as seen by the players was similarly described.  By the early 1990s, however,MUDs had spun off many variants.  Many were still “hackn’ slash” dungeon games (which were also offered on America Online and other commercial services).  Many of theseMUD-like games such as AOL’s Neverwinter Nights offeredsimple graphics.  Meanwhile other games began to offermore sophisticated social interactions as well as the abilityof players to make their own additions to the game world,including buildings. ",0
27,"Today’s online games feature a “persistent world” hosted onone or more servers that grows and develops from day to dayand in which the “avatars” or representatives of thousandsof players interact with game-generated creatures or oneanother, using client software.  Players can spend hundredsof hours helping their characters develop skills, increasingtheir levels through experience points gained from successful combat or other activities.  Players (and their characters) frequently form organizations such as guilds or clans,because the tougher challenges generally require the cooperation of different types of classes of characters (fighters,healers, and magic-users). Modern MM ORPGs began in the late 1990s with suchtitles as Ultima Online and EverQuest.  The most popularMMORPG in the mid-2000s was World of Warcraft. ",0
28,"As with shoppers, investors have increasingly been attractedto the interactivity and ease of online transactions.  In addition to allowing stocks to be bought or sold with just a fewclicks, online brokers (also called discount brokers) chargemuch lower transaction fees than their traditional counterparts, typically less than $10 per trade. Some online brokers, such as E*Trade, Scottrade, andTD Ameritrade, were established as Internet brokers.  However, traditional brokerages such as Charles Schwab andWaterhouse have also opened online discount brokerages. In addition to fast, inexpensive trading, many onlinebrokers also offer a variety of resources and tools, including stock quotes and charts, research reports, and screening programs to help investors pick the mutual funds orindividual investments that meet their objectives.  For moresophisticated investors, some brokers offer simulations fortesting investment strategies and programmed trading,which will execute buy or sell orders automatically depending on specified conditions. Online brokers can specialize, seeking customers whowant to make frequent trades but do not need other support, or investors who are interested in obtaining IPOs (initial public offerings) of up-and-coming companies.  Somebrokers may emphasize mutual funds and cater to retirement accounts, while others might offer government or corporate bonds, foreign stocks, “penny stocks,” or more exoticinvestments. The interactivity and low transaction costs in onlineinvesting may encourage people to become involved inhighly speculative penny stocks, options, day trading, foreign exchange markets, and other areas that are not suitablefor most individual investors.  While there is a great dealof useful information available online, it is a good idea tobegin by discussing investment goals and potential riskswith a trusted financial adviser. ",0
29,"In evaluating a job site it is important to get a feel forthe kinds of jobs offered and the target audience, such asprofessionals, recent graduates, white-collar or service sector jobs, and so on.  Other important features to look forinclude:, powerful search or filtering capability, such as by typeof job or employer, keywords in job description, orlocality, the ability to put one’s resume online and edit orupdate it as needed. , the ability to have several versions of one’s resumetailored to different types of jobs, automatic e-mail alerts about newly added jobs thatmeet the user’s criteria, privacy protections so that contact information fromresumes is not used for marketing or other nonemployment purposes, lack of fees to job seekers (normally employers are theservice’s source of revenue)",0
30,"A persistent problem in artificial intelligence , is how to provide a software systemwith a model that it can use to reason about a particularsubject or domain.  A data model or ontology basically consists of classes to which the relevant objects might belong,relationships between classes, and attributes that objects inthat class can possess.  (For implementation of these ideaswithin programming languages, see classes and objectoriented programming. )For example, a business ontology might include classessuch as:, Entity—a business or person, Supplier—an Entity that provides wholesale goods orservices, Customer—an Entity that buys the company’s goodsor services, Contractor—an Entity that performs work for thecompany on contract",0
31,"Ontologies can be used to provide guidance to a variety oftypes of programs (for example, see expert system, natural language processing, and software agent).  Thusif an automatic news summarizer program encounters astory that includes references to opposing lawyers and legalissues, it could apply an ontology that defines the likelyrelationship of the participants in the case. Creating useful ontologies is quite labor intensive interms of the human thinking and coding involved.  However, there have been substantial efforts in recent years tocreate anthologies for many fields, particularly in biologyand genetics.  The Web Ontology Language (OWL) is apopular tool for creating ontologies that can be used tomake Web content more understandable to programs ,. Meanwhile, an ambitious and long-running projectcalled Cyc (for Encyclopedia) under the direction of Douglas Lenat has been engaged in creating what amounts tovast ontologies for many of the domains included in everyday human life as well as specialized fields of knowledge.  Alarge portion of this work has been made available as opensource. ",0
32,"According to Stallman and many other advocates, “opensource” software is not necessarily free.  What is requiredis that users receive the full source code (or have it readilyavailable for free or at nominal charge).  Users are free tomodify or expand the source code to create and distributenew versions of the software.  Following a legal mechanismthat Stallman calls “copyleft,” the distributor of opensource software must allow subsequent recipients the samefreedom to revise and redistribute.  However, not all software that is billed as open source follows all of Stallman’srequirements, including being copylefted.  Formally, opensource software is generally licensed according to variousversions of the General Public License (GPL).  The latestversion, GPL3, released in 2007, has been controversial. Among other things, it more aggressively attempts to prevent open-source software from being restricted or otherwise hampered by being combined with patented softwareor proprietary hardware. ",0
33,"An operating system is an overarching program that manages the resources of the computer.  It runs programs andprovides them with access to memory (RAM), input/outputdevices, a file system, and other services.  It provides application programmers with a way to invoke system services, andgives users a way to control programs and organize files. The “core” functions include “booting” the system and initializing devices, process management(loading programs intro memory assigning them a share ofprocessing time), and allowing processes to communicatewith the operating system or one another ,. Multiprogramming systems often implement not only processes (running programs) but also threads, or sections ofcode within programs that can be controlled separately. A memory management scheme is used to organize andaddress memory, handle requests to allocate memory, freeup memory no longer being used, and rearrange memory tomaximize the useful amount ,. There is also a scheme for organizing data created orused by programs into files of various types ,.  Mostoperating systems today have a hierarchical file system thatallows for files to be organized into directories or foldersthat can be further subdivided if necessary",0
34,"Designers of modern operating systems face a numberof continuing challenges:, security, in a world where nearly all computers arenetworked, often continuously ,, the tradeoff between powerful, attractive functionssuch as scripting and the security vulnerabilities theytend to present, the need to provide support for new applications suchas streaming audio and video ,, ease of use in installing new devices ,The continuing development of new user-interfaceconcepts, including alternative interfaces for the disabled and for special applications ,, the growing use of multiprocessing and multiprogramming, requiring coordination of processors sharingmemory and communicating with one another ,, distributed systems where server programs, clientprograms, and data objects can be allocated amongmany networked computers, and allocations continually adjusted or balanced to reflect demand on thesystem ,, the spread of portable, mobile, and handheld computers and computers embedded in devices such asengine control systems ,.  (Sometimes the choice isbetween devising a scaled-down version of an existing operating system and designing a new OS that isoptimized for devices that may have limited memoryand storage capacity. )",0
35,"Generally speaking, the levels of precedence for mostlanguages are as follows:1.  scope resolution operators (specify local v.  globalversions of a variable)2.  invoking a method from a class, array subscript,function call, increment or decrement3.  size of (gets number of bytes in an object), addressand pointer dereference, other unary operators(such as “not” and complement); creation and deallocation functions; type casts 4.  class member selection through a pointer5.  multiplication, division, and modulus6.  addition and subtraction7.  left and right shift operators8.  less than and greater than9.  equal and not equal operators10.  bitwise operators (AND, then exclusive OR, inclusive OR)11.  logical operators (AND, then OR)12.  assignment statements",0
36,"Today it is easy to optically scan text or graphics printed onpages and convert it into a graphical representation for storage in the computer ,.  However, a shape suchas a letter c doesn’t mean anything in particular as a graphic. Optical character recognition (OCR) is the process of identifying the letter or other document element that correspondsto a given part of the scanned image and converting it tothe appropriate character ,.  Ifthe process is successful, the result is a text document thatcan be manipulated in a word processor, database, or otherprogram that handles text.  Raymond Kurzweil (1948– )marketed the first commercially practicable general-purposeoptical character recognition system in 1978. ",0
37,"Once the document page has been scanned into an imageformat, there are various ways to identify the characters. One method is to use stored templates that indicate the pattern of pixels that should correspond to each character.  Generally, a threshold of similarity is defined so that an exactmatch is not necessary to classify a character: The templatemost similar to the character is chosen.  Some systems storea set of templates for each of the fonts most commonly foundin printed text.  (Recognizing cursive writing is a much morecomplex process: See handwriting recognition. )A more generalized method uses structural features(such as “all t’s have a single vertical line and a shortercrossbar line”) to classify characters.  To analyze a character,the different types of individual features are identified andthen compared to a set of rules to determine the charactercorresponding to that particular combination of features. Sometimes thresholds or “fuzzy logic” are used to decidethe probable identity of a character. ",0
38,"Jaguar, panther, tiger, and leopard—these and other namesof sleek big cats represent versions of Apple’s Macintoshoperating system, OS X (pronounced “OS 10”—see AppleCorporation and Macintosh).  Unlike the previous MacOS, OS X, while broadly maintaining Apple’s user interfacestyle ,, is based on a version of UNIXcalled OpenStep, developed by NeXT starting in the 1980s,.  OS X development began when Steve Jobsreturned as Apple CEO in 1997 ,and the company bought NeXT, acquiring the software.  Thefirst version, OS X 10. 0, or Cheetah, was released in 2001,but the system was not widely used until 10. 1 (Puma) wasreleased later the same year. At the core of OS X is a free and open-source version ofUNIX called “Darwin,” with a kernel called XNU.  On top ofthis Apple built a distinctive and subtly colorful user interface called Aqua and a new version of the Macintosh Finderfile and program management system",0
39,"Today OS X includes a variety of useful software packages—some free and some optional.  These include iLife (digitalmedia management), iWork (productivity), and Front Row(home media center).  OS X10. 5 also includes Time Machine,an automatic backup system that can restore files (includingdeleted files) as well as earlier system settings. For software developers, OS X provides an integrateddevelopment environment called “Xcode,” which workswith modified open-source compilers for major programming languages, including C, C++, and Java.  Further,because OS X is UNIX-based, many UNIX and Linux programs can be recompiled to run on it.  Since mid-2005 Apple(and OS X) have been transitioning from the earlier IBM/Motorola processors to Intel processors.  This transition waslargely complete by 2007, though OS X 10. 5 (Leopard) stillprovides support for applications written for the PowerPC. OS X has been well received by critics, and together withits bundled software has made the Macintosh a popularplatform for users who want a seamless computing experience, particularly with regard to graphics and media. ",0
40," With a parallel connection, however, the eight bitsof the byte are sent simultaneously, each along its own wire,so parallel ports are generally faster than serial ports.  Also,since the data is transmitted simultaneously, the protocolfor marking the beginning and end of each data byte is simpler.  On the other hand, parallel cables are more expensive(since they contain more wires) and are generally limitedto a length of 10 feet or so because of electrical interferencebetween the parallel wires. The original parallel interface for personal computerswas designed by Centronics, and a later version of this 36-pin connector remains popular today.  Later, IBM designed a25-pin version.  In addition to the wires carrying data, additional wires are used to carry control signals",0
41,"After many years of effort researchers have been able tocreate systems that can recognize particular human faces,.  On the other hand, any normalsix-month-old child can effortlessly recognize familiarfaces (such as parents).  The fundamental task of turningraw data (whether from senses, instruments, or computerfiles) into recognizable objects or drawing inferences iscalled pattern recognition.  Pattern recognition is at theheart of many areas of research and application in computing ,. Despite the challenge in getting machines to do what comes naturally for biological organisms, the potentialpayoffs are immense. A pattern-recognition system begins with data, whetherstored or real-time (such as from a robot’s camera).  The firsttask in turning potentially billions of bytes of data intomeaningful objects is to extract features from what is likelya high proportion of redundant or irrelevant data.  (Withvisual images, this often involves finding edges that defineshapes. ) The extracted features are then classified to determine what objects they might represent.  This can be doneby comparing structures to templates or previously classified data or by applying statistical analysis to determine thelikely correlation of the new data to existing patterns ,. Pattern recognition often includes learning algorithmsas well; indeed, the field is often considered to be a subtopicof machine learning.  For example, classification systemscan be refined by “training” them and reinforcing successful determinations ,. ",0
42,"The first stage in making computing available away fromthe office desk was the development of “portable” and thenlaptop computers in the 1980s ,. Laptops, however, are relatively heavy and bulky, and thusnot suitable for activities such as making notes at meetings or keeping track of appointments while on the go. The logical solution to that need was to develop a computer small enough to carry in a pocket or purse.  The firsthandheld computer to achieve widespread recognition wasApple’s Newton, which the company referred to as a “personal digital assistant. ” This term, usually abbreviated toPDA, became a generic category with the introduction ofthe Palm Pilot, which first appeared in 1996, followed bythe seemingly ubiquitous RIM Blackberry in 1999. ",0
43,"The PDF specifications are open source, so anyone canwrite software to create or read documents in the format. PDF includes three elements: a subset of the PostScript pagedescription language ,, a system for specifying and embedding common fonts (or referring to otherfonts), and a system for “packaging” the text and graphicsdescriptions into a file in compressed form.  Later versionsof the PDF specification also allow users to interact withthe document, such as by filling in fields in a form or adding annotations to the text.  PDF also includes support fortags , and descriptors that can be used with programs such as screen readers for the blind. PDF also includes support for encrypting documents sothey can only be read with a password, and for controllingwhether the document can be copied or printed, though thisdepends on the user’s software understanding and obeyingthe restrictions. Although creating and editing PDF documents originally required the relatively expensive Adobe Acrobat software, there are now a number of free or low-cost editorsand other PDF utilities for Windows, Mac OS X, and Linux/UNIX platforms. ",0
44,"The explosive growth of the World Wide Web has confronted programmers with the need to find ways to link databases and other existing resources to Web sites.  The specifications for such linkages are found in the Common Gateway Interface ,.  However, the early facilities for writing CGI scripts were awkward and often frustrating to use. Back in 1986, UNIX developer Larry Wall had created a language called Perl (Practical Extraction and Report Language).  There were already ways to write scripts for simple data processing , as well as a handy pattern-manipulation language ,.  However, Wall wanted to provide a greater variety of functions and techniques for finding, extracting, and formatting data.  Perl attracted a following within the UNIX community.  Since much Web development was being done on UNIX-based systems by the mid- and late-1990s, it was natural that many webmasters and applications programmers would turn to Perl to write their CGI scripts. As with many UNIX scripting languages, Perl’s syntax is broadly similar to C.  However, the philosophy behind C is to provide a sparse core language with most functionality being handled by standard or add-in program libraries.  Perl, on the other hand, starts with most of the functionality of UNIX utilities such as sed (stream editor), C shell, and awk, including the powerful regular expressions familiar to UNIX users.  The language also includes a “hash” data type (a collection of paired keys and values) that makes it easy for a program to maintain and check lists such as of Internet hosts and their IP addresses ,. ",0
45,"In today’s health care environment patients often have only a few minutes to ask their doctor important questions about their condition and possible treatments.  Patients often feel they have been left on their own when it comes to obtaining detailed information.  According to surveys by the Pew Internet & American Life Project, by the end of 2005 about 20 percent of Web users were reporting that the Internet “has greatly improved the way they get information about health care. ” Further, 7 million users had reported that Web sites had “played a crucial or important role in coping with a major illness. ”A variety of Web sites ranging from comprehensive and excellent to dubious (at best) offer health-related information.  In evaluating them, it is important to determine who sponsors the site and what is the source of the information provided.  The very extensive WebMD site, for example, is reviewed for accuracy by an independent panel of experts.  One of the foremost medical institutions, the Mayo Clinic, also has an authoritative site.  The site OrganizedWisdom. com offers a search engine that emphasizes information that has been reviewed by doctors for accuracy, while eliminating low-quality or duplicative results. Even if information is accurate, however, users may often lack the necessary background or context for interpreting it correctly.  Understanding the results of medical studies, for example, requires some knowledge of how studies are designed, the population used, and the statistical significance and applicability of the results.  As a practical matter, therefore, patients should not make any major decisions about diet, medication, or treatment options without consulting a medical professional.  Attempts at self-diagnosis can be particularly prob",0
46,"When modern digital computing emerged in the 1940s, it evolved from two roots: engineering (particularly electrical engineering) and mathematics.  The goals of the earliest computer designers were focused naturally enough on computing, although several early thinkers , had already begun to think of computers as symbol-processing and knowledgeretrieving machines, not just number crunchers. As computer scientists began to become more concerned about the structure of data and the modeling of real-world objects in computer languages ,, they began to wrestle with some areas long familiar to philosophers.  As data structure involved into knowledge representations, epistemology (the philosophical investigation of the meaning and accessibility of knowledge) became more relevant, particularly in developing systems for artificial intelligence and machine learning.  Also relevant is ontology (the nature and relationship of entities—see ontologies and data models), particularly with regard to the modern effort to encode relationships between items of knowledge into Web pages ,",0
47,"Just about anyone with an e-mail account has received messages purporting to be from a bank, a popular e-commerce site such as Amazon or eBay, or even a government agency.  Typically the message warns of a problem (such as a suspended account) and urges the recipient to click on a link in the message.  If the user does so, what appears to look like the actual site of the relevant institution is actually a “spoof,” or fake site.  If the user goes on to enter information such as account numbers or passwords in order to fix the “problem,” the information actually goes to the operator of the fake site, where it can be used for fraudulent purchases or even impersonation ,.  The bogus site can also attempt to download viruses, spyware, keyloggers, or other forms of “malware” to the unwitting user’s computerThis all-too-common scenario is called “phishing,” alluding to “fishing” for unwary users with various sorts of bait, with the f changed to ph in keeping with traditional hacker practice.  Phishing is similar to other techniques for manipulating people through deception, fear, or greed that hackers often refer to as “social engineering. ” Unlike oneon-one approaches, however, phishing relies on the ability to send large quantities of e-mail at virtually no cost ,, the availability of simple techniques for disguising both e-mail addresses and Web addresses (URLs), and the ease with which the appearance of a Web site can be convincingly replicated. Although e-mail is the most common “hook” for phishing, any form of communication, including text or instant messages, can be used.  Recently sites such as MySpace have become targets for automated phishing expeditions that changed links on pages to point to fraudulent sites ",0
48,"Wary users have a number of ways to reduce their chance of being “phished. ” Some signs of bogus messages include:, The message is addressed generically (“dear PayPal user”) or to the user’s e-mail address rather than the account name. , The text of the message contains spelling errors or poor grammar. , The URL shown for a link in the message (perhaps via a “tool tip”) does not match the institution’s real Web address. Fortunately there are ways in which software can help detect and block most phishing attempts.  A good spam filter is the first line of defense and can block many phishing messages from getting to the user in the first place.  Anti-phishing features are also increasingly included in Web browsers, or available as plug-ins.  Thus “blacklists” of known phishing sites can be checked in real time and warnings given, or the site’s address can be blocked from access by the system.  Web sites can also introduce an added layer of security: Bank of America, for example, asks users to select and label one of several images offered by the bank.  The image and label are subsequent displayed as part of the log-in process.  If the user does not see the image and the user’s label, then the site is presumably not the real bank site",0
49,"The PHP processor parses only the code within the delimiters <? and ?>.  (An alternative set of delimiters is <script language =‘php’> </script>. Besides being embedded in HTML pages, PHP can be used interactively at the command line, where it has replaced older languages such as awk, Perl, or shell scripting for many users.  PHP can also be linked to user-interface libraries (such as GTK+ for Linux/UNIX) to create applications that run on the client machine rather than the server. PHP has a basic set of data types plus one called “resource” that represents data processed by special functions that return images, text files, database records, and so on.  Additionally, PHP5 provides full support for objects, including private and protected member variables, constructors and destructors, and other features similar to those found in C++ and other languages. There are numerous libraries of open-source objects and functions that enable PHP scripts to perform common Internet tasks, including accessing database servers (such as MySQL) as well as extensions to the language to handle popular Web formats such as Adobe Flash animation.  Programmers have access to a wide range of PHP resources through PEAR (the PHP Extension and Application Repository). The combination of sophisticated features and easy interactive scripting has made PHP the language of choice for many Web developers, who use it as part of the group of technologies called LAMP, for Linux, Apache (Web server), MySQL (database), and PHP. ",0
50,"Because of its many practical features and its availability for the popular IBM 360 mainframes, PL/I enjoyed considerable success in the late 1960s and 1970s.  The language was later ported to most major platforms and operating systems.  When personal computers came along, PL/I became available for IBM’s OS/2 operating system as well as for Microsoft’s DOS and Windows, although the language never really caught on in those environments. Computer scientists such as structured programming guru Edsger Dijkstra decried PL/I’s lack of a clear, welldefined structure.  In his Turing Award Lecture in 1972, Dijkstra opined that “I absolutely fail to see how we can keep our growing programs firmly within our intellectual grip when by its sheer baroqueness the programming language—our basic tool, mind you!—already escapes our intellectual control. ” (See Dijkstra, Edsger. )On a practical level the sheer number of features in the language meant that truly mastering it was a lengthy process.  A language like C, on the other hand, had a much simpler “core” to master even though it was less versatile.  PL/I also tended to retain the mainframe associations from its birth at IBM, while C grew up in the world of minicomputers and the UNIX community and proved more suitable for PCs.  Nevertheless, PL/I provided many examples that language designers could use in attempting to design better implementations. ",0
51,"By the mid-1990s, Intel was promoting a standard for the automated detection and configuration of devices.  Known as Plug and Play (PnP), this standard was incorporated in versions of Microsoft Windows starting with Windows 95 ,.  The required hardware support soon appeared on PC motherboards and expansion cards. With Plug and Play the user simply connects a printer, scanner, or other device to the PC.  Windows detects that a device has been connected and queries it for its official name and other information.  If necessary, Windows can then prompt the user for a disk containing the appropriate driver or even search for a driver on a Web site. The concept of Plug and Play extends beyond the Windows world, however.  In recent years there has been interest in developing a Universal Plug and Play (UPnP) protocol by which a variety of devices could automatically configure themselves with any of a variety of different networks.  This would be particularly helpful for home users who are increasingly setting up small networks so they can share broadband Internet connections, as well as the growing number of users who want their desktop PC to work with handheld (palm) computers and other devices.  Microsoft supports UPnP in versions of Windows starting with ME and XP. ",0
52,"A number of applications programs include the ability for third-party developers to write small programs that extend the main program’s functionality.  For example, thousands of “filters” (algorithms for transforming images) have been written for Adobe Photoshop.  These small programs are called plug-ins because they are designed to connect to the main program and provide their service whenever it is desired or required. Perhaps the most commonly encountered plug-ins are those available for Web browsers such as Firefox, Netscape, or Internet Explorer.  Plug-ins can enable the browser to display new types of files (such as multimedia).  Many standard programs for particular kinds of files are now provided both as stand-alone applications and as browser plug-ins.  Examples include Adobe (PDF document format), Apple QuickTime (graphics, video, and animation), RealPlayer (streaming video and audio), and Macromedia Flash (interactive animation and presentation).  These and many other plug-ins are offered free for the downloading, in order to increase the number of potential users for the formats and thus the market for the development packages. One of the most useful plug-ins found in most browsers is one that allows the browser to run Java applets ,.  In turn, Java is often used to write other plug-ins. Beyond such traditional workhorses, a number of innovative browser plug-ins have appeared, particularly for the increasingly popular Firefox browser.  For example, there are plug-ins that enable the user to view and work with the HTML and other elements of the page being viewed.  Another popular area is plug-ins that make it easier to capture and organize material from Web pages, going well beyond the standard favorites or bookmark facility. ",0
53,"Podcasting (from iPod plus broadcasting) lets users subscribe to and automatically download regularly distributed content (such as radio broadcasts) over the Internet.  The media files can be stored on an Apple iPod or other media player ,, personal computer, or other device ,.  Podcasting became popular starting around 2004–05 and has become widely used by individuals and organizations. Typically, files to be podcast are put on a Web server.  The URLs for the files and other information (such as episode titles) is provided in files called feeds, using a format such as RSS or Atom ,.  The user installs client software (such as iPodder), browses the feeds (such as through an online directory), and decides what to subscribe to.  The software then periodically checks the feeds, obtains the URLs of the latest files, and downloads them automatically.  The software can, if desired, then transfer the downloaded files to a portable media player, such as over a USB connection",0
54,"There are many sources of podcasts.  News organizations can provide regular audio or video podcasts as a supplement to regular text material.  Podcasting also offers a way for a small news organization or independent journalist to build an audience using equipment as simple as a microphone and perhaps a video camera.  Podcasts also provide a way for political organizations to keep in touch with supporters (and perhaps supply them with talking points).  Any source of periodically distributed audio or video can be a candidate for podcasting.  These include class lectures, corporate communications, and even religious services",0
55,"Early computer printers were limited to one or a few built-in fonts, either stamped on typewriter style keys on daisy wheels, or stored as patterns in the printer’s software (with dot matrix printers).  In the mid-1970s, when Xerox researchers were developing the laser printer, they realized they needed an actual programming language that could describe fonts, graphics, and other elements that could be printed on the more versatile new printers.  PARC researchers developed InterPress; meanwhile two of them, John Warnock and Chuck Geschke, founded their own company in 1982 ,.  They then created a more streamlined version of InterPress that they called PostScript.  The first printer to include built-in PostScript capability was Apple’s LaserWriter, in 1985.  PostScript soon became the standard for a burgeoning industry ,. Because PostScript is an actual programming language (for a somewhat similar language, see Forth), software such as word processors can include functions that turn a text document into a PostScript document, ready for printing.  A PostScript interpreter in the printer (or even in another application) interprets the PostScript commands to re-create the document.  The commands specify rasters (combinations of straight lines and curves), which can be scaled and transformed to provide the specified output, including fonts, which can be enhanced by including “hints” to help the system identify key features.  This processor is thus sometimes called a Raster Image Processor (RIP)",0
56,"Whether at a business meeting or a scientific conference, the use of slides or transparencies has been largely replaced by software that can create a graphic presentation.  Generally, the user creates a series of virtual “slides,” which can consist of text (such as bullet points) and charts or other graphics.  Often there are templates already structured for various types of presentations, so the user only needs to supply the appropriate text or graphics.  There are a variety of options for the general visual style, as well as for transitions (such as dissolves) between slides.  Another useful feature is the ability to time the presentation and provide cues for the speaker.  Finished presentations can be shown on a standard monitor screen (if the audience is small) or output to a screen projectorMicrosoft PowerPoint is the most widely used presentation program.  It includes the ability to import Excel spreadsheets, Word documents, or other items created by Microsoft Office suite applications.  The user can switch between outline view (which shows the overall structure of the presentation) to viewing individual slides or working with the slides as a collection. There are a number of alternatives available including Apple’s Keynote and Open Office, which includes a presentation program comparable to PowerPoint.  Another alternative is to use HTML Web-authoring programs to create the presentation in the form of a set of linked Web pages.  (PowerPoint and other presentation packages can also convert their presentations to HTML. ) Although creating presentations in HTML may be more difficult than using a proprietary package and the results may be somewhat less polished, the universality of HTML and the ability to run presentations from a Web site are strong advantages of that approach. ",0
57,"The large computers that first became available in the 1950s , used “line printers. ” These devices have one hammer for each column of the output.  A rapidly moving band of type moves under the hammers.  Each hammer strikes the band when the correct character passes by.  Printing is therefore done line by line, hence the name.  Line printers were fast (600 lines per minute or more) but like the mainframes they served, they were bulky and expensive. The typewriter offered another point of departure for designing printers.  A few early computers such as the BINAC (an offshoot of ENIAC) used typewriters rigged with magnetically controlled switches (solenoids).  However, a more natural fit was with the Teletype, invented early in the 20th century to print telegraph messages.  Since the Teletype is already designed to print from electrically transmitted character codes, it was easy to rig up a circuit to translate the contents of computer data into appropriate codes for printing.  (Since the Teletype could send as well as receive messages, it was often used as a control terminal for computer operators or for time-sharing computer users into the 1970s. )The daisy-wheel printer was another typewriter-like device.  It used a movable wheel with the letters embedded in slim “petals” (hence the name).  It was slow (about 10 characters a second), noisy, and expensive, but it was the only affordable alternative for early personal computer users who required “letter-quality” output. ",0
58,"The dot-matrix printer, which came into common use in the 1980s, uses a different principle of operation than typewriter-style printers.  Unlike the latter, the dot-matrix printer does not form solid characters.  Instead, it uses an array of magnetically controlled pins (9 pins at first, but 24 on later models).  Each character is formed by pressing the appropriate pins into a ribbon that pushes into the paper, leaving a pattern of tiny dots. Besides being relatively inexpensive, dot-matrix printers are versatile in that a great variety of character styles or fonts can be printed ,, either by loading different sets of bitmaps.  Likewise, graphic images can also be printed.  However, because the characters are made of tiny dots, they don’t have the crisp, solid look of printed type",0
59,"Good project management software provides many tools for the purpose.  Available charts and reports often include:,? Gantt charts that use bars to show the duration and percentage of completion of the various overlapping subprojects or tasks. , PERT (Program Evaluation and Review Technique) charts that show each subproject or task as a rectangular “node” with information about the task.  The connections between nodes show the relationships (dependencies) between the items.  PERT charts are usually used at the beginning stages of planning. , Analysis tools that show critical paths and bottlenecks (places where one or more tasks falling behind might threaten large portions of the project).  Generally, the more preceding items a task is dependent on, the more likely that task is to fall behind. , Tools for estimating the probability for completion of a given task based on the probabilities of tasks it is dependent on, as well as other factors such as the likelihood of certain resources becoming available. , A system of alerts or “stoplights” that show slowdowns, potential problems, or areas where work has stopped completely.  These can be set to be triggered when various specified conditions occur. , Integration between project management and budget reporting so tasks and the project as a whole can be monitored in relation to budget constraints. , Integration between the project management software and individual schedules kept in PIM software such as Microsoft Outlook or in handheld computers (PDAs) such as the PalmPilot. , Integration between project management and software for scheduling meetin",0
60,"Computing is a complex, pervasive, and increasingly vital human activity.  It is not surprising that human psychology can play an important role in many aspects of computer use. Since the 1960s psychology (in particular see cognitivescience) has contributed to the structuring of interaction between computer systems and users ,.  It is important to note the significant differences between how computers and humans perceive and process information: computers are extremely fast in processing in a highly structured setting (e. g. , a program).  The human brain, on the other hand, while thousands of times slower, is thus far greatly superior in coping with loosely structured data through pattern recognition, the making of analogies, and generalization.  A number of researchers , have promoted the idea of creating a human-computer synergy where the structure of the system takes advantage of both the machine’s computational and data-retrieval abilities and the human user’s ability to work with the larger picture.  Such research is continuing as autonomous software , and is beginning to interact with Web user",0
61,"The Internet and its perception as a shared cyberspace adds new dimensions to the psychology of computing.  In fact, the emphasis here is not on computation per se but on the representation of ideas and images, communication, social interaction, and identity.  In particular, pioneering work , has illuminated ways in which online interactions affect identity and sense of self—even encouraging the assumption of multiple identities ,.  Indeed, virtual worlds such as Second Life offer new ways to study the formation of communities and social interactions. On the positive side, it has been argued that cyberspace has encouraged people (particularly adolescents) to experiment with new identities in a relatively safe environment, but lack of inhibition and experience can lead to risky behavior such as involvement with sexual predators.  The very fact that many people (particularly the young) may spend several hours a day or more immersed in the online world has also led to concerns; some psychologists have even suggested that “Internet addiction disorder” (IAD) be included as an official mental disorder similar to compulsive gambling.  However, as of 2007, the American Medical Association has not recommended that IAD be classified as a mental disorder, and the American Society of Addiction Medicine has resisted such a status.  Generally, excessive or inappropriate use of the Internet has been seen as a symptom of more traditional diagnoses such as obsession or compulsion. ",0
62,"Python is particularly useful for system administrators, webmasters, and other people who have to link various files, data source, or programs to perform their daily tasks.  The language currently has a small but growing (and quite enthusiastic) following. Without the semicolons and braces found in C and related languages, Python looks rather like BASIC.  Also note that the type of input data doesn’t have to be declared.  The runtime mechanism will assume it’s numeric from the expression found in the print statement.  Python programs thus tend to be shorter and simpler than C, Java, or even Perl programs.  The simple syntax and lack of data typing does not mean that Python is not a “serious” language, however.  Python contains full facilities for object-oriented programming, for example. Python programs can be written quickly and easily by trying commands out interactively and then converted the script to bytecode, a machine-independent representation that can be run on an interpreter designed for each machine environment.  Alternatively, there are translation programs that can convert a Python script to a C source file that can then be compiled for top speed. ",0
63,"By combining mirroring, error correction, and/or striping, different “levels” of RAID can be implemented to suit different needs.  There are various trade-offs: Striping can increase access speed, but uses more storage space and, by increasing the number of disks, also increases the chance that one will fail.  Implementing error correction can make failure recoverable, but slows data access down because data has to be read from more than one location and compared. The most commonly used RAID levels are:, RAID 0—striping data across disks, higher speed but no error correction; failure of any disk can make data unrecoverable, RAID 1—mirroring (data stored on at least two disks), data intact as long as one disk is still operating, RAID 3 and 4—striping plus a dedicated disk for parity (error checking), RAID 5—striping with distributed parity; data can be restored automatically after a failed disk is replaced, RAID 6—like RAID 5 but with parity distributed so that data remains intact unless more than two drives failIn actuality, RAID configurations can be very complex, where different levels can be “layered” above one another, with each treating the next as a virtual drive, until one gets down to the actual hardware.  Although RAID is often implemented using a physical (hardware) controller, operating systems can also create a virtual RAID structure in software, interposed between the logical drive as seen by the read/write routines and the physical drives. ",0
64,"Back in the days of mechanical clocks, curious kids would sometimes take a clock apart to try to figure out how it worked.  A few were even able to reassemble the clock correctly—these youngsters were likely to become engineers! With software, reverse engineering is the process of “taking apart” software and analyzing its operation without having access to the program code itself.  Among other possibilities, reverse engineering may allow one to:, provide equivalent functions without violating copyright laws, emulate one operating system within another ,, determine a file format so other programs can use it as well (interoperability), document the operation of a program whose documentation is lost or no longer available, determine whether a competing product violates one’s patents or copyrights",0
65,"Passive RFID tags have no power supply; the power induced by the reading signal is used to transmit the response.  Because this power is very small, passive tags can only be read at distances from about 4 inches (10 cm) to a few yards (meters), depending on the antenna size and type.  The main advantage of passive tags is that the lack of a battery makes them small, lightweight, and inexpensive, making them ideal for attaching to merchandise (they have also been embedded under the skin of pets and, in a few cases, even people).  Smart cards for use in transit systems and similar applications are also passive; the system is activated by “tagging” or bringing the card near the reader. Active RFID tags have their own battery.  Their advantage is that they are able to initiate communication with the reader, and the signal they send is much stronger, more reliable, and with greater range (up to about 1,500 feet [500 m]).  The stronger signal allows for communication in rougher environments (such as outdoors for tracking cattle or shipping containers). ",0
66,"Current uses for RFID tags and cards of various types include:, automatic fare payments systems for transit systems, automatic toll payments for bridges and turnpikes, automatic book checkout systems for libraries, where it reduces repetitive strain injury (RSI) in staff and simplifies checking shelves, student ID cards, passports (RFID has been included in new U. S.  passports since 2006), tracking cattle, including determining the origin of unhealthy animals, identification chips placed beneath the skin of pets, experimental human RFID implants (pioneered by British computer scientist Kevin Warwick) and now used by VIP customers in a few nightclubs, tracking goods from original shipment to inventory (Wal-Mart now requires its major suppliers to include RFID labels with shipments), scientific sensors, such as seismographic instruments",0
67,"The benefits of RFID technology are numerous: better inventory control ,; more secure passports and other forms of ID; faster, easier access to transportation systems; and potentially, the avoidance of mishaps in hospitals, such as the wrong patient receiving a drug or procedure. However, there are privacy and security concerns that remain to be fully resolved.  The primary threat is that unauthorized persons could illicitly obtain information or track people or goods, for purposes ranging from simple larceny to identity theft.  Privacy rights organizations have also raised concerns that information about consumer purchases could be used for unwanted marketing (or sold to third parties), while information about a library patron’s reading habits could trigger unwarranted government investigations in the name of fighting terrorism. There is an incentive to produce RFID cards and tags that are resistant to unauthorized reading or tampering.  A cryptographic protocol can be used such that no information will be sent or received unless the reader and tag “know” the correct keys.  Another possibility is to create a device that can “jam” reading attempts in the device’s vicinity, perhaps protecting a customer’s grocery cart from being scanned.  Finally, RFID cards can be put inside in a sleeve of material that blocks the signals.  However, cryptographic and other security technologies raise the cost of RFID devices and may make them impracticable for some applications. In September 2006 the National Science Foundation awarded a $1. 1 million grant to the RFID Consortium for Security and Privacy to study potential risks and safeguards for the technology.  That same year a group of major corporations together with the National Consumers League released a draft set of standards and guidelines for best practices in using RFID, with broader scope than the existing EPC (electronic product code) standard",0
68,"Programmers and managers of software development are generally aware of the need for software to properly deal with erroneous data ,.  They know that any significant program will have bugs that must be rooted out ,.  Good software engineering practices and a systematic approach to assuring the reliability and quality of software can minimize problems in the finished product ,.  However, serious bugs are not always caught, and sometimes the consequences can be catastrophic.  For example, in the Therac 25 computerized X-ray cancer treatment machine, poorly thought-out command entry routines plus a counter overflow resulted in three patients being killed by massive X-ray overdoses.  The overdoses ultimately occurred because the designers had removed a physical interlock mechanism they believed was no longer necessary. Any computer application is part of a much larger environment of humans and machines, where unforeseen interactions can cause problems ranging from inconvenience to loss of privacy to potential injury or death.  Seeing these potential pitfalls requires thinking beyond the specifications and needs of a particular project.  For many years the Usenet newsgroup comp. risks (and its collected form, Risks Digest) have chronicled what amounts to an ongoing symposium where knowledgeable programmers, engineers, and others have pointed out potential risks in new technology and suggested ways to minimize them",0
69,"In 1921, the Czech playwright Karel Capek wrote a play called R. U. R.  or Rossum’s Universal Robots.  Robot is a Czech word that has been translated as work(er), serf, or slave.  In the play the robots, which are built by factories to work in other factories, eventually revolt against their human masters. During the 1960s, real robots began to appear in factory settings ,.  They were an outgrowth of earlier machine tools that had been programmed by cams and other mechanisms.  An industrial robot is basically a movable arm that ends in a “hand” called an end effector.  The arm and hand can be moved by some combination of hydraulic, pneumatic, electrical, or mechanical means.  Typical applications include assembling parts, welding, and painting.  The robot is programmed for a task either by giving it a detailed set of commands to move to, grasp, and manipulate objects, or by “training” the robot by moving its arm, hand, and effectors through the required motions, which are then stored in the robot’s memory.  By the early 1970s, Unimation, Inc.  had created a profitable business from selling its Unimate robots to factories. The early industrial robots had very little ability to respond to variations in the environment, such as the “work piece” that the robot was supposed to grasp being slightly out of position.  However, later models have more sophisticated sensors to enable them to adjust to variations and still accomplish the task.  The more sophisticated computer programs that control newer robots have internal representations or “frames of reference” to keep track of both the robot’s internal parameters (angles, pressures, and so on) and external locations in the work area",0
70,"Industrial robots work in an extremely restricted environment, so their world representation can be quite simple.  However, robots that can move about in the environment have also been developed.  Military programs have developed automatic guided vehicles (AGVs) with wheels or tracks, capable of navigating a battlefield and scouting or attacking the enemy ,.  Space-going robots including the Sojourner Mars rover also have considerable onboard “intelligence,” although their overall tasks are programmed by remote commands. Indeed, the extent to which mobile robots are truly autonomous varies considerably.  At one end is the “robot” that is steered and otherwise controlled by its human operator, such as law enforcement robots that can be sent into dangerous hostage situations.  (Another example is the robots that fight in arena combat in the popular Robot Warsshows. )",0
71,"Moving toward greater autonomy, we have the “service robots” that have begun to show up in some institutions such as hospitals and laboratories.  These mobile robots are often used to deliver supplies.  For example, the HelpMate robot can travel around a hospital by itself, navigating using an internal map.  It can even take an elevator to go to another floor. Service robots have had only modest market penetration, however.  They are relatively expensive and limited in function, and if relatively low-wage more versatile human labor is available, it is generally preferred.  For now mobile robots and service robots are most likely to turn up in specialized applications in environments too dangerous for human workers, such as in the military, law enforcement, handling of hazardous materials, and so on. ",0
72,"IBM did succeed in creating RPG (Report Program Generator), a language designed to make it easier for programmers (including beginners) to generate business reports. Most COBOL programs read data, perform tests and calculations, and print the results.  RPG, first released in 1964 for use with the new System/360 mainframe and the smaller System/3, simplifies this process and eliminates most writing of program code statements. A “classic” RPG program is built around the “RPGcycle,” consisting of three stages.  During the input stage, the input device(s), file type, access specifications, and data record structure are specified.  (These specifications can be quite elaborate. ) The heart of the program specifies calculations to be performed with the various data fields, while the output section specifies how the results will be laid out in report form, including such things as headers, footers, and sections. Subsequent versions of RPG added more features.  RPGIV, released in 1994, includes the ability to define subroutines, for example.  IBM has also released VisualAge RPG, which allows for the creation and running of RPG programs in the Microsoft Windows environment.  There are also tools for interfacing RPG programs with various database systems and to use RPG for writing Web-based (CGI) programs",0
73,"Web sites such as news providers and blogs , are constantly posting new material.  While readers can periodically visit a site to look for new material, an increasingly popular option is to subscribe to a “Web feed” and receive the latest information automatically.  The most commonly used tool for Web feeds is RSS, which can stand for Really Simple Syndication, Rich Site Summary, or RDF Site Summary, depending on the format used. The data in an RSS feed can include article titles, summaries, excerpts (such as the first paragraph), or the complete article or posting.  Feeds can also include multimedia such as graphics, video, or sound.  The data (and any linked material) is formatted using standard markup elements ,.  As part of the process of setting up a feed on the Web server, the feed is “published” so that it can be found and read using a client program called a reader or aggregator (the latter can combine feeds or organize them in a newspaper-like format for convenience).  RSS readers can be stand-alone applications or be included with many modern Web browsers and e-mail clients.  Alternatively, Webbased readers or aggregators such as NewsGator Online can allow feeds to be read using any Web browser.  Readers of Web pages can find RSS feeds by looking for a “subscribe” icon or the words RSS or XML.  Specialized search engines such as Bloglines can also help users find interesting feeds.  Additionally, information on the server can also be used by software to automatically deliver the latest content",0
74,"Forerunners of RSS go back to the mid-1990s, with RDF Site Summary first appearing in 1999 for use on Netscape’s portal.  The adoption of RSS by the New York Times in 2002 greatly aided the popularization of the format, as did the growing number of blogs that needed a way for contributors and readers to keep in touch.  Today Web browsers such as Internet Explorer, Mozilla Firefox, and Safari support RSS.  File-sharing services such as BitTorrent can be combined with RSS to deliver content automatically to users’ hard drives.  An offshoot of RSS called Atom has been less widely adopted, but offers better compatibility with XML standards and better management of multimedia content",0
75,"Rich Text Format was developed in the later 1980s by programmers at Microsoft.  Its purpose is to allow for interchange of documents between Microsoft Word and other software, while preserving the original formatting. An RTF file is itself a plain text file containing the document text enclosed in control codes that determine the formatting. Although RTF is an 8-bit format, special escape sequences can be used to specify 16-bit Unicode characters, such as for non-Roman alphabets. Libraries and utilities are available for reading and writing RTF from most popular programming languages, including Perl, PHP, and Ruby. In practice, RTF created by word processors tends to contain many control codes needed to ensure compatibility with older programs, making the files bulky and not practicable to edit directly.  However, saving a file in RTF is a good way to ensure that a document can be used by recipients who may have, for example, older versions of Word. ",0
76,"Ruby is a versatile yet consistent programming language that has become popular in recent years, particularly for Web development.  Designed by Yukihiro Matsumoto and first released in 1995, Ruby has a compact syntax familiar to many users of Perl and other scripting languages ,, avoiding, for example, the need to declare variable types.  However, Ruby is also a thoroughgoing object-oriented language somewhat like Smalltalk ,.  Matsumoto has stressed that the design of the language is intended to stress being natural and enjoyable for the programmer, rather than focusing on the needs of the machine",0
77,"The most popular programming environment for Ruby is Ruby on Rails, an open-source application framework aimed particularly at writing programs that connect Web sites to databases.  The framework is based on the model view controller approach (separating data access and logic from the user interface) and includes “scaffolding” that can be quickly filled in to provide data-driven Web sites with basic functionality.  Developers can also create plug-ins to extend the built-in packages",0
78,"SAP (NYSE symbol: SAP) is a German acronym for Systeme, Anwendungen, und Produkete in der Datenverarbeitung (“Systems, Applications, and Products in Data Processing”).  Five former IBM engineers in Germany founded the company in 1972. Although unfamiliar to the American public, unlike IBMand Microsoft, SAP is the world’s largest business software company, and fourth-largest software provider in general (behind Microsoft, IBM, and Oracle).  The company operates worldwide through three geographical divisions. ",0
79,"SAP specializes in Enterprise Resource Planning (ERP), enhancing a corporation’s ability to manage its key assets and needs and to plan for the future.  This software consists of three tiers: the database, an application server, and the client.  Early versions of this software were designed to run on mainframes.  Other major products include:, SAP NetWeaver, which integrates all other SAP modules using modern open-standard Web technologies ,, Customer Relationship Manager ,, Supply Chain Management ,, Supplier Relationship Management, Human Resource Management System, Product Lifestyle ManagementExchange Infrastructure, Enterprise Portal, SAP Knowledge Warehouse",0
80,"SAP has recognized for some time that while its base of large Fortune 500 companies has given it steady income, changing trends in business have been limiting the software giant’s growth.  In particular, the trend has been toward smaller, simpler, more scalable applications that can be integrated with modern Web services.  In September 2007 SAP announced SAP Business ByDesign, a flexible set of enterprise management services that are delivered over the Web.  However, it remains to be seen how well SAP will be able to compete with more agile companies such as NetSuite and Salesforce. com, and whether the company will be able to upgrade its existing large company user base without disaffecting it. SAP’s major competitor in the United States is Oracle ,, which has sued SAP in 2007 for unfairly downloading and using patches and support materials from Oracle and using them to support former Oracle customers.  SAP and Oracle have generally had quite different growth strategies: SAP grows by expanding and extending its own products, while Oracle has grown mainly through acquiring other companies.  However, in October 2007 SAP acquired Business Objects, a leader in “business intelligence” systems, for $6. 8 billion.  This may signal SAP’s willingness to engage in further strategic acquisitions",0
81,"In order for a computer to work with information, the information must be digitized—converted to data that application programs can recognize and manipulate ,.  Computer users have thus been confronted with the task of converting millions of pages of printed words or graphics into machine-readable form.  Since it is expensive to re-key text (and impractical to redraw images), some way is needed to automatically convert the varying shades or colors of the text or images into a digitized graphics image that can be stored in a file. This is what a scanner does.  The scanner head contains a charge-coupled device (CCD) like that used in digital cameras ,.  The CCD contains thousands or millions of tiny regions that can convert incoming light into a voltage level.  Each of these voltage levels, when amplified, will correspond to one pixel of the scanned image.  (A color scanner uses three different diodes for each pixel, each receiving light through a red, green, or blue filter. )",0
82,"The operation of the head depends on the type of scanner.  In the most common type, the flatbed scanner, a motor moves the head back and forth across the paper, which lies facedown on a glass window.  In a sheet-fed scanner, the head remains stationery and the paper is fed past it by a set of rollers.  Finally, there are handheld scanners, where the job of moving the scanner head is performed by the user moving the scanner back and forth over the page. ",0
83,"The resolution of a scanner depends on the number of pixels into which it can break the image.  The color depth depends on how many bits of information that it can store per pixel (more information means more gradations of color or gray).  Resolutions of 2,400 dots per inch (dpi) or more are now common, with up to 36 bit color depth, allowing for about 68. 7 billion colors or gradations ,Besides considerations of resolution and color depth, the quality of a scanned image depends on the quality of the scanner’s optics as well as on how the page or other object reflects light.  As anyone who has browsed eBay listings knows, the quality of scans can vary considerably.  Most scanners come with software that allows for the scanner to be controlled and adjusted from the PC, and image-editing software can be used to further adjust the scanned image",0
84,"These general principles also apply to systems where more than one processor is available ,, but there is the added complication of deciding where the scheduling program will be run.  In a multiprocessing system that has one “master” and many “slave” processors, the scheduling program runs on the master processor.  This arrangement is simple, but it means that when a slave processor wants to schedule a program it must wait until the scheduling program gets its next time-slice on the master processor. One alternative is to allow any processor that has free time to run the scheduling algorithm.  This is harder to set up because it requires a mechanism to make sure two processors do not try to run the scheduling program at the same time, but it smoothes out the bottleneck that would arise from relying on a single processor. ",0
85,"o trends in recent years have changed the emphasis in scheduling algorithms.  One is the continuing drop in price per unit of processing power and memory.  This means that maximum efficiency in using the hardware can often give way in favor of catering to the user’s convenience and perceptions by giving more priority to interaction with the user.  The other development is the growing use of systems where much of the burden of graphics and interactivity is placed on the user’s desktop, thus simplifying the complexity of sch",0
86,"From microbiology to plasma physics, modern science would be impossible without the computer.  This is not because the computer has replaced the scientific method of observation, hypothesis, and experiment.  Modern scientists essentially follow the same intellectual procedures at did Galileo, Newton, Darwin, and Einstein.  Rather, understanding of the layered systems that make up the universe has now reached so complex and detailed a level that there is too much data for an individual human mind to grasp.  Further, the calculations necessary to process the data usually can’t be performed by unaided humans in any reasonable length of time.  This can be caused either by the inherent complexity of the calculation , or the sheer amount of data (as in DNA sequencing; see bioinformatics and data mining)",0
87,"Even if scientists have a basic understanding of a system, it may be hard to determine what the overall results of the interaction of the many particles (or other elements) in the system will be.  This is true, for example, in the analysis of events taking place in nuclear reactors.  Fortunately computers can apply the laws of the system to each of many particles and determine the resulting actions from their aggregate behavior ,.  Simulation is particularly important in fields where actual experiments are not possible because of distance or time.  Thus, a hypothesis about the formation of the universe can be tested by applying it to a set of initial conditions believed to reflect those at or near the time of the big bang. However, even the most skilled scientists have trouble relating numbers to the shape and interaction of real-world objects.  Computers have greatly aided in making it possible to visualize structures and phenomena using high-resolution 3D color graphics ,.  Features of interest can be enhanced, and arbitrary (“false”) colors can be used to visually show such things as temperature or blood flow.  These techniques can also be used to create interactive models where scientists can, for example, combine molecules in new ways and have the computer calculate the likely properties of the result.  Finally, computer visualization and modeling can be used both to teach science and to give the general public some visceral grasp of the meaning of scientific theories and discoveries",0
88,"By the mid-1990s, many thousands of pages were being added to the World Wide Web each day ,.  The availability of graphical browsing programs such as Mosaic, Netscape, and Microsoft Internet Explorer , made it easy for ordinary PC users to view Web pages and to navigate from one page to another.  However, people who wanted to use the Web for any sort of systematic research found they needed better tools for finding the desired information. There are basically three approaches to exploring the Web: casual “surfing,” portals, and search engines.  A user might find (or hear about) an interesting Web page devoted to a business or other organization or perhaps a particular topic.  The page includes a number of featured links to other pages.  The user can follow any of those links to reach other pages that might be relevant.  Those pages are likely to have other interesting links that can be followed, and so on.  Most Web users have surfed in this way: It can be fun and it can certainly lead to “finds” that can be bookmarked for later reference.  However, this approach is not systematic, comprehensive, or efficient. Alternatively, the user can visit a site such as the famous Yahoo! started by Jerry Yang and David Filo ,.  These sites specialize in selecting what their editors believe to be the best and most useful sites for each topic, and organizing them into a multilevel topical index.  The portal approach has several advantages: The work of sifting through the Web has already been done, the index is easy to use, and the sites featured are likely to be of good quality.  However, even Yahoo!’s busy staff can examine only a tiny portion of the estimated 1 trillion or so Web pages being presented on about 175 million different Web sites (as of 2008).  Also, the sites selected and featured by portals are subject both to editorial discretion (or bias) and in some cases to commercial interest. ",0
89,"The ever-growing World Wide Web consists of billions of linked HTML documents (and other resources), but most of the links contain no information about why the linkage has been made or what it might mean.  Services such as Google can automatically trace the links and index each page , with the aid of “metadata” such as keywords that summarize page content.  However, discovering the relationships between data items on pages, or between pages—and their meaning, or semantics—requires human scrutiny. In his 1999 book Weaving the Web, World Wide Web creator Tim Berners-Lee , described a new way in which Web pages might be organized in the future:I have a dream for the Web [in which computers] become capable of analyzing all the data on the Web—the content, links, and transactions between people and computers.  A “Semantic Web,” which should make this possible, has yet to emerge, but when it does, the day-to-day mechanisms of trade, bureaucracy and our daily lives will be handled by machines talking to machines.  The “intelligent agents” people have touted for ages will finally materialize. In other words, by encoding definitions of objects and their relationships into the text of Web pages, programs , can be written to use this information to answer sophisticated questions such as “which devices from this vendor use open-source software?”",0
90,"The semantic Web is not something that can appear overnight—after all, it will take considerable human effort to encode the information needed for machines to understand Web resources, and additional effort to code the application programs that will take advantage of that information.  However, the potential payoff is huge, allowing both human and automated searchers to tackle much more sophisticated tasks. For example, the University of Maryland is developing a prototype semantic search engine called Swoogle.  It can extract information and determine relationships between documents that include RDF or OWL elements.  Swoogle can also help users find appropriate ontologies for exploring a subject ,. Much research needs to be done.  For example, there is the problem of deriving a measure of “reliability” or “trust” based on the data sources used to answer the query, which may be scattered all over the world and represent very different kind",0
91,"A growing number of people 50 and older have been learning how to use computer technology and especially applications such as e-mail and Web browsing.  However, a substantial number of seniors have expressed reluctance to join the digital world—as of January 2006, the Pew Internet & American Life Project found that only 34 percent of persons 65 and over were online.  Some reasons why seniors have avoided the technology include the following:, the belief that it would be too hard to learn to use it, uncertainty about what can be done online and whether it is worth the effort, fear of well-publicized dangers such as viruses and identity theft, the expense of a personal computer and Internet accessFortunately a number of these factors are gradually being ameliorated.  There are numerous books and courses (such as at adult education or senior centers) that introduce the essentials of computing to seniors.  Properly installed security and filtering software, together with some user education, can minimize the chances of being victimized online.  Finally, Internet-capable PCs are now available for around $300 or less, though the cost of broadband access has not fallen as rapidly as that of hardware. ",0
92,"There are basically two ways to move data from a computer to or from a peripheral device such as a printer or modem.  A byte (8 bits) of data can be moved all at once, with each bit traveling along its own wire ,.  Alternatively, a single wire can be used to carry the data one bit at a time.  Such a connection is called a serial port. The serial port receives data a full byte at a time from the computer bus and uses a UART (Universal Asynchronous Receiver-Transmitter) to extract the bits one at a time and send them through the port.  A corresponding circuit at the other end accumulates the incoming bits and reassembles them into data bytes. The data bits for each byte are preceded by a start-bit to signal the beginning of the data and terminated by an stopbit.  Depending on the application, an additional bit may be used for parity ,.  Devices connected by a serial port must “negotiate” by requesting a particular connection speed and parity setting.  Failure to agree results in gibberish being received. ",0
93,"The official standard for serial transmission is called RS232C.  It defines various additional pins to which wires are connected, such as for synchronization (specifying when the device is ready to send or receive data) and ground.  Physically, the old-style connectors are called DB-25 because they contain 25 pins (many of which are not used).  Most newer PCs have DB-9 (i. e.  nine pin) connectors.  A “gender changer” can be used in cases where two devices both have male connectors (with pins) or female connectors (with corresponding sockets). Because they use a single data transmission line and include error-correction, serial cables can be longer than parallel cables (25 feet or more, as opposed to 10–12 feet).  Serial transmission is generally slower (at up to 115,200 bits/second) than parallel transmission.  Serial connections have generally been used for such devices as modems (whose speed is already limited by phone line characteristics), keyboards, mice, and some older printers.  Today the faster and more flexible USB , is replacing serial connections for many devices including even keyboards",0
94,"In 1982, Andrew Fuegelman created a program called PC-Talk.  This program provided a better way for users with modems to connect to the many bulletin board systems that were starting to spring up.  Fluegelman was familiar with the common practice of public radio and TV broadcasters of soliciting pledge payments to help support their “free” service.  He decided to do something similar with his program.  He distributed it to many bulletin boards, where users could download it for free.  However, he asked users who liked the program and wanted to continue to use it to pay him $25. Fluegelman dubbed his method of software distribution “freeware” (because it cost nothing to try out the program).  Other programmers began to use the same method with their own software.  This included Jim Knopf, author of the PC-File database program, and Bob Wallace, who offered PC-Write as a full-featured alternative to expensive commercial word processing program.  Because Fluegelman had trademarked the term freeware, these other authors began to call their offerings shareware. Today freeware means software that can be downloaded at no cost and for which there is no charge for continued use.  The program may be redistributed by users as long as they don’t charge for it",0
95,"To see a simple shell in action, a Windows user need only bring up a command prompt, type the word dir, and press Enter.  A shell called command. com provides the user interface for users of IBM PC-compatible systems running MS-DOS.  The command processor displays a prompt on the screen.  It then interprets , the user’s commands.  If the command involves one of the shell’s internal operations (such as “dir” to list a file directory), it simply executes that routine.  For example the command:dir temp /pwould be interpreted as a call to execute the dir function, passing it the name “temp” (a directory) and the /p, which dir interprets as a “switch” or instruction telling it to pause the directory listing after each screenful of text.  If the command is an external MS-DOS utility such as “xcopy” (a file copying program), the shell runs that program, passing it the information (mainly file names) from the command line.  Finally, the shell can run any other executable program on the system.  It is then that program’s responsibility to interpret and act upon any additional information that was provided. MS-DOS also has the ability for the command. com shell to read a series of commands stored in a text file called a batch file, and having the *. bat (batch) extension.  This allowed for rudimentary scripting of system housekeeping operations or other routine tasks ,. ",0
96,"A simulation is a simplified (but adequate) model that represents how a system works.  The system can be an existing, real-world one, such as a stock market or a human heart, or a proposed design for a system, such as a new factory or even a space colony. If a system is simple enough (a cannonball falling from a height, for example), it is possible to use formulas such as those provided by Newton to get an exact answer.  However, many real-world systems involve many discrete entities with complex interactions that cannot be captured with a single equation.  During the 1940s, scientists encountered just this problem in attempting to understand what would happen under various conditions in a nuclear reaction. Together with physicist Enrico Fermi, two mathematicians, John von Neumann , and Stanislaw Ulam, devised a new way to simulate complex systems.  Instead of trying fruitlessly to come up with some huge formula to “solve” the whole system, they applied probability formulas to each of a number of particles—in effect, “rolling the dice” for each one and then observing their resulting distribution and behavior.  Because of its analogy to gambling, this became known as the Monte Carlo method.  It turned out to be widely useful not only for simulating nuclear reactions and particle physics but for many other activities (such as bombing raids or the spread of disease) where many separate things behave according to probabilities. ",0
97,"Simulations and simulation techniques are used for a tremendous range of applications today.  Besides helping with the understanding of natural systems in physics, chemistry, biology, or engineering, simulation techniques are also applied to human behavior.  For example, the behavior of consumers or traders in a stock market can be explored with a simulation based on game theory concepts.  Artificial intelligence techniques (such as expert systems) can be used to give the individual “actors” in a simulation more realistic behavior. Simulations are often used in training.  A modern flight simulator, for example, not only simulates the aerodynamics of a plane and its response to the environment and to control inputs, but detailed graphics (and simulated physical motion) can make such training simulations feel very realistic, if not quite to Star Trek holodeck standards.  Whether for flight, military exercises, or stock trading, simulations can provide a much wider range of experiences in a relatively short time than would be feasible (or safe) using the real-world activity.  Simulations can also play an important part in testing software or systems or in predicating the results of business decisions or strategies. Simulations are also frequently sold as entertainment.  Many commercial strategy and role-playing games as well as vehicle simulators contain surprisingly complex simulations that make the games both absorbing and challenging ,.  Such games can also have considerable educational value. ",0
98,"Some typical features of a smart building include the following:, lighting that is controlled by time of day, scheduling, and occupancy sensors, temperature and air-flow sensors to determine the amount of cooling, heating, or fresh air needed, controls for central heating, hot water, and air conditioning systems, optimizing efficiency and minimizing energy use, alarms for intrusion, fire, carbon monoxide/dioxide, and other hazards, alarms indicating failure or unsafe operating conditions for various devices, integration of alarm and status messages with communications systems, enabling users to receive them by e-mail, text message, phone, or other means",0
99,"The smart card is the next generation of transaction devices.  Magnetically coded credit, debit, and ATM cards have been in use for many years.  These cards contain a magnetic strip encoded with a small amount of fixed data to identify the account.  All the actual data (such as account balances) is kept in a central server, which is why credit cards must be validated and transactions approved through a phone (modem) link.  Some magnetic strip cards such as those used in rapid transit systems are rewritable, so that, for example, the fare for the current ride can be deducted.  Telephone cards work the same way.  Nevertheless, these cards are essentially passive tokens containing a small amount of data.  They have little flexibility. However, since the mid-1970s it has been possible to put a microprocessor and rewritable memory into a card the size of a standard credit card.  These smart cards can store a hundred or more times the data of a magnetic strip card.  Further, because they have an onboard computer ,, they can interact with a computer at the point of service, exchanging and updating information",0
100,"Besides the microprocessor and associated circuitry, the smart card contains a small amount of RAM (random access memory) to hold “scratch” data during processing, as well as up to 64 kB of ROM (read-only memory) containing the card’s programming instructions.  The program is created on a desktop computer and written to the ROMthat is embedded in the card.  Finally, the card includes up to 64 kB of EEPROM (Electrically Erasable Programmable Read Only Memory) for holding account balances and other data.  This memory is nonvolatile (meaning that no power is needed to maintain it), and can be erased and rewritten by the card reader. “Contact” cards must be swiped through the reader and are most commonly used in retail, phone, pay TV, or health care applications.  “Contactless” cards need only be brought into the proximity of the reader, which communicates with it via radio signals or a low-powered laser beam.  Contactless cards are more practical for applications such as collecting bridge tolls ,. The card reader (or terminal) at the point of sale contains its own computer, which runs software that requests particular services from the card’s program, including providing identifying information and balances, updating balances, and so on. ",0
101,"Some of the major smartphone manufacturers and their operating systems include the following:, Symbian (Symbian OS), used by Nokia, Motorola, Samsung, and others, Windows Mobile (enhanced Windows CE), popular in phones used in Asia, Blackberry (RIM), the popular PDA/smartphone, Linux, used as the base on which to build a variety of PDA/phone operating systems, including products from Motorola, Palm, and Nokia (Maemo), OS X (Apple), used in Apple’s innovative and very popular iPhone",0
102,"Originally standing for Simple Object Access Protocol, but now no longer an acronym, SOAP is a standard way to access Web services ,.  In today’s Web, where what appears to users to be a single site or application is usually built from many services, such a facility is essential. Prior to SOAP, Web applications usually communicated through remote procedure calls (RPC).  However there were problems with compatibility of applications running under different operating systems (and perhaps using different programming languages), as well as security problems that often led to such facilities being blocked. SOAP, on the other hand, uses the same HTTP recognized by all Web servers and browsers ,—indeed, it can also use secure HTTP (https). A SOAP request (or message) is an ordinary XML file , that includes an “envelope” element specifying it to be a SOAP message, an optional header, a body element containing the information pertaining to the function or transaction requested, and an optional fault element to specify error processing.  After receiving the message, the destination server returns a message providing the requested information. ",0
103,"Today, millions of people—middle, high school, and college students, but increasingly adults as well—have pages on popular Web sites such as MySpace and Facebook.  These sites are significant examples of social networking: the use of Web sites and communications and collaboration technology to help people find, form, and maintain social relationships. The origins of social networking can be traced to online venues that arose in the 1970s and 1980s, notably Usenet and, later, online chat boards ,.  In the late 1990s social networking Web sites began to appear, including Classmates. com (helping people find and communicate with former schoolmates) and SixDegrees. com, which emphasized “knows someone who knows someone who .  .  . ” kinds of links. By the mid-2000s the two biggest sites were Facebook and MySpace.  Founded in 2006 by Mark Zuckerberg, Facebook was originally restricted to Harvard students, but eventually became open to any college student, and then high schools and even places of employment.  (The name comes from a book given to incoming students in some schools to familiarize them with their peers. ) As of late 2007 Facebook had more than 55 million active members and had become the seventh most visited of all Web sites",0
104,"Social scientists can use a variety of software throughout the research process.  For example, researchers might use the following:, Web and bibliographical search tools to find existing research on their topic, note-taking and concept-diagramming (“mind-mapping”) software, software to conduct polls or surveys and compile the results, social networking analysis to better understand a group’s structure and dynamics, statistical analysis tools to analyze the findings ,, map-based systems for studying geographical aspects ,, modeling software to simulate the mechanism being studied, using mathematical techniques such as the Monte Carlo and Markov-Chain methods",0
105,"In general, the overall steps in developing a program are:Detailed specification of what the program will be required to do.  This can include developing a prototype and getting user’s reaction to it. , Creation of a suitable program architecture—algorithm(s) and the data types, objects, or other structures needed to implement them ,. , Coding—writing the program language statements that implement the structure. , Verification and testing of the program using realistic data and field testing ,. ,? Maintenance, or the correction of errors and adding of requested minor features (short of creating a new version of the program)",0
106,"Sony Corporation (NYSE symbol: SNE) is the electronics business unit of Sony Group, a large Japanese multinational company that plays a leading role in worldwide electronics, games, and entertainment media (movies and music), introducing and shaping many now-familiar standards. The company traces its origin to a radio repair shop started by Masaru Ibuka in a bombed-out building in Tokyo in 1945.  He was soon joined by Akio Morita, and the men started an electronics company whose name translates in English to Tokyo Telecommunications Engineering Corporation.  They started by building tape recorders, but in the early 1950s the two entrepreneurs were among the earliest to realize the potential of the transistor, marketing transistor radios starting in 1956.  The devices essentially established the modern consumer electronics field, perfectly fitting with a new music fad among American teenagers—rock and roll. With their marketing success, Ibuka and Morita realized that they needed a simple, catchy name that would appeal to Americans and other non-Japanese customers.  In 1958 they came up with Sony.  Although the name did not exist in any language (and thus could be made proprietary), “Sony” evokes English words such as “sound” and “sonic. ” (It also resembled a Japanese slang phrase “sony-sony,” for something like what we would call “geeks” or “nerds” today. )",0
107,"the company’s successful consumer products have included the following:, Trinitron tubes for televisions and computer monitors (no longer sold in the United States), Walkman portable music player (1979), 3. 5? floppy disk (1983), which flourished until the later 1990sSony? ? ? ? 445, Discman CD-based music player (1984), Handycam camcorder and Video format (1985), Digital audio tape, or DAT (1987), Blu-ray optical discSony would also become a major player in the console gaming market ,.  In 1994 the company introduced the PlayStation, followed by later models in 2000 and 2006.  Sony is also a significant seller of digital cameras, including the Mavica floppy disc (later CD), since discontinued.  The company also introduced its proprietary “memory stick” for storage",0
108,"The simplest and least efficient kind of sort is called the selection sort.  Rather like a bridge player organizing a hand, the selection sort involves finding the record with the lowest key and swapping it with the first record, then scanning back through for the next lowest key and swapping it with the second record, and so on until all the records are sorted.  While this uses memory very efficiently (since the records are sorted in place), it is not only slow, but also gets worse fast.  That is, the time taken to sort n records is proportional to n2. The selection approach suffers because on each pass the sort determines not only the record with the lowest key but 446? ? ? ? sorting and searchingthe one with the next lowest key.  However, that information is not retained.  The heapsort, invented by John Williams in 1964, uses a binary tree to store a heap of sorted records ,.  Once the heap is built, the tree nodes can be used to store record numbers in a corresponding array that will represent the sorted database.  The heapsort is efficient because no records are physically moved, and the only memory needed is for the heap and array.  The heapsort is generally considered the fastest and most reliable general-purpose sorting algorithm, with a maximum running time of log n",0
109,"The bubble sort is based on making comparisons and swaps.  It makes the most convenient comparison possible: each record with its neighbor.  The algorithm looks at the first two records.  If the second has a lower key than the first, the records are swapped.  The procedure continues with the second and third records, then the third and fourth, and so on through all the records, swapping pairs of adjacent records whenever they are out of order.  After one pass the record with highest key will have “bubbled up to” the end of the list.  The procedure is then repeated for all but the last record until the two highest records are at the end, and so on until all the records are sorted.  Unfortunately, the number of comparisons and swaps that must be made makes the bubble sort as slow as the selection sort. ",0
110,"The quicksort improves on the basic bubble sort by first choosing a record with a key approximately midway between the lowest and highest.  This key is called the pivot.  The records are then moved to the left of the pivot if they are lower than it, and to the right if higher (that is, the records are divided into two partitions).  The process is then repeated to split the left side with a new pivot, and then the right side likewise.  This is continued until the partition size is one, and the records are now all sorted.  (Because of this repeated partitioning, quicksort is usually implemented using a procedure that calls itself repeatedly—see recursion. )Devised by C.  A.  R.  Hoare in 1962, quicksort is much faster than the bubble sort because records are moved over greater distances in a single operation rather than simply being exchanged with their neighbors.  Assuming an appropriate initial pivot value is chosen, running time is proportional to the logarithm of n rather than to the square of n. The difference becomes dramatic as the size of the database increases",0
111,"The bubble sort and quicksort are designed to work with records that are in random order.  However, in many applications a database grows slowly over time.  At any given time the existing database is already sorted, so it hardly makes sense to have to resort the whole database each time a new record is added. Instead, an insertion sort can be used.  In its simplest form, the algorithm looks sequentially through the sorted records until it finds the first record whose key is higher than that of the new record.  The new record can then be inserted just before that record, much like the way a bridge player might organize the cards in a hand.  (Since inserting a record and physically moving all the higher records up in memory can be time-consuming, a linked list of key values and associated record number is often used instead.  (See list processing. ) That way only the links need to be changed rather than any records being moved. The insertion sort was improved by Donald L.  Shell in 1959.  His “shellsort” takes a recursive approach (like that in the quicksort), and applies the insertion sort procedure to successively smaller partitions. Another improvement on the insertion sort is the mergesort.  As the name implies, this approach begins by creating two small lists of sorted records (using a simple comparison algorithm), then merging the lists into longer lists.  Merging is accomplished by looking at the two keys on the top of two lists and taking whichever is lowest until the lists are exhausted.  The merge sort also lends itself to a recursive approach, and it is comparable in speed and stability to the heapsort",0
112,"All of the sorting algorithms discussed so far rely upon some form of comparison.  However, it also possible to sort records by calculating their relative positions or distribution ,.  In its simplest form, an array can be created whose range of indexes is equal to 1 to the maximum possible key value.  Each key is then stored in the index position equal to its value (that is, a record with a key of 2314 would be stored in the array at position Array[2314].  This procedure works well, but only if the keys are all integers, the range is small enough to fit in memory, and there are no duplicate keys (since a duplicate would in effect overwrite the record already stored in that position). A more practical approach is to use a formula (hash function) that should create a unique hash value for each key.  The function must be chosen to minimize “collisions” where two keys end up with the same hash value, which creates the same problem as with duplicate keys.  A hash sort is quite efficient within those constraints",0
113,"There are a number of ways that sound can be sampled, stored, or generated digitally ,.  Here we will look at some of the most popular sound file formats. WAVThe WAV (wave) file format is specific to Microsoft Windows.  It essentially stores the raw sample data that represents the digitized audio content, including information about the sampling rate (which in turns affects the sound quality).  Since WAV files are not compressed, they can consume considerable disk space. AIFFAIFF stands for Audio Interchange File Format, and is specific to the Apple Macintosh and to Silicon Graphics (SGI) platforms.  Like WAV, it stores actual sound sample data.  A variant, AIFF-C, can store compressed sound. AUThe AU (audio) file format was developed by Sun Microsystems and is used mainly on UNIX systems, and also in Java programming. MIDIMIDI stands for Musical Instrument Digital Interface.  Unlike most other sound formats, MIDI files don’t represent sampled sound data.  Rather, they represent virtual musical instruments that synthesize sound according to complex algorithms that attempt to mirror the acoustic characteristics of real pianos, guitars, or other instruments.  Since MIDI is like a “score” for the virtual instruments rather than storing the sounds, it is much more compact than sampled sound formats.  MIDI is generally used for music composition rather than casual listening. MP3MP3 is actually a component of the MPEG (Moving Picture Expert Group) multimedia standard, and stands for MPEG1 Audio Layer 3.  It is now the most popular sound format, using compression to provide a balance of sound quality",0
114,"While spam can appear in any communications medium (including chat, instant messaging, and even blogs), the most prevalent type is e-mail spam, which costs U. S.  businesses billions of dollars a year in processing expenditures, lost time, and damage caused by malicious software (malware) for which spam can be either a delivery vehicle or an inducement.  In 2007 an estimated 90 billion spam messages were sent each day. The fundamental driving force of spam is the fact that, given one has Internet access, sending e-mail costs essentially nothing, no matter how many messages are sent.  Thus even if only a tiny number of people respond to a spam solicitation (such as for sexual-enhancement products), the result is almost pure profit for the spammer. Besides directly making fraudulent solicitations for products that are ineffective, counterfeit, or nonexistent, spam carries two other dangers: inducements to click to visit fake Web sites , and attachments containing viruses or other dangerous software ,",0
115,"Ways to stop the spread of spam include the following:, e-mail filtering software, using a combination of text analysis by keyword or statistical correlation , and lists of Internet locations (domains) associated with spamming; filtering can be done both by service providers and individual users, or collaboratively, tightening the technical requirements for messages to be accepted by mail servers (much spam has poorly formatted headers), improving techniques for blocking the viruses used by spammers to set up their bots—see computervirus and firewall, attempting to shut down the infrastructure that supports spam operations, such as hosts who allow e-mail, and sellers of spamming software and illicitly gathered address lists",0
116,"Spyware and adware are two pervasive threats to computer users.  Both are programs that are installed more or less surreptitiously, often accompanying an attractive-looking “free” software package or media download.  Depending on how widely it is defined, as many as eight out of 10 PCs may be infected by some sort of spyware.  Signs of infection can include the system slowing down or periodically freezing, Web browsers that fail to display the expected home page or search results, and the appearance of numerous unwanted pop-up windows (a sign of adware). Ranging from least to most harmful, spyware and adware can do the following:, Display annoying advertising that can clog up the screen or cover up information (some adware can also be spyware that uses information about the user to target advertising), Track Web browsing to provide information to sell to marketers ,, Obtain personal information for use in identity theft, Install keyloggers (programs that record keystrokes, such as passwords being entered) or other “back door” or “trojan” programs",0
117,"Growing concern about spyware has prompted the use of antispyware programs such as Ad-Aware and Spybot-Search & Destroy, as well as a free program from Microsoft.  Antispyware programs are also being included in popular security suites from companies such as Symantec and McAfee.  The programs work similarly to antivirus programs, watching for suspicious behavior or “signatures” matching known spyware or adware.  Depending on the program, the spyware can be blocked from executing at all or removed from the system. The software varies considerably in effectiveness, so users may have to run several different programs to completely remove an “infestation. ”Spyware has been generally given a lower priority than viruses or even spam.  When challenged, spyware makers generally claim that the user authorized its installation (at least implicitly) by installing the utility or other software that contains it.  Although antispyware legislation has been introduced in Congress, it has not passed as of mid-2008.  However, state officials such as former New York State Attorney General Eliot Spitzer successfully sued a spyware company, winning a $7. 5 million settlement",0
118,"A stack is simply a list that represents successive locations in memory into which data can be inserted.  The operation of a stack can be visualized as being rather like the springloaded platform onto which dishes are stacked for washing in some restaurants.  As each dish (number) is added, the stack is “pushed. ” Because only the item “on top” (the last one added) can be removed (“popped”) at any given time, a stack is described as a LIFO (last in, first out) structure.  (Note that this is different from a queue, where items can be added or removed from either end [see queue]. )Stacks are useful whenever nested items must be tracked.  For example, a procedure might call a procedure that in turn calls another procedure.  The stack can keep track of the parameters (as well as the calling address) for each pending procedure. Stacks can also be used to evaluate nested arithmetic expressions.  ",0
119,"Web users increasingly have access to such content as news broadcasts, songs, and even full-length videos.  The problem is that the user must receive the content in real time at a steady pace, not in sputters or jerks.  However, factors such as load on the Web server and network congestion between the server and user can cause delays in transmission.  One way to reduce the problem would be to compress the data ,.  However, excessive compression would compromise audio or picture quality to an unacceptable extent.  Fortunately, a technology called streaming offers a way to smooth out the transmission of large amounts audio or video content ,. When a user clicks on an audio or video link, the player software (or Web browser plug-in) is loaded and the transmission begins.  Typically, the player stores a few seconds of the transmission ,, so any momentary delays in the transmission of data packets will not appear as the data starts to play.  Assuming the rate of transmission remains sufficient, enough data remains in the buffer so that data can be “fed” to the playing software at a steady pace.  If, however, there is too much delay due to network congestion, the playback will pause while the player refills its buffer",0
120,"Founded in 1982, Sun Microsystems (NASDAQ symbol: JAVA) has played an important role in the development of computer workstations and servers, UNIX-based operating systems, and the Java programming language ,. During the 1980s, Sun was known mainly for its workstations for programmers and graphics professionals, running on its own SPARC series microprocessors.  However, 460? ? ? ? structured programmingby the 1990s the growing power of regular desktop PCs was reducing the need for special-purpose workstations.  As the Web grew starting in the 1990s, Sun’s line of multiprocessing Web servers became quite successful, though the “dotbust” of the early 2000s cut revenues. One of Sun’s founders was a key developer of UNIXsoftware ,.  Sun developed its own version of UNIX (SunOS) for its workstations in the 1980s, and then joined with AT&T to develop the widely used UNIX System V Release 4, which in turn became the basis for Sun’s new operating system, Solaris.  (Sun has also supported the use of Linux on its hardware. )",0
121,"Supercomputers are always more expensive and somewhat less reliable than standard computers, so they are used only when necessary.  As the power of standard computers continues to grow, applications that formerly required a multimillion-dollar supercomputer can now run on a desktop workstation (a good example is the creation of detailed 3D graphics). On the other hand, there are always applications that will soak up whatever computing power can be brought to bear on them.  These include analysis of new aircraft designs, weather and climate models, the study of nuclear reactions, and the creation of models for the synthesis of proteins.  The neverending battle of organizations such as the National Security Agency (NSA) to monitor worldwide communications and crack ever-tougher encryption also demands the fastest available supercomputers ,. ",0
122,"The term supply chain management was developed in the 1980s to refer to the systematic efforts to improve the efficiency and reliability of this vital business activity.  Although the details will vary with the industry, a supply chain can include the following activities:, obtaining the raw materials or components needed for the product, manufacturing finished products, marketing the product, distributing the product to retailers or other outlets, servicing the product and supporting customers, (increasingly) providing for the ultimate recycling or disposal of the product",0
123,"A system administrator is the person responsible for managing the operations of a computer facility to ensure that it runs properly, meets user needs, and protects the integrity of users’ data.  Such facilities range from offices with just a few users to large campus or corporate facilities that may be served by a large staff of administrators. The system administrator’s responsibilities often include:, setting up accounts for new users, allocating computing resources (such as server space) among users, configuring the file, database, or local area network (LAN) servers, installing new or upgraded software on users’ workstations, keeping up with new versions of the operating system and networking software, using various tools to monitor the performance of the system and to identify potential problems such as device “bottlenecks” or a shortage of disk space, ensuring that regular backups are made, configuring network services such as e-mail, Internet access, and the intranet (local TCP/IP network), using tools such as firewalls and virus scanners to protect the system from viruses, hacker attacks, and other security threats ,, providing user orientation and training, creating and documenting policies and procedures",0
124,"The systems analyst serves as the bridge between the needs of the user and the capabilities of the computer system.  The systems analyst goes into action when users request that some new application or function be provided (usually in a corporate computing environment). The first step is to define the user’s requirements and to prepare precise specifications for the program.  In doing so, the systems analyst is aided by methodologies developed by computer scientists over the last several decades ,.  Often flowcharts or other aids are used to help visualize the operation of the program ,. After communicating with the user, the systems analyst must then communicate with the programmers, helping them understand what is needed and reviewing their work as they begin to design the program.  Although the systems analyst may do little actual programming, he or she must be familiar with programming tools and practices.  This may make it possible to suggest existing software or components that could be adapted instead of undertaking the cost and time involved with creating a new program.  As a program is developed, systems analysts are often responsible for designing tests to ensure that the software works properly ,",0
125,"Applications programmers write programs to help users work better, while systems programmers write programs to help the computer itself work better ,.  Systems programmers generally work for companies in the computer industry that develop operating systems, network facilities, program language compilers and other software development tools, utilities, and device drivers.  However, systems programmers can also work for applications developers to help them interface their programs to the operating system or to devices ,. Modern operating systems are highly complex, so systems programmers tend to specialize in particular areas.  These might include device drivers, software development tools, program language libraries, applications programming interfaces (APIs), and utilities for monitoring system conditions and resources.  Systems programmers develop the infrastructure needed for networking, as well as multiple-processor computers and distributed computing systems.  Systems programmers also play a key role when an application program must be “ported” to a different platform or simply modified to run under a new version of the operating system",0
126,"As the name suggests, a tablet PC is a small computer about the size of a notebook (not to be confused with a “notebook PC,” which is a small, light laptop).  The user can write on the screen with a stylus to take notes (for similar functionality, see graphics tablet), draw, and make selections with stylus or fingertip. If the user writes on the screen, software converts the writing to the appropriate characters and stores them in a file ,.  As with some PDAs, there may also be a system of shorthand “gestures” that can be used to write more quickly.  Alternatively, the user can type with stylus or fingertips on a “virtual keyboard” displayed on the screen ,. A more versatile and natural interface is becoming available: “multitouch,” pioneered by the Apple iPhone and Microsoft Surface, can recognize multiple motions and pressure points simultaneously.  This allows the user to, for example, flick the finger to “turn a page” or use a pinching motion to “pick up” an object. Applications for tablet PCs include many PDA-type applications ,, field note taking, inventory, and other tasks that require a device that is not encumbering.  Because of its compactness, a tablet PC can also be a good reader for ebooks ,. ",0
127,"An interesting variant is the Internet tablet, best known in Nokia’s N-series.  These are smaller and lighter than a tablet PC.  The Nokia N810, for example, has a slide-out keyboard as well as a virtual screen keyboard.  The most notable feature is the Internet browser and related applications, such as e-mail and instant messaging, and built-in wireless connections ,.  Although there is no phone, Internet-based services such as Skype can be used to place calls, or a Bluetooth-equipped mobile phone.  The Nokia series uses a variant of Linux and can run a large variety of open-source applications",0
128,"Anyone who has seen computers in old movies is familiar with the row of large, freestanding tape cabinets with their spinning reels of tape.  The visual cue that the computer was running consisted of the reels thrashing back and forth vigorously while rows of lights flashed on the computer console.  Magnetic tape was indeed the mainstay for data storage in most large computers , in the 1950s through the 1970s. In early mainframes the main memory (corresponding to today’s RAM chips) consisted of “core”—thousands of tiny magnetized rings crisscrossed with wires by which they could be set or read.  Because core memory was limited to a few thousand bytes (kB), it was used only to hold the program instructions , and to store temporary working data while the program was running. The source data to be processed by the program was read from a reel of tape on the drive.  If the program updated the data (rather than just reporting on it), it would generally write a new tape with the revised data.  In large facilities a person called a tape librarian was in charge of keeping the reels of tape organized and providing them to the computer operators as needed. ",0
129,"By the 1990s, PC users generally used tapes only for making backups.  A typical backup tape drive uses DAT (digital audio tape) cartridges that hold from hundreds of megabytes to several gigabytes of data.  Most drives use a rotating assembly of four heads (two read and two write) that verify data as it’s being written.  As a backup medium, tape has a lower cost per gigabyte than disk devices.  It is easy to use and can be set up to run unattended (except for periodically changing cartridges). However, since tapes are written and read sequentially, they are not convenient for restoring selected files ,.  Many smaller installations now prefer using a second (“mirror”) hard drive as backup, using disk arrays , or using recordable CDs or optical drives for smaller amounts of data ,. Many large companies and government agencies have thousands of reels of tape stored away in their vaults since the 1960s, including data returned from early NASA space missions.  As time passes, it becomes increasingly difficult to guarantee that this archived data can be successfully read.  This is due both to gradual deterioration of the medium and the older data formats becoming obsolete ,. ",0
130,"Tcl includes a number of extensions that, for example, provide access to popular database formats such as MySQL and can interface with other programming languages such as C++ and Java.  The most widely used extension is Tk, which provides a library for creating user interfaces for a variety of operating systems and languages such as Perl, Python, and Ruby. Tcl has been described as a “glue” to connect existing applications.  It is relatively easy to write and test a script interactively (often at the command line), and then insert it into the code of an application.  When the application runs, the Tcl interpreter runs the script, whose output can then be used by the main application ,",0
131,"The header information also includes:, The total length of the packet.  In theory packets can be as large as 65 kbytes; in practice they are limited to a smaller maximum. , An identification number that can be used if a packet is broken into smaller pieces for efficiency in transmission.  This allows the packet to be reassembled at the destination. , A “time to live” value that specifies how many hops (movements from one intermediate host to another) the packet will be allowed to take.  This is reduced by 1 for each hop.  If it reaches 0, the packet is assumed to have gotten “lost” or stale, and is discarded. , A protocol number (the protocol is usually TCP, see below). , A checksum for checking the integrity of the header itself (not the data in the packet). , The source and destination addresses. The source and destination are given as IP addresses, which are 32 bits long and typically written as four sets of up to three numbers each—for example, 208. 162. 106. 17",0
132,"Most software companies now have Web sites that include a support section that offers services such as, Frequently Asked Questions (FAQ) files with answers to common problems. , A searchable “knowledge base” of articles relating to various aspects of the software, including compatibility with other products, operating system issues, and so on. , Forms or e-mail links that can be used to submit questions to the company.  Typically questions are answered in one or two working days. , A bulletin board where users can share solutions and tips relating to the software. Web sites for publications such as PC Magazine and ZDNet also offer articles and other resources for working with the various versions of Microsoft Windows and popular applications. ",0
133,"As millions of people became new computer users during the 1980s, a thriving computer book publishing industry offered users a more user-friendly approach than that usually provided in the manuals issued by the software companies.  The “Dummies” books, offering bite-sized servings of information written in a breezy style and accompanied by cartoons, eventually spread beyond computers into hundreds of other fields and the format was then copied by other publishers.  Publishers such as Sams, Coriolis, and particularly O’Reilly have aimed their offerings at more experienced users, programmers, and multimedia developers. Computer trade books are often written by experienced developers and systems programmers who can offer advanced knowledge and “tips and tricks” to their less experienced colleagues.  Since many technical “gurus” are not experienced writers, the best results often come from collaboration between the expert and an experienced technical writer and/or editor who can review the material for completeness, organization, and clarity. In recent years there has been some contraction in the computer book industry.  This has arisen from several sources: improved on-line help included in products; the dominance of many applications areas by a handful of products; and fewer people needing beginner-level instructions",0
134,"Many technical writers work within software companies or in the information systems departments of other corporations, universities, or government agencies.  Their work is generally more highly structured than that of the manual or book writer.  As part of a development team, a technical writer may be in charge of creating documentation describing the data structures, classes, and functions within the program.  This task is aided by a variety of tools including facilities for extracting such information automatically from C++ or Java programs.  The writer may also be responsible for maintaining logs that show each change or addition made to the program during each compiled version or “build. ”This type of technical writing requires detailed knowledge of operating systems, programming languages, software development tools, and software engineering methodology.  It also requires the ability to work well as part of a team, often under conditions of high pressure. ",0
135,"Major policy issues involving information technology industries include:, foreign trade and the protection of intellectual property ,, attempts to reform the patent system to prevent what is seen as dubious and expensive litigation, the need for an increasing number of trained workers and providing a sufficient number of visas for foreign workers ,, preserving equal access to the Internet ,, which pits content providers against telecommunications companies, promoting the development of a next-generation Internet infrastructure (“Internet 2”)government support for computer research (such as through the National Science Foundation)—see government funding of computer research, favorable treatment of online businesses with regard to taxation (often objected to by traditional brickand-mortar businesses)—see e-commerce, laws against computer-related fraud and other crime ,, Privacy regulations ,The computer industry is also involved in issues that will affect its future over the longer term, such as the need to improve math and science education in elementary and high schools, energy and environmental policy, and issues such as health care and pensions that affect all sectors of the economy. ",0
136,"Prior to the court-ordered breakup of AT&T in 1984, the phone industry functioned in a monolithic way and was not very responsive to the needs of the growing computer networking industry. The breakup of AT&T led to growing competition, providing a wider variety of telecommunications equipment and lower phone rates just as PC users were starting to buy modems and sign up with online services and bulletin boards.  The growing deregulation movement in the 1990s (culminating in the Telecommunications Act of 1996) furthered this process by opening cable and broadcast television, radio, and other wireless communication to competition. With more than half of American Internet users on high-speed connections ,, the delivery of communications and media over the Net can only grow.  Wireless and mobile services (satellite, cell network, and 802. 11—see wireless computing) have also been growing vigorously.  The result is that the “information highway” now has many lanes, with some being express lanes. ",0
137,"Telecommuting (also called telework) is the ability to work from home or from some location other than the main office.  According to a report by the nonprofit organization WorldatWork, 28. 7 million people worked from home at least one day a month in 2006.  (Self-employed persons, of course, have a much higher rate of working from home. )Telecommuting was made possible by the growing capabilities of home computers and the availability of network connections that allow the worker at home to have access to most of the people and facilities that would be available if the worker were on site.  Workers and companies that promote telecommuting often cite the following advantages:, elimination of stressful, time-wasting commutes, workers may be more productive because they have fewer office distractions, unnecessary meetings, etc. , reduction of traffic, air pollution, and fuel costs, greater flexibility in working hours, the ability of working parents with small children to combine child care and work to some extent, reduction of costs associated with office facilities",0
138,"Some of the problems or disadvantages cited include:, Worker productivity may decrease due to lack of sufficient discipline and workers becoming distracted at home. ,? Managers may have trouble keeping track of or evaluating the activities of workers who are not physically present. , Telecommuters may miss critical information and go “out of the loop. ”, Security can be compromised, particularly through theft of laptops containing sensitive personal data. , Possible legal liabilities and application of OSHA rules to home working situations. ",0
139,"The alternative is to connect the remote participant to a mobile robot (this is sometimes called telerobotics).  Such robots already exist, although their capabilities are limited and they are not yet widely used for meetings.  RodneyBrooks, director of the MIT Artificial Intelligence Laboratory, foresees a not very distant future in which such robots will be commonplace. The robot will have considerable built-in capabilities, so the person who has “roboted in” to it won’t need to worry about the mechanics of walking, avoiding obstacles, or focusing vision on particular objects.  Seeing and acting through the robot, the person will be able to move around an environment as freely as persons who are physically present.  The operator can give general commands amounting to “walk over there” or “pick up this object” or perform more delicate manipulations by using his or her hands to manipulate gloves connected to a force-feedback mechanism. Brooks sees numerous applications for robotic telepresence.  For example, someone at work could “robot in” to his or her household robot and do things such as checking to make sure appliances are on or off, respond to a burglar alarm, or even refill the cat’s food dish.  Robotic telepresence could also be used to bring expertise (such as that of a surgeon) to any site around the world without the time and expense of physical travel.  Indeed, robots may be the only way (for the foreseeable future) that humans are likely to explore environments far beyond Earth ,. ",0
140,"The term template is used in a several contexts in computing, but they all refer to a general pattern that can be customized to create particular products such as documents. In a word processing program such as Microsoft Word, a template (sometimes called a style sheet) is a document that comes with a particular set of styles for various elements such as titles, headings, first and subsequent paragraphs, lists, and so on.  Each style in turn consists of various characteristics such as type font, type style (such as bold), and spacing.  The template also includes properties of the document as a whole, such as margins, header, and footer. To create a new document, the user can select one of several built-in templates for different types of documents such as letters, faxes, and reports, or design a custom template by defining appropriate styles and properties.  Special sequences of programmed actions can also be attached to a template ,. Templates can be created and used for applications other than word processing.  A spreadsheet template consists of appropriate macros and formulas in an otherwise blank spreadsheet.  When it is run, the template prompts the user to enter the appropriate values and then the calculations are performed.  A database program can have input forms that serve in effect as templates for creating new records by inputting the necessary data. ",0
141,"The primary use of text editors today is to create programs and scripts.  These must generally be created using only standard ASCII characters ,, without all the embedded formatting commands and graphics found in word processing documents.  Programmer’s text editors can be very sophisticated in their own right, providing features such as built in syntax checking and formatting or (as with the Emacs editor) the ability to program the editor itself.  Ultimately, however, program editors must create a source code file that can be processed by the compiler. Text editors are also useful for writing quick, short scripts , and can be handy for writing HTML code for the Web.  However, many Web pages are now designed using word processor–like programs that convert the WYSIWYG (what you see is what you get) formatting into appropriate HTML codes automatically",0
142,"Although they use different devices and formats, text messaging on cell phones and PDAs and instant messaging through online services have much in common.  Both involve sending short messages to other users who can receive them and reply as soon as they are online.  (This is an ad hoc connection that differs from a chat room [see chat, online] in that the latter is an established location where people go to converse with other members.  It also differs from an online discussion group [see conferencing systems and netnews and newsgroups] where messages are posted and may be replied to later, but there is no real-time communication. )Text messaging or texting uses a protocol called Short Message Service (SMS), which is available with most cell phones and service plans as well as PDAs that have wireless connections.  When a user sends a message to a designated recipient, it goes to a service center where it is routed to the destination phone; if that phone is not connected, the message is stored and retried later.  Typically messages are limited to 160 characters, though up to six or so messages can be concatenated and treated as a longer message. While texting did not become popular until the late 1990s, instant messaging began in the 1970s as a way for multiple users on a shared computer or network (such as a UNIX system) to communicate in real time using commands such as send and talk (the latter being more conversational—see chat, online).  In the late 1980s and early 1990s, various dial-up services , provided for sending text messages (AOL was the first to use the term instant messages for its facility).  By the mid-1990s instant messaging was well established on the Internet, often employing a graphical user interface, as with ICQ and AOL Instant Messenger (AOL later acquired ICQ as well). ",0
143,"Between 2000 and 2004 the numbers of text messages sent worldwide soared from 17 billion to 500 billion.  At about a dime a message, texting became a major source of revenue for phone companies.  Since then, texting has continued to grow, particularly in parts of Europe, the Asia-Pacific region (particularly China), and Japan (where it has largely become an Internet-based service). In the United States texting is most popular among teenagers ,.  It is not uncommon to see a bench full of teens talking excitedly to one another while carrying on simultaneous texting with unseen friends in what, to many adult onlookers, appears to be an incomprehensible code, their conversation perhaps ending with ttyl (talk to you later). Loosely affiliated groups communicating by text , have organized everything from “happenings” to serious protest campaigns (as in the anti-WTO [World Trade Organization] demonstrations in Seattle in 1999 and in the Philippines uprising in 2001. )",0
144,"As the name implies, a touchscreen is a screen display that can respond to various areas being touched or pressed.  Invented in 1971, the first form of touchscreens to become part of daily life were found on automatic teller machines (ATMs) and point-of-sale credit card processors. Touchscreens can detect the pressure of a finger or stylus in several ways: A “resistive” touchscreen uses two layers of electrically conductive metallic material separated by a space.  When an area is touched, the two layers are electrically connected, and the change in electrical current is registered and converted to a code that identifies the location touched.  Surface acoustic wave (SAW) touchscreens use an ultrasonic wave that is interrupted by a touch; capacitive touchscreens respond to the change in electron storage (capacitance) caused by contact with a human body.  Various other acoustic, mechanical (strain-based), or optical systems can also be used, with the latter being particularly popular",0
145,"Many computer applications involve the arrival of a set of data that must be processed in a specified way.  For example, a bank’s ATM system receives a customer’s request to deposit money together with identification of the account and the amount to be deposited.  The system must accept the deposit, update the account balance, and return a receipt to the customer.  This is an example of real-time transaction processing. Some applications process transactions in batches.  For example, a company may run a program once a month that generates paychecks and withholding stubs from employee records that include hours worked, number of dependents claimed, and so on.  Indeed, in the ATM example, the account balance is typically not updated during the on-line transaction, but instead a batch transaction is stored.  Overnight that transaction will be processed together with other transactions affecting that account (such as checks), and the balance will then be officially updated.  (The program module that keeps track of the progress of transactions is called a transaction monitor. )",0
146,"The tree is a data structure that consists of individual intersections called nodes.  The tree normally starts with a single root node.  (Unlike real trees, data trees have their root at the top and branch downward. ) The root connects to one or more nodes, which in turn branch into additional nodes, often through a number of levels.  (A node that branches downward from another node is called that node’s childnode. ) A node at the bottom that does not branch any further is called a terminal node or sometimes a leaf. Trees are useful for expressing many sorts of hierarchical structures such as file systems where the root of a disk holds folders that in turn can hold files or additional folders, and so on down many levels.  (A corporate organization chart is a noncomputer example of a hierarchical tree. )",0
147,"A binary tree is a tree in which no node has more than two child nodes.  To move through data stored in a binary tree, a program can use two pointers, one to the current node’s left child and one to its right child ,.  The pointers can then be used to trace the paths through the nodes.  If the tree represents a file that has been sorted ,, comparing nodes to the desired value and branching accordingly quickly leads to the desired record. Alternatively, the data can be stored directly in contiguous memory locations corresponding to the successive numbers of the nodes.  This method is faster than having to trace through successive pointers, and a binary search algorithm can be applied directly to the stored data.  On the other hand it is easier to insert new items into a linked list ,. A common solution is to combine the two structures, storing the linked list in a contiguous range of memory by storing its root in the middle of the range, its left child at the beginning of the range, its right child at the end, and then repeatedly splitting each portion of the range to store each level of children.  Intuitively, one can see that algorithms for processing such stored trees will take a recursive approach ,",0
148,"The more than five-century-old art of typography (the design, arrangement, and setting of printing type) was transformed in the latter part of the 20th century by digital technology.  With the exception of some traditional presses devoted to the fine book market, nearly all type used today is designed and set by computer. Most users are familiar with the typefaces distributed with their operating system and software, such as the popular Adobe and TrueType ,.  Many such font designs are based on (and sometimes named after) traditional typefaces, modified for readability using typical displays and printers. For control of composition, there are three overlapping levels of software, ranging from easiest to use (but most limited) to most complex, versatile, and precise.  Modern word processors such as Microsoft Word and Open Office provide enough control for many types of shorter documents ,.  Desktop publishing software adds facilities suitable for layout of fliers, brochures, newsletters, and similar publications that often mix text and graphics ,. More elaborate documents such as books, magazines, and newspapers require more sophisticated facilities to control the layout and flow of text.  Some traditional choices include LaTex (for the Tex typesetting program), used particularly by scientists and other academics, and the older troff and its offshoots on UNIX systems.  More recent programs include Quark, FrameMaker, PageMaker, and InDesign.  Related utilities often used in digital typography include font editors (for design and modification) and utilities to convert fonts from one format to another",0
149,"Ubiquitous (or pervasive) computing focuses not on individual computers and tasks but on a world where most objects (including furniture and appliances) have the ability to communicate information.  (This has also been called “the Internet of things. ”) This can be viewed as the third phase in a process where the emphasis has gradually shifted from individual desktops (1980s) to the network and Internet (1990s) to mobile presence and the ambient environmentSome examples of ubiquitous computing might include:, picture frames that display pictures attuned to the user’s activities, “dashboard” devices that can be set to display changing information such as weather and stock quotes, parking meters that can provide verbal directions to nearby attractions, kiosks or other facilities to provide verbal cues to guide travelers, such as through airports, home monitoring systems that can sense and deal with accidents or health emergencies",0
150,"The essential core of the UNIX system is the kernel, which provides facilities to organize and access files ,, move data to and from devices, and control the running of programs (processes).  In designing UNIX, Thompson deliberately kept the kernel small, noting that he wanted maximum flexibility for users.  Since the kernel was the only part of the system that could not be reconfigured or replaced by the user, he limited it to those functions that reliability and efficiency dictated be handled at the system level. Another way in which the UNIX kernel was kept simple was through device independence.  This meant that instead of including specific instructions for operating particular models of terminal, printers, or plotters within the kernel, generic facilities were provided.  These could then be interfaced with device drivers and configuration files to control the particular devices. A UNIX system typically has many users, each of whom may be running a number of programs.  The interface that processes user commands is called the shell.  It is important to note that in UNIX a shell is just another program, so there can be (and are) many different shells reflecting varying tastes and purposes ,.  Traditional UNIXshells include the Bourne shell (sh), C shell (csh), and Korn shell (ksh).  Modern UNIX systems can also have graphical user interfaces similar to those found on Windows and Macintosh personal computers ,. ",0
151,"Traditional print and broadcast media divide the world into two groups: content producers and content consumers.  However, as noted by its creator Tim Berners-Lee at its very beginning, the World Wide Web had at least the potential for users to take an active role in linking existing content and contributing their own ,.  Indeed, Berners-Lee wanted Web client software to include not only browsing functions but easy ways for users to create their own Web pages. In reality, early users faced something of a learning curve, usually having to cope with HTML to some extent, for example.  But by the mid-2000s a variety of new media of communication had become readily accessible using an ordinary Web browser at sites that host the required software.  The most prominent applications are blogs , and wikis, particularly Wikipedia ,. Meanwhile, inexpensive digital still and video cameras and easy-to-use editing software encouraged people to make their own media creations.  Sites to enable users to upload, share, and comment on their creations have flourished ,. The growth of sites such as Facebook and MySpace , has also provided new ways for users from junior high school age on up to create and share content",0
152,"The marketplace has spoken, and the desktop GUI is now the mainstream interface for most ordinary PC users.  However, there are a variety of other interfaces that are used for particular circumstances or applications, such as:, touchscreens (as with ATMs) ,, handwriting or written “gesture” recognition, such as on handheld computers , or for drawing tablets, voice-controlled systems ,, trackballs, joysticks, and touchpads (used as mouse alternatives), virtual reality interfaces using head-mounted systems, sensor gloves, and so on ,Because much interaction with computers is now away from the desktop and taking place on laptops, handheld, or palm computers, and even in cars, there is likely to be continuing experimentation with user interface design. ",0
153,"Dating back to the mid-1990s, VBScript is a scripting language developed by Microsoft and based on its popular Visual Basic programming language ,.  It is also part of the evolution of what Microsoft called “active scripting,” based on components that allow outside access to the capabilities of applications.  The host environment in which scripts run is provided through Windows (as with Windows Script Host) or within Microsoft’s Internet Explorer browser. For client-side processing, VBScript can be used to write scripts embedded in HTML pages, which interact with the standard Document Object Model , in a way similar to other Web scripting languages (in particular, see JavaScript).  However, VBScript is not supported by popular non-Microsoft browsers such as Firefox and Opera, so developers generally must use the widely compatible JavaScript instead.  VBScript can also be used for processing on the Web server, particularly in connection with Microsoft’s Web servers ,. Because versions of Windows starting with Windows 98 include Windows Script Host, VBScripts can also be written to run directly under Windows.  One unfortunate consequence was scripts containing worms (such as the I LOVE YOU worm) or other malware and mailed as attachments to unwary use",0
154,"When videotape first became available in the 1950s, recorders cost thousands of dollars and could only be afforded by TV studios.  Today the VCR is inexpensive and ubiquitous.  However, it is hard to edit videotape.  Tape is a linear medium, meaning that to find a given piece of video the tape has to be moved to that spot.  Removing or adding something involves either physically splicing the tape (as is done with film) or more commonly, feeding in tape from two or more recorders onto a destination tape.  Besides being tedious and limited in capabilities, “linear editing” by copying loses a bit of quality with each copying operation. Today, however, it is easy to shoot video in digital form , or to convert analog video into digital form.  Digital video is a stream of data that represents sampling of the source signal, such as from the chargecoupled device (CCD) that turns light photons into electron flow in a digital camera or digital camcorder.  This process involves either software or hardware compression for storage and decompression for viewing and editing (such a scheme is called a CODEC for “compression/decompression”).  The most widely used formats include DV (Digital Video) and MPEG (Motion Picture Expert Group), which has versions that vary in the amount of compression and thus fidelity",0
155," A little later, bulletin boards and especially systems such as the WELL (Whole Earth ’Lectronic Link) based in the San Francisco Bay Area , provided long-term outlets for people to share information and interact on-line. Looking at the WELL, a writer named Howard Rheingold introduced the term virtual community in a 1993 book.  He explored the ways in which a sufficiently compelling and versatile technology encouraged people to form long-term contacts, form personal relationships, and carry out feuds.  When on-line, participants experience such a venue as the WELL as a place that becomes almost as tangible (and often as “real”) as a physical place such as a small town or corner bar. Virtual community members who live in the same geographical area sometimes do get together physically (the WELL has had picniclike “WELL Office Parties” for many years).  Members can band together to support a colleague who faces a crisis such as the life-threatening illness of a son (on the WELL, blank postings called beams are often used as an expression of sympathy).  The virtual community can also serve as a rallying point following a physical disaster such as the 1989 earthquake in the San Francisco Bay Area.  On a daily basis, virtual communities can often provide help or advice from a remarkable variety of highly qualified experts. ",0
156," the computing field, virtualization involves the creation of a working model or representation of one system within a different system.  This idea has been widely used in the field since the 1960s.  Some applications of virtualization include:, An appropriate model of a system (such as a programming framework—see application programming interface) that hides unneeded details can make it easier for programmers to understand and access its functions ,. , A compiler for a language that compiles all programs to an intermediate representation (such as “bytecode”).  A virtual machine running on each kind of platform can then run the code, taking care of the details required by the host hardware ,. , A virtual machine created in software can be designed to perform all the functions available on a particular hardware platform or operating system, allowing software to be run on a system different from the one for which it was originally written ,.  For example, there are a number of virtualization programs (such as VMWare for PCs) that can create separate areas in memory, each running a different operating system, such as a version of Windows or Linux. ,? Multiple processors or entire computers can be treated as a single entity for processing a program, with software designed to assign threads of execution to physical processors and to coordinate the use of shared data ,. , A physical device such as a disk drive can be made to appear as several separate devices to the operating system (for better organization of data).  Similarly, many servers can run on the same physical machine.  Conversely, multiple drives can appear to be a single logical device while providing redundancy and error recovery ,. , A secure “virtual private network” can be created within the larger public Internet.  The virtual system takes care of encrypting and transmitting data through the physical network. ",0
157,"The concept of virtualization can also be applied to how work involving computers is being conceptualized and organized in the modern world ,.  A “virtual office” or even “virtual corporation” is a business entity that is not tied to a physical location, but uses networks, communications technology, and facilities such as video conferencing to keep workers in touch.  Alternatively, several organizations can share the same physical space (such as for mail or shipping) while maintaining their separate identities. Similarly, people can form long-lasting social networks while meeting physically seldom (if at all)—see socialnetworking and virtual community",0
158,"Besides military training, currently the most viable application for VR seems to be entertainment.  VR techniques have been used to create immersive experiences in elaborate facilities at venues such as Disneyland and Universal Studios, and to some extent even in local arcades.  VR that is accompanied by convincing physical sensations has allowed for the creation of a new generation of roller coasters that if built physically would be too expensive, too dangerous, or even physically impossible. However, there are other significant emerging applications for VR.  When combined with telerobotic technology ,, VR techniques are already being used to allow surgeons to perform operations in new ways.  VR technology can also be used to make remote conferencing more realistic and satisfactory for participants.  Clearly the potential uses for VR for education and training in many different fields are endless.  VR technology combined with robotics could also be used to give disabled persons much greater ability to carry out the tasks of daily life. In the ultimate VR system, users will be networked and able to simultaneously experience the environment, interacting both with it and one another.  The technical resources and programming challenges are also much greater for such applications.  The result, however, might well be the sort of environment depicted by science fiction writers such as William Gibson ,",0
159,"The basic idea of VoIP is simple: the Internet can carry packets of any sort of data ,, which means it can carry the digitized human voice as well, carrying ordinary phone calls.  There are several ways to do this:, a regular phone plus an adapter that connects to the computer and compresses and converts between regular analog phone signals and the digital equivalent, a complete “IP phone” unit that includes all needed hardware and software—no computer needed, just a network connection, such as to a router, use of the computer’s own sound card and speakers with a microphone, plus software (often free)Using that last option, VoIP service can be essentially free, regardless of distance.  However, one can only call someone who is currently connected to the Internet and also has VoIP software",0
160,"At least as currently implemented, VoIP does have some disadvantages:, Like cordless phones (but unlike traditional phones), VoIP requires that the user be connected to power.  This may make the system unavailable in an emergency. , Also, in an emergency, a 911 operator has no way to know where the caller is located geographically.  This could be a problem if the caller is unable to provide this information. , While a regular phone is a pretty simple device, VoIP requires special hardware or a PC, which might fail. , VoIP requires a working Internet connection—in practice, a high-speed connection ,.  Load or instability in the network could cause interruptions in calls or a lowering of voice quality. , As with other data sent over the Internet, there are potential security concerns.  Encryption can be used to secure VoIP calls, but this in turn leads to concerns by law enforcement agencies seeking to implement eavesdropping warrants. ",0
161,"For some time, technology pundits have talked about computers being literally woven into daily life, embedded in clothing and personal accessories.  However, implementations have thus far seen only limited use.  For example, watches with limited computer functions , have not proven popular—a watch large enough for input and display of information would likely be too bulky for comfort.  (People have also walked about with attached webcams, although the novelty seems to have quickly worn off. )",0
162,"Although the most important part of Web 2. 0 is its business and social models, a number of Web technologies are needed to provide the flexibility and rich interaction needed to offer a new Web experience.  These include:, dynamic, efficient generation of content ,, programming interfaces , using structured text files ,, platforms for running applications in the browser, such as Google apps, merging and customizing content from different sources ,, user subscription to content platforms for user-created content and collaboration",0
163,"Some typical features of a modern Web browser include, navigation buttons to move forward and back through recently visited pages, tabs to switch between Web pages, a “history” panel allowing return to pages visited in recent days, a search button that brings up the default search engine (which can be chosen by the user), the ability to save page as “favorites” or “bookmarks” for easy retrieval",0
164,"Today a Web user can view a live news broadcast, listen to music from a radio station, or view a document formatted to near-print quality.  All these activities are made possible by “helper” software , that gives the Web browser the capability to load and display or play files in special formats.  Examples include the Adobe PDF (Portable Document Format) reader, the Windows Media Player, and RealPlayer for playing video and audio content ,. What makes the browser even more versatile is the ability to load and run programs from Web sites ,.  Java was highly touted starting in the mid-1990s, and some observers believed that by making Web browsers into platforms capable of running any sort of software, there would be less need for proprietary operating systems such as Microsoft Windows",0
165,"Thousands of real-time views of the world are available on the Web.  These include everything from the prosaic (a coffee machine at MIT) to the international (a view of downtown Paris or Tokyo) to the sublime (a Rocky Mountain sunset).  All of these views are made possible thanks to the availability of inexpensive digital cameras ,. To create a basic webcam, the user connects a digital camera to a PC, usually via a USB cable.  A program controls the camera, taking a picture at frequent intervals (perhaps every 30 seconds or minute).  The picture is received from the camera as a JPG (JPEG) file.  The program then uploads the picture to the user’s Web page (usually using file transfer protocol, or ftp), replacing the previous picture.  Users connected to the Web site can click to see the latest picture.  Alternatively, a script running on the server can update the picture automatically. ",0
166,"The Web filter examines requests made by a Web user , and blocks those associated with sites deemed by the filter user to be objectionable.  There are two basic mechanisms for determining whether a site is unsuitable.  The first is to check the site’s address (URL) against a list and reject a request for any site on the list.  (Most filter programs come with default lists; the filter user can add other sites as desired.  Generally, the filter is installed with a password so only the authorized user [such as a parent] can change the filter’s behavior. )The other filtering method relies on a list of keywords associated with objectionable activities (such as pornography).  When the user requests a site, the filter checks the page for words on the keyword list.  If a matching word or phrase is found, the site is blocked and not shown to the user. ",0
167,"There are many online services (including some free ones) that will provide users with personal Web pages.  There are also programs such as Microsoft FrontPage that allow users to design Web pages by arranging objects visually on the screen and setting their properties.  However, creating and maintaining a complete Web site with its many linked pages, interactive forms and interfaces to databases and other services is a complicated affair.  For most moderate to large-size organizations, it requires the services of a new category of IT professional: the webmaster",0
168,"understanding how the Web site responds to and manages requests ,, fluency in the basic formatting of text and other page content and the use of frames and other tools for organizing and presenting text ,, extended formatting and content organization facilities such as Cascading Style Sheets (CSS), Dynamic HTML (DHTML), and Extensible Markup Language ,, use of graphics formats and graphics and animation programs (such as Photoshop, Flash, and DreamWeaver), extending the interactivity of Web pages through writing scripts using tools such as JavaScript and PHP ,, dealing with platform and compatibility issues, including browser compatibility",0
169,"When a user types in (or clicks on) a link in the browser window, the browser sends a HTTP request ,.  To construct the request, the browser first looks at the address (URL) in the user request.  An address such as http://www. well. com/conferencing. html consists of three parts In order to direct the browser’s request to the appropriate host and server, the browser sends the URL to a name server ,.  The name server provides the appropriate numeric IP address ,.  The browser then sends an HTTP “get” request to the server’s IP address. Assuming the page requested is valid, the server sends the HTML file to the browser.  The browser in turn interprets the formatting and display instructions in the HTML file and “renders” the text and graphics appropriately.  It is remarkable that this whole process from user click to displayed page usually takes only a few seconds, even if the Web site is thousands of miles away and requests must be relayed through many intervening computers",0
170,"There are several ways in which Web services can be accessed:, Remote Procedure Call (RPC), which generally uses WSDL and follows a format similar to the traditional way programs call upon library functions, An organization based on the available messages rather than calls or operations ,, Representational State Transfer (REST), which views applications or services as collections of “resources” with specific addresses (URLs) and specific requests using HTTPA variety of other specifications and approaches can be used; this area is a very fluid one.  Fortunately, programmers and even users , can build new Web applications without having to know the details of how the underlying services work. ",0
171,"Wiki software varies in details such as use of markup languages, programming interface, and platform.  However, most wikis include the following features:, Users can create new pages (articles) or edit existing ones. , Pages contain links to related pages, sometimes using “wiki words” where WordsAreScrunchedTogetherWithIntialCaps. , Simple markup can be used to create such effects as boldface, headings, or lists.  The wiki software usually translates this to HTML for rendering. , A record is kept of each contribution or edit, often displayed on a “Recent Changes” page. ,? Many wikis use a database (such as MySQL) to store and retrieve pages.  Some wikis simply store each page as a file, and a few (such as TiddlyWiki) store all pages together as a single document. , Wikis can be public (open to anyone) or restricted, such as to members of an organization. , The administrator of the wiki establishes guidelines or standards (such as for citing sources for facts) and procedures for dealing with disputes and controversial topics",0
172,"Typically, a wireless LAN uses a frequency band with each unit on a slightly different frequency, thus allowing all units to communicate without interference.  (Although radio frequency is now most popular, wireless LANs can also use microwave links, which are sometimes used as an alternative to Ethernet cable in large facilities. )Usually there is a network access point, a PC that contains a transceiver and serves as the network hub (it may also serve as a bridge between the wireless network and a wired LAN).  The hub computer can also be connected to a high-speed Internet service via DSL or cable.  It has an antenna allowing it to communicate with wireless PCs up to several hundred feet away, depending on building configuration. Each computer on the wireless network has an adapter with a transceiver so it can communicate with the access point.  The adapter can be built-in (as is the case with some handheld computers), or mounted on a PC card (for laptops) or an ISA card (for desktop PCs) or connected to a USB port. ",0
173,"The term word processor was actually coined by IBMin the 1960s to refer to a system consisting of a Selectric typewriter with magnetic tape storage.  This allowed the typist to record keystrokes (and some data such as margin settings) on tape.  Material could be corrected by being rerecorded.  The tape could then be used to print as many perfect copies of the document as required.  A version using magnetic cards instead of tape appeared in 1969. The first modern-style word processor was marketed by Lexitron and Linolex.  It also used magnetic tape, but it added a video display screen.  Now the writer could see and correct text without having to print it first.  A few years later, a new invention, the floppy disk, became the standard storage medium for dedicated word processing systems. The word-processing systems developed by Wang, Digital Equipment Corporation, Data General, and others became a feature in large offices in the late 1970s.  These systems were essentially minicomputers with screens, keyboards, and printers and running a specialized software program.  Because these systems were expensive (ranging from about $8,000 to $20,000 or more), they were not affordable by smaller businesses.  Typically, they were operated by specially trained personnel (who became known also as “word processors”) to whom documents were funneled for processing, as with the old “typing poo",0
174,"Some typical features today include:, different views of the document, including an outline showing headings down to a user-specified level, automatic table of contents and index generation, tables and multicolumn text, automatic formatting of bulleted and numbered lists, built-in and user-defined styles for headings, paragraphs, and so on. , the ability to use built-in or user-defined templates to provide starting settings for new documents ,, the ability to record or otherwise specify a series of commands to be performed automatically ,, spelling and grammar checkers, the ability to incorporate a variety of graphics image formats in the document, automatic formatting and linking of Web hyperlinks within documents, the ability to import and export documents in a variety of formats, including Web documents ",0
175,"Zuse was born on June 22, 1910, in Berlin.  He stud-ied civil engineering at the Technische Hochschule Berlin-Charlottenburg, receiving his degree in 1935.  One of histasks in engineering was performing calculations of thestress on structures such as bridges.  At the time these cal-culations were carried out by going through a series of stepson a form over and over again, plugging in the data andcalculating by hand or using an electromechanical calcula-tor.  Like other inventors before him, Zuse began to wonderwhether he could build a machine that could carry outthese repetitive steps automatically.  departing from other calculator designers). The Z1 had trouble storing and retrieving numbers andnever worked well.  Undeterred, Zuse began to develop anew machine that used electromechanical telephone relays(a ubiquitous component that was also favored by HowardAiken [see Aiken, Howard]).  The new machine workedmuch better, and Zuse successfully demonstrated it at theGerman Aerodynamics Research Institute in 1939. With World War II under way, Zuse was able to obtainfunding for his Z3, which was able to carry out automaticsequences from instructions. Zuseused spare time from his military duties at the Henschelaircraft company to work on the Z4, which was completedin 1949. ",0
176,"Since the late 1990s, Web users (particularly younger ones)have been adept at sharing media content online ,.  In the 2000 decade, however,the emphasis has shifted to users not merely sharing otherpeoples’ content, but creating their own ,.  The first part of the recipe was the availability ofubiquitous digital cameras and camcorders; the second partwas easy-to-use video-editing software; and the third partwas a Web site that could host the results. Created in 2005 by three former PayPal employees, thevideo-sharing site YouTube has been the leading venue foramateur video.  Although available content includes clipsfrom movies and TV shows (some unauthorized), much ofthe most interesting content is original videos created anduploaded by users.  Beyond just sharing or accessing con-tent, users are encouraged to rate and comment on the vid-eos they see, and users can also subscribe to “feeds” of newmaterial that is likely to be of interest to them. By 2008 more than 83 million videos were available onYouTube—and hundreds of thousands added each day. ",0
177,"YouTube broke into the highly visual field ofpolitical advertising.  Most candidates in the 2008 presiden-tial primaries have put their statements and other videos onYouTube. ups).  Political commentators and journalists have also beenactive in putting their opinions on YouTube (or comment-ing on those of others).  Perhaps the political establishment’sbiggest nod to YouTube is the series of debates cosponsoredby CNN and YouTube, bringing together the Republicanand Democratic primary fields. YouTube has had its share of criticism: Critics havecharged the service with not sufficiently policing copyrightviolations and violent content (including videos of fightsor bullying in schools), as well as neo-Nazi propaganda,scenes of animal abuse, and videos by anti-American insur-gent groups, as well as generally tasteless exhibitionism.  Afew countries and some schools have responded by block-ing access to the service. ",0
178,"Yahoo!has played an importantrole in the development of Web services.  In 1994 Stan-ford students Jerry Yang and David Filo developed the firstpopular directory of Web sites.  Realizing thatthe millions of Web users flocking to their site provided anopportunity for advertising and services, the two partnersincorporated Yahoo! in 1995. Yahoo! continued to grow, and the company acquireda number of other online services, which they used to pro-vide Web-based e-mail, Web hosting, and news.  But havingflown so high, Yahoo! had far to fall when the dot-com mar-ket bubble burst in 2001: A stock that had traded at around$130. 00 per share fell as low as $4. 06. However, Yahoo! proved its resilience as one of the fewearly dot-coms to survive and has continued to thrive inthe post-bubble era since 2002.  The company made strate-gic partnerships with telecommunications companies suchas BT and Verizon.  Yahoo! entered a continuing strugglewith another Web services powerhouse , whileacquiring new media sites (such as the photo-sharing ser-vice Flickr and the social “bookmarking” service del. icio. us), and creating new services ,.  Yahoo! also provides onlinestorefronts, competing in that venue mainly with eBay. ",0
179,"Computers and technology play a role in the lives of mostyoung people that many adults have difficulty compre-hending.  Children in industrialized countries are liable toencounter video games even before they arrive at school. Once there, they will be exposed to a considerable amountof educational software, depending on their school’s afflu-ence ,.  Upon returningfrom school, there are more sophisticated games, MySpacepages to keep updated ,, sophisti-cated tools for creating music and video, and, of course, theInternet in all its vast diversity.  Meanwhile, a web of inces-sant messages , islikely to keep the youngster in touch with friends. ",0
180,"In recent years there has been growing concern that Internet users  may  eventually  be  treated  differently  by  service  pro-viders  depending  on  the  kind  of  data  they  download  or  the kind  of  application  programs  they  use  online.   Advocates  of network (or net) neutrality , want legislation that would bar cable, DSL, or other provid-ers , from making  such  distinctions,  such  as  by  charging  content  pro-viders higher fees for high volumes of data or even blocking certain applications.  Advocates of net neutrality believe that, since there are rather limited choices for broadband Internet service,  discrimination  on  the  basis  of  Web  content  could lead to a loss of freedom for consumers and providers alike. ",0
181,"NLP is a multidisciplinary field that draws from linguis-tics and computer science, particularly artificial intelligence ,.  In terms of linguistics, a program must be able to deal with words that have multiple mean-ings (“wind up the clock” and “the wind is cold today”) as well as grammatical ambiguities (in the phrase “little girl’s school” is it the school that is little, the girls, or both?).  Of course each language has its own forms of ambiguity. Programs can use several strategies for dealing with these problems, including using statistical models to predict the likely meaning of a given phrase based on a “corpus” of existing text in that language ,. As formidable as the task of extracting the correct (lit-eral) meaning from text can be, it is really only the first level of natural language processing.  If a program is to success-fully summarize or draw conclusions about a news report from North Korea, for example, it would also have to have a knowledge base of facts about that country and/or a set of “frames” , about how to interpret vari-ous situations such as threat, bluff, or compromise. )",0
182,"Users of modern operating systems such as microsoft Win-dows are familiar with multitasking, or running several programs at the same time.  For example, a user might be writing a document in a word processor, pause to check the e-mail program for incoming messages, type a page address int o a Web browser, then return to writing.  meanwhile, the operating system may be running a number of other pro-grams tucked unobtrusively into the background, such as a virus checker, task scheduler, or system resource monitor. Each running program “takes turns” using the PC’s cen-tral processor.  In early versions of Windows, multitasking was cooperative, with each program expected to periodically yield the processor to Windows so it could be assigned to the next program in the queue.  One weakness of this approach is that if a program crashes, the CPU might be “locked up” and the system would have to be rebooted.  However, Win-dows NT, 2000, and xP (as well as operating systems such as UNIx) use preemptive multitasking.  The operating sys-tem assigns a “slice” of processing (CPU) time to a program and then switches it to the next program regardless of what might be happening to the previous program.  Thus, if a pro-gram  “crashes,”  the  CPU  will  still  be  switched  to  the  next program,  and  the  user  can  maintain  control  of  the  system and shut down the offending program. ",0
183,"Microsoft . NET is a programming platform , that is intended to pro-vide a clear and consistent way for applications written in a variety of languages such as C++, C#, and Visual Basic to access Windows functions and to interact with other programs and services on the same machine or over the Internet. . NET consists of the following main parts:, Base Class Library of data types and common func-tions (such as file manipulation and graphics) that is available to all . NET languages, Common Language Runtime, which provides the code that applications need to run within the operat-ing system, manage memory, and so forth (“Common language” means it can be used for any . NET pro-gramming language. ), ASP . NET, a class framework for building dynamic Web applications and services (the latest version of ASP—see activeseRveRpages), ADO . NET, a class framework that allows programs to access databases and data services",0
184,"Whatever memory chips or other devices are installed in a computer,  the  operating  system  and  application  programs must  have  a  way  to  allocate,  use,  and  eventually  release portions of memory.  The goal of memory management is to use available memory most efficiently.  This can be difficult in  modern  operating  environments  where  dozens  of  pro-grams may be competing for memory resources. Early  computers  were  generally  able  to  run  only  one program at a time.  These machines didn’t have a true oper-ating  system,  just  a  small  loader  program  that  loaded  the application  program,  which  essentially  took  over  control of the machine and accessed and manipulated the memory.  Later systems offered the ability to break main memory into several  fixed  partitions.   While  this  allowed  more  than  one program to run at the same time, it wasn’t very flexible. ",0
185,"If  computers  were  merely  fast  sequential  calculators,  they would still be of some use.  However, much of the power of the  computer  comes  from  its  ability  to  carry  out  repetitive tasks  without  supervision.   The loop  is  the  programming language  structure  that  controls  such  activities.   Virtually every language has some form of loop construct, with vari-ations  in  syntax  ranging  from  the  relatively  English-like COBOL  and  Pascal  to  the  more  cryptic  C. ",0
186,"Starting in the 1980s, many organizations sought to connect their  employees’  desktop  computers  so  they  could  share central  databases,  share  or  back  up  files,  communicate  via e-mail,  and  collaborate  on  projects.   A  system  that  links computers  within  a  single  office  or  home,  or  a  larger  area such as a building or campus, is called a local area network (LAN).   (Larger  networks  linking  branches  of  an  organiza-tion  throughout  the  country  or  world  are  called  wide  area networks, or WANs.  See netWoRk. )",0
187,"Naturally  there  must  be  software  to  manage  the  transmis-sion and reception of data packets.  The structure of a packet (sometimes  called  a frame)  has  been  standardized  with  a preamble, source and destination addresses, the data itself, a  checksum,  and  two  special  layers  that  interface  with  the differing ways that Ethernet and token ring networks physi-cally handle the packets. ",0
188,"A list is a series of data items that can be accessed sequen-tially  by  following  links  from  one  item  to  the  next.   Lists can be very useful for ordering or sorting data items and for storing them on a stack or queue. There  are  two  general  approaches  to  constructing  lists.  In a data list used with procedural programming languages such  as  C,  each  list  item  consists  of  a  structure  consisting of a data member and a pointer.  The pointer, called “next,” contains the address of the next item.  A program can easily “step through” a list by starting with the first item, process-ing  its  data,  then  using  the  pointer  to  move  to  the  next item,  continuing  until  some  condition  is  met  or  the  end  of the list is reached. ",0
189,"Lists are generally used to provide convenient access to rel-atively  small  amounts  of  data  where  flexibility  is  required.  Unlike an array, a list need use only as much memory as it needs to accommodate the current number of items (includ-ing their associated pointers).  A LISP-style node list can be even  more  flexible  in  that  items  with  varying  sizes  and types of data can be included in the same list.  Lists are thus a  more  flexible  way  to  implement  such  things  as  look-up tables. ",0
190,"Linux  is  an  increasingly  popular  alternative  to  proprietary operating systems.  Its development sprang from two sources.  First was the creation of open-source versions of UNIx utili-ties  ,  by  maverick  programmer  Richard  Stallman as  part  of  the gNU  (“gnu’s  not  UNIx”)  project  during  the 1980s.  Although these tools were useful, the kernel, or basic set of operating system functions, was still missing ,.   Starting  in  1991,  another  creative  programmer,  Linus Torvalds, began to release open-source versions of the UNIx kernel  ,.   The  combination  of  the  ker-nel  and  utilities  became  known  as  Linux  (a  combination of Linus  and  UNIx),  though  Stallman  and  his  supporters believe that gNU/Linux is a more accurate name. ",0
191,"The  idea  behind  an  operating  system  kernel  is  that  there is  a  relatively  small  core  set  of  “primitive”  functions  that are necessary for the operation of system services ,.   These  functions  can  be  provided  in  a single component that can be adapted and updated as desir-able.  The fundamental services include:,    Process control—scheduling  how  the  processes  (pro-grams or threads of execution within programs) share the CPU, switching execution between processes, cre-ating  new  processes,  and  terminating  existing  ones ,. ,      Interprocess   communication—sending   “messages” between  processes  enabling  them  to  share  data  or coordinate their data processing. ,      memory   management—allocating   and   freeing   up memory  as  requested  by  processes  as  well  as  imple-menting  virtual  memory,  where  physical  storage  is treated as an extension of main (RAm) memory.  (See memoRymanagement. ),    File  system  services—creating,  opening,  reading  from, writing to, closing, and deleting files",0
192,"JavaScript  is  one  of  several  popular  languages  that  can enable  Web  pages  to  interact  with  users  more  quickly  and efficiently ,.  The  language  first  appeared  in  the  mid-1990s’  Netscape  2 browser under the name LiveScript.  Technically, JavaScript is  the  Sun microsystems  trademark  for  its  implementation of  a  standard  called  ECmAScript.   Despite  the  name,  Java-Script  is  not  directly  related  to  the  Java  programming  lan-guage. In  its  early  years  JavaScript  was  perhaps  a  victim  of  its own  success.   Having  a  relatively  easy-to-use  scripting  lan-guage  provided  an  easier  way  to  add  features  such  as  3D buttons  and  pop-up  windows  to  formerly  humdrum  Web forms.   However,  as  with  an  earlier  generation’s  fondness for multiple fonts, early JavaScript programmers were often prone  to  add  unnecessary  and  confusing  clutter  to  Web pages.   Besides  sometimes  annoying  users,  early  JavaScript also  suffered  from  significant  differences  in  how  it  was implemented  by  the  major  browsers.   As  a  result,  Netscape users  were  sometimes  stymied  by  JavaScript  written  for micr  osoft Internet Explorer, and vice versa.  Finally, browser flaws have sometimes allowed JavaScript to be used to com-promise  security  such  as  by  installing  malware-infested “browser helpers. ” As a result, many security experts began to  recommend  that  users  disable  JavaScript  execution  in their browsers. ",0
193,"Java  has  largely  fulfilled  this  promise  for  Web  developers.  C++  programmers  have  an  easy  learning  curve  to  Java, since  the  two  languages  have  very  similar  syntax  and  a similar use of classes and other object-oriented features.  On the  other  hand,  programmers  who  don’t  know  C++  benefit from  Java  being  more  streamlined  than  C++.   For  example, Java avoids the necessity to use pointers ,  and  uses  classes  as  the  consistent  building block of program structure.  Software powerhouses such as microsoft (until recently) and IBm have joined Sun in pro-moting Java. Another  much-touted  feature  of  Java  is  its  platform independence.  The language itself is separate from the vari-ous  operating  system  platforms.   For  each  platform,  a  Java Vir tual machine ( JVm) is created, which interprets or com-piles the code generated by the Java compiler so it can run on that platform. For  security,  Java  applets  run  within  a  “sandbox”  or restricted  environment  so  the  user  is  protected  from  mali-cious   Java   programs.    (For   example,   programs   are   not allowed  to  access  the  user’s  disk  or  to  connect  the  user’s machine to another Web site. ) Web browsers can also be set to disable the running of Java applets. ",0
194,"Java  is  a  computer  language  similar  in  structure  to  C++.  Although Java is a general-purpose programming language, it is most often used for creating applications to run on the Internet,  such  as  Web  servers.   A  special  type  of  Java  pro-gram called an applet can be linked into Web pages and run on the user’s Web browser ,. As  an  object-oriented  language,  Java  uses  classes  that provide commonly needed functions including the creation of user interface objects such as windows and buttons ,.   A  variety  of sets  of  classes  (“class  frameworks”)  are  available,  such  as the AWT (Abstract Windowing Toolkit). ",0
195,"An interpreter is a program that analyzes (parses) program-ming commands or statements in a high-level language ,,  creates  equivalent  executable instructions in machine code , and executes them.  An interpreter differs from a compiler in that the lat-ter converts the entire program to an executable file rather than processing and executing it a statement at a time . many  earlier  versions  of  the  BASIC  programming  lan-guage  were  implemented  as  interpreters.   Since  an  inter-preter only has to hold one program statement at a time in memory, it could run on early microcomputers that had only a  few  tens  of  thousands  of  bytes  of  system  memory.   How-ever,  interpreters  run  programs  considerably  more  slowly than a compiled program would run.  One reason is that an interpreter “throws away” each source code statement after it  interprets  it.   This  means  that  if  a  statement  runs  repeat-edly ,, it must be re-interpreted each time it runs.  A  compiler,  on  the  other  hand,  would  create  only  one  set of  machine  code  instructions  for  the  loop  and  then  move on.   Also,  because  a  compiler  keeps  the  entire  program  in memory,  it  can  analyze  the  relationship  between  multiple statements  and  recognize  ways  to  rearrange  or  substitute them for greater efficiency. ",0
196,"An  Internet  service  provider  is  any  organization  that  pro-vides  access  to  the  Internet.   While  nonprofit  organiza-tions  such  as  universities  and  government  agencies  can  be Internet service provider       considered  to  be  ISPs,  the  term  is  generally  applied  to  a commercial, fee-based service. Typically, a user is given an account that is accessed by logging in through the operating system’s Internet connec-tion facility by supplying a user ID and password.  Once con-nected, the user can run Web browsers, e-mail clients, and other programs that are designed to work with an Internet connection.  most  ISPs  now  charge  flat  monthly  fees  rang-ing  from  $20  or  so  for  dial-up  access  to  around  $40–$60 for  high-speed  cable  or  DSL  connections  ,.  Some  services  such  as  America  Online  and  CompuServe include  ISP  service  as  part  of  a  package  that  also  includes such  features  as  software  libraries,  discussion  forums,  and instant messaging.  Online services tend to be more expen-sive than “no frills” ISP services",0
197,"Internet  radio  is  the  provision  of  radio  broadcast  content over  the  Internet  ,.   Basically,  the  digitized sound  files  of  the  broadcasts  can  be  accessed  and  played using  widely  available  software  such  as  Windows media Player or RealPlayer.  Internet radio began in the mid-1990s, and  today  an  increasing  number  of  broadcast  stations  are offering their programming in this form, allowing them to reach  audiences  far  beyond  the  reach  of  their  signal.   Some stations  stream  live  (during  the  actual  broadcast),  while others  make  programs  available  for  download.   (For  auto-matic  downloading  of  broadcasts,  see podcasting).   There are also “radio stations” that provide their content only via the  Internet.   Internet  radio  should  not  be  confused  with satellite or cable radio, which carry conventional radio sig-nals in real time. ",0
198,"The  growth  of  the  Internet  and  its  centrality  in  business, education,  and  other  fields  has  led  many  programmers to  specialize  in  Internet-related  applications.   These  can include the following:,    low-level infrastructure  (networking  [wired  and  wire-less], routing, encryption support, and so on),    Web servers and related software,      e-commerce infrastructure ,,      interfacing with databases,      data analysis and extraction ,,      support for searching ,,      autonomous  software  to  navigate  the  net  ,,      Internet-based  communications  ,,      systems  to  deliver  text  and  media  ,,      support  for  collaborative  use  of  the  Internet  ,,      security software (firewalls, intrusion analysis, etc. )",0
199,"   Netnews  (also  called  Usenet,  for  UNIx  User  Net-work)  is  in  effect  the  world’s  largest  computer  bul-letin  board.   It  began  in  1979,  when  Duke  University and the University of North Carolina set up a simple mechanism for “posting” text files that could be read by  other  users.   Today  there  are  tens  of  thousands  of topical “newsgroups” and millions of messages (called articles).   Although  still  impressive  in  its  quantity  of content,  many  Web  users  now  rely  more  on  discus-sion  forums  based  on  Web  pages  ,. ",0
200,"    Ftp  (File  Transport  Protocol)  enables  the  transfer  of one  or  more  files  between  any  two  machines  con-nected  to  the  Internet.   This  method  of  file  transfer has  been  largely  supplanted  by  the  use  of  download links  on  Web  pages,  except  for  high-volume  applica-tions  (where  an  ftp  server  is  often  operated  “behind the  scenes”  of  a  Web  link).   FTP  is  also  used  by  Web developers  to  upload  file",0
201,"   Telnet  is  another  fundamental  service  that  brought the  Internet  much  of  its  early  utility.   Telnet  allows  a user at one computer to log into another machine and run a program there.  This provided an early means for users  at  PCs  or  workstations  to,  for  example,  access the  Library  of  Congress  catalog  online.   However,  if program  and  file  permissions  are  not  set  properly  on the  “host”  system,  telnet  can  cause  security  vulner-abilities.   The  telnet  user  is  also  vulnerable  to  having IDs and passwords stolen, since these are transmitted as  clear  (unencrypted)  text.   As  a  result,  some  online sites  that  once  supported  telnet  access  now  limit access to Web-based forms.  (Another alternative is to use a program called “secure shell” or ssh, or to use a telnet client that supports encryption. )",0
202,"  WAIS  (Wide  Area  Information  Service)  is  a  gateway that  allows  databases  to  be  searched  over  the  Inter-net.   WAIS  provided  a  relatively  easy  way  to  bring large  data  resources  online.   It,  too,  has  largely  been replaced by Web-based database services. ",0
203,"The Internet is the worldwide network of all computers (or networks of computers) that communicate using a particu-lar protocol for routing data from one computer to another ,.   As  long  as  the  programs  they  run  follow  the rules  of  the  protocol,  the  computers  can  be  connected  by a variety of physical means including ordinary and special phone  lines,  cable,  fiber  optics,  and  even  wireless  or  satel-lite transmission. ",0
204,"Internationalization  and  localization  are  ways  to  adapt computer  software  (often  created  in  the  United  States  or Europe) to other languages and cultures.  The abbreviations I18n and L10n are sometimes used for internationalization and  localization,  respectively  (the  numbers  in  each  word refer  to  the  number  of  letters  in  the  alphabet  between  the letters).  The two processes are complementary. ",0
205,"By  the  mid-2000  decade,  the  biggest  intellectual  property battles  were  not  about  esoteric  program  codes  but  rather revolved around how to satisfy the ordinary home consum-er’s  appetite  for  music  and  video  while  preserving  produc-ers’  revenues.   Increasingly,  music  and  even  video  is  being downloaded rather than being bought in commercial pack-aging at the local store. In  the Sony  v.   Universal  case  (1984)  the  Supreme  Court ruled  that  manufacturers  of  devices  such  as  VCRs  were not  liable  for  their  misuse  if  there  were  “substantial  non-infringing uses”—such as someone making a copy of legally possessed  media  for  their  own  use.   However,  in  2005  the Supreme  Court  ruled  that grokster,  a  decentralized  file-sharing service, could be held liable for the distribution of illegally copied media if it “actively induced” such copying. By   2006   media   industry   lobbyists   (particularly   the Recording  Industry  Institute  of  America,  or  RIAA)  were promoting  a  number  of  bills  in  Congress  that  would  fur-ther  restrict  consumers’  rights  to  use  media.   Such  mea-sures might include requiring that devices be able to detect “flagged”  media  and  refuse  to  copy  it  ,,  as  well  as  adding  stricter  provisions  to  the Digital millennium  Copyright  Act  (DmCA).   These  mea-sures  are  opposed  by  cyber-libertarian  groups  such  as  the Electronic Frontier Foundation and consumer groups such as the Home Recording Rights Coalition. ",0
206,"Intellectual  property  can  be  defined  as  the  rights  the  cre-ator  of  an  original  work  (such  as  an  invention  or  a  book) has  to  control  its  reproduction  or  use.   Developers  of  new computer  hardware,  software,  and  media  content  must  be able to realize a return on their time and effort.  This return is threatened by the ease with which programs and data on disks can be illicitly copied and redistributed.  Several legal mechanisms can be used to deter such behavior. ",0
207,"Information theory is the study of the fundamental charac-teristics of information and its transmission and reception.  As  a  discipline,  information  theory  took  its  impetus  from the ideas of Claude Shannon ,. In  his  seminal  paper  “A mathematical  Theory  of  Com-munication”  published  in  the Bell  System  Technical  Journalin 1948, Shannon analyzed the redundancy inherent in any form  of  communication  other  than  a  series  of  purely  ran-dom  numbers.   Because  of  this  redundancy,  the  amount  of information  (expressed  in  binary  bits)  needed  to  convey  a message  will  be  less  than  the  number  in  the  original  mes-sage.   It  is  because  of  redundancy  that  data  compression algorithms can be applied to text, graphics, and other types of  files  to  be  stored  on  disk  or  transmitted  over  a  network ",0
208,"While  much  attention  is  paid  by  system  designers  to  the representation, storage and manipulation of information in the computer, the ultimate value of information processing software is determined by how well it provides for the effec-tive retrieval of that information.  The quality of retrieval is dependent  on  several  factors:  hardware,  data  organization, search algorithms, and user interface. ",0
209,"Image  processing  is  a  general  term  for  the  manipulation of  a  digitized  image  to  produce  an  enhanced  or  more  con-venient  version.   Some  of  the  earliest  applications  were  in the military (aerial and, later, satellite reconnaissance) and in  the  space  program.   The  military  and  space  programs had a great need for extracting as much useful information as  possible  from  images  that  were  often  gathered  under extreme or marginal conditions.  They also needed to make cameras  and  other  hardware  components  simultaneously more  compact  and  more  efficient,  and  generally  had  the funds to pay for such specialized developments. Once  developed,  higher-quality  image  processing  sys-tems found their way into other applications such as domes-tic  surveillance  and  medical  imaging.   The  development  of cameras that could directly turn light into digitized images , made image processing seam-less by avoiding the necessity of scanning images from tra-ditional film. Image  processing  applications  can  be  divided  into three general categories: enhancement, interpretation, and maintenance. ",0
210,"Starting  in  the  late  1950s,  in  computer  facilities  at mIT, Stanford,  and  other  research  universities  people  began  to encounter  persons  who  had  both  unusual  programming skill  and  an  obsession  with  the  inner  workings  of  the machine.   While  ordinary  users  viewed  the  computer  sim-ply  as  a  tool  for  solving  particular  problems,  this  peculiar breed of programmers reveled in extending the capabilities of  the  system  and  creating  tools  such  as  program  editors that  would  make  it  easier  to  create  even  more  powerful programs.  The movement from mainframes that could run only one program at a time to machines that could simulta-neously  serve  many  users  created  a  kind  of  environmental niche  in  which  these  self-described hackers  could  flourish.  Indeed,  while  administrators  sometimes  complained  that hackers  took  up  too  much  of  the  available  computer  time, they  often  depended  on  them  to  fix  the  bugs  that  infested the  first  versions  of  time-sharing  operating  systems.   Hack-ers also tended to work in the wee hours of the night while normal users slept. ",0
211,"The base 16 or hexadecimal system is a natural way to rep-resent the binary data stored in a computer.  It is more com-pact than binary because four binary digits can be replaced by a single “hex” digit. The  following  table  gives  the  corresponding  decimal, binary, and hex values from 0 to 15:Note that decimal and hex digits are the same from 0 to 9,  but  hex  uses  the  letters  A–F  to  represent  the  digits  cor-responding to decimal 10–15.  The system extends to higher numbers  using  increasing  powers  of  16,  just  as  decimal uses  powers  of  10:  For  example,  hex  FF  represents  binary 111  11111 or decimal 255.  many of the apparently arbitrary numbers encountered in programming can be better under-stood  if  one  realizes  that  they  correspond  to  convenient groupings of bits: FF is eight bits, sufficient to hold a single character ,.  In low-level pro-gramming memory addresses are also usually given in hex ",0
212,"In  operating  systems  and  certain  programming  languages (such as LISP), a heap is a pool of memory resources avail-able  for  allocation  by  programs.   The  memory  segments (sometimes called cells) can be the same size or of variable size.   If  the  same  size,  they  are  linked  together  by  pointers ,.    memory is then allocated for a vari-able by traversing the list and setting the required number of  cells  to  be  “owned”  by  that  variable.   (While  some  lan-guages such as Pascal and C use explicit memory allocation or deallocation functions, other languages such as LISP use a separate runtime module that is not the responsibility of the programmer. )",0
213,"In  the  early  days  of  computing,  the  programmers  of  a  sys-tem  tended  to  also  be  its  users  and  were  thus  intimately familiar  with  the  program’s  operation  and  command  set.  To search a hashed database, the hashing formula is first applied to the search key, yielding a hash value.  That value can then be used in a binary search to quickly zero in on the matching record, if any.         health, personalIf not a programmer, the user of a mainframe program was probably  at  least  a  well-trained  operator  who  could  work with  the  aid  of  a  brief  summary  or  notes  provided  by  the programmer.   However,  with  the  beginnings  of  office  auto-mation  in  the  1970s  and  the  growing  use  of  desktop  com-puters in office, home, and school in the 1980s, increasingly complex  programs  were  being  put  in  the  hands  of  users who  often  had  only  minimal  computer  training  ,. While  programs  often  came  with  one  or  more  tutorial or  reference  manuals,  designers  realized  that  offering  help through  the  program  itself  would  have  some  clear  advan-tages.  First, the user would not have to switch attention from the computer screen to look things up in a manual.  Second, the  help  system  could  be  programmed  to  not  only  provide information,  but  also  to  help  the  user  find  the  informa-tion needed in a given situation.  For example, related topics could be linked together and a searchable index provided",0
214,"A hash  is  a  numeric  value  generated  by  applying  a  math-ematical  formula  to  the  numeric  values  of  the  characters in a string of text ,.  The for-mula is chosen so that the values it produces are always the same  length  (regardless  of  the  length  of  the  original  text) and  are  very  likely  to  be  unique.   (Two  different  strings should  not  produce  the  same  hash  value.   Such  an  event  is called a collision. )",0
215,"The  two  major  application  areas  for  hashing  are  informa-tion  retrieval  and  cryptographic  certification.   In  databases, an index table can be built that contains the hash values for the key fields and the corresponding record number for each field,  with  the  entries  in  hash  value  order.   To  search  the database, an input key is hashed and the value is compared with  the  index  table  (which  can  be  done  using  a  very  fast binary search).  If the hash value is found, the corresponding record number is used to look up the record.  This tends to be much faster than searching an index file directly. ",0
216,"Even  after  decades  of  evolution  in  computing,  the  hard disk  drive  remains  the  primary  means  of  fast  data  storage and  retrieval  in  computer  systems  of  all  sizes.   The  disk itself consists of a rigid aluminum alloy platter coated with a  magnetic  oxide  material.   The  platter  can  be  rotated  at speeds of more than 10,000 rpm.  A typical drive consists of a stack of such platters mounted on a rotating spindle, with a read/write head mounted above each platter. Early  hard  drive  heads  were  controlled  by  a  stepper motor, which positioned the head in response to a series of electrical pulses.  (This system is still used for floppy drives. ) Today’s hard drives, however, are controlled by a voice-coil actuator,  similar  in  structure  to  an  audio  speaker.   The  coil surrounds a magnet.  When a current enters the coil, it gen-erates a magnetic field that interacts with that of the perma-nent magnet, moving the coil and thus the disk head.  Unlike the stepper motor, the voice coil is continuously variable and its  greater  precision  allows  data  tracks  to  be  packed  more tightly on the platter surface, increasing disk capacity. The  storage  capacity  of  a  drive  is  determined  by  the number  of  platters  and  the  spacing  (and  thus  number)  of tracks  that  can  be  laid  down  on  each  platter.   Capacities have  steadily  increased  while  prices  have  plummeted:  In 1980,  for  example,  a  hard  drive  for  an  Apple  II  microcom-put  er cost more than $1,000 and held only 5 mB of data.  As of  2007  internal  hard  drives  with  a  capacity  of  500 gB  or more cost around a $150. 00. ",0
217,"most interfaces between users and computer systems involve the  equivalent  of  switches—keyboard  keys  or  mouse  but-tons.   These  interfaces  cannot  respond  to  degrees  of  pres-sure (for an exception, see gRaphicstablet).  Further, there is  no  feedback  returned  to  the  user  through  the  interface device—the key or mouse does not “push back. ”Haptic (from the greek word for “touch”) interfaces are different in that they do register the pressure and motion of touch, and they often provide touch feedback as well. ",0
218,"Some emerging or near-future uses of haptic technology include:One approach to handwriting recognition involves the extraction of a stroke pattern and its comparison to a database of templates rep-resenting various letters and symbols.  Ultimately the corresponding ASCII character is determined and stored. haptic interfaces ,      remote  surgery,  where  the  surgeon  can  feel  the  resis-tance of tissues and the location of anatomical features,      use of haptic technology to provide robots with more humanlike gripping capabilities,      3D sculpture in a virtual 3D world modeling the char-acteristics of different materials and tools",0
219,"Starting  in  the  late  1950s,  in  computer  facilities  at mIT, Stanford,  and  other  research  universities  people  began  to encounter  persons  who  had  both  unusual  programming skill  and  an  obsession  with  the  inner  workings  of  the machine.   While  ordinary  users  viewed  the  computer  sim-ply  as  a  tool  for  solving  particular  problems,  this  peculiar breed of programmers reveled in extending the capabilities of  the  system  and  creating  tools  such  as  program  editors that  would  make  it  easier  to  create  even  more  powerful programs.  The movement from mainframes that could run only one program at a time to machines that could simulta-neously  serve  many  users  created  a  kind  of  environmental niche  in  which  these  self-described hackers  could  flourish.  Indeed,  while  administrators  sometimes  complained  that hackers  took  up  too  much  of  the  available  computer  time, they  often  depended  on  them  to  fix  the  bugs  that  infested the  first  versions  of  time-sharing  operating  systems.   Hack-ers also tended to work in the wee hours of the night while normal users slept. ",0
220,"A  handwriting  recognition  system  begins  by  building a representation of the user’s writing.  With a pen or stylus system, this representation is not simply a graphical image but  includes  the  recorded  “strokes”  or  discrete  movements that  make  up  the  letters.   The  software  must  then  create  a representation  of  features  of  the  handwriting  that  can  be used  to  match  it  to  the  appropriate  character  templates.  Handwriting  recognition  is  actually  an  application  of  the larger problem of identifying the significance of features in a pattern. ",0
221,"A  number  of  handheld  computers  beginning  with  Apple’s Newton in the mid-1990s and the now popular Palm devices and  BlackBerry  have  some  ability  to  recognize  handwrit-ing.   However,  current  systems  can  be  frustrating  to  use 0        handwriting recognitionbecause accuracy often requires that users write very care-fully  and  consistently  or  (as  in  the  case  of  the  Palm)  even replace their usual letter strokes with simplified alternatives that the computer can more easily recognize.  If the user is allowed  to  use  normal  strokes,  the  system  must  be  gradu-ally  “trained”  by  the  user  giving  writing  samples  and  con-firming the system’s guess about the letters.  As the software becomes  more  adaptable  and  processing  power  increases (allowing  more  sophisticated  algorithms  or  larger  neural networks  to  be  practical)  users  will  be  able  to  write  more naturally and systems will gain more consumer acceptance.  (One step in this direction is the Tablet PC, a notepad-sized computer with a digitizer tablet and a stylus and handwrit-ing recognitions  software,  included  in  Windows xP  and expanded  in  Windows  Vista.   Programs  such  as microsoft OneNote  use  handwriting  recognition  to  allow  users  to incorporate  handwritten  text  into  notes  that  can  be  orga-nized and quickly retrieved. )Currently,  handwriting  recognition  is  used  mainly  in niche  applications,  such  as  collecting  signatures  for  deliv-ery services or filling out “electronic forms” in applications where  the  user  must  be  mobile  and  relatively  hands-free",0
222,"game  consoles  are  computer  devices  dedicated  to  (or  pri-marily  used  for)  playing  video  games.   The  earliest  such devices  appeared  in  the  1970s  from magnavox  and  then Atari, and could only play simple games like Pong (a crude simulation  of  ping-pong).   Slightly  later  systems  began  to feature cartridges that allowed them to play a greater vari-ety of games. ",0
223,"The  normal  method  for  getting  a  computer  to  perform  a task  is  to  specify  the  task  clearly,  choose  the  appropriate approach  ,,  and  then  implement  and  test the  code.   However,  this  approach  requires  that  the  pro-grammer  first  know  the  appropriate  approach,  and  even when there are many potentially suitable algorithms, it isn’t always clear which will prove optimal. ",0
224,"Variations of genetic algorithms or “evolutionary program-ming” have been used for many applications.  In engineering development, a virtual environment can be set up in which a  simulated  device  such  as  a  robot  arm  can  be  allowed  to evolve  until  it  is  able  to  perform  to  acceptable  specifica-tions.  (NASA has also used genetic programs competing on 80 computers to design a space antenna. ) Different versions of an expert system program can be allowed to compete at performing  tasks  such  as  predicting  the  behavior  of  finan-cial markets.  Finally, a genetic program is a natural way to simulate  actual  biological  evolution  and  behavior  in  fields such as epidemiology",0
225,"A  genetic  program  consists  of  a  number  of  copies  of  a routine  that  contain  encoded  “genes”  that  represent  ele-ments  of  algorithms.   The  routines  are  given  a  task  (such as  sorting  data  or  recognizing  patterns)  and  the  most  suc-cessful  routines  are  allowed  to  “reproduce”  by  exchanging genetic  material.   (Often,  further  “mutation”  or  variation  is introduced  at  this  stage,  to  increase  the  range  of  available solutions. )  The  new  “generation”  is  then  allowed  to  tackle the  problem,  and  the  process  is  repeated.   As  a  result,  the routines  become  increasingly  efficient  at  solving  the  given problem, just as organisms in nature become more perfectly adapted to a given environment. ",0
226,"Bill  gates  built microsoft,  the  dominant  company  in  the computer software field and in doing so, became the world’s wealthiest  individual,  with  a  net  worth  measured  in  the tens  of  billions.   Born  on  October  28,  1955,  to  a  successful professional  couple  in  Seattle, gates’s  teenage  years  coin-cided  with  the  first  microprocessors  becoming  available  to electronics hobbyists. gates showed both technical and business talent as early as age 15, when he developed a computerized traffic-control system.   He  sold  his  invention  for  $20,000,  then  dropped out  of  high  school  to  work  as  a  programmer  for  TRW  for the very  respectable  salary  of  $30,000.   By  age  20, gates had  returned  to  his  schooling  and  become  a  freshman  at Harvard, but then he saw a cover article in Popular Electron-ics.   The  story  introduced  the  Altair,  the  first  commercially available microcomputer kit. ",0
227,"geographic  data  can  be  stored  as  either  a  raster  or  a vector representation.  A raster system divides the area into a grid and assigns values to each cell in the grid.  For exam-ple, each cell might be coded according to its highest point of elevation, the amount of vegetation (ground cover) it has, its  population  density,  or  any  other  factor  of  interest.   The simple  grid  system  makes  raster  data  easy  to  manipulate, but the data tends to be “coarse” since there is no informa-tion about variations within a cell. ",0
228,"The power of geographic information systems comes from the ability to integrate data from a variety of sources, whether aerial  photography,  census  records,  or  even  scanned  paper maps.  Once in digital form, the data can be represented in a variety of ways for various purposes.  A sophisticated gis can be  queried  to  determine,  for  example,  how  much  of  a  pro-posed  development  would  have  a  downhill  gradient  and  be below sea level such that flooding might be a problem.  These results can in turn be used by simulation programs to deter-mine,  for  example,  whether  release  of  a  chemical  into  the groundwater from a proposed plant site might affect a partic-ula r  town  two  miles  away.  geographic  information  systems are  thus  vital  for  the  management  of  a  variety  of  complex systems  that  are  distributed  over  a  geographical  area,  such as water and sewage systems, power transmission grids, and traffic control systems.  Other applications include emergency planning (and evacuation routes) and the long-term study of the effects of global warming trends. ",0
229,"globalization  can  be  described  as  a  group  of  trends  that are  breaking  down  the  boundaries  between  national  and regional  economies,  making  countries  more  dependent  on one  another,  and  resulting  in  the  freer  flow  of  labor  and resources.   These  trends  have  been  praised  by  free  trade advocates  and  decried  by  proponents  of  labor  rights  and environmentalism.   However  one  feels  about  them,  it  is clear  that  global  trends  are  reshaping  the  computer  and information  industry  in  many  ways,  and  pose  significant challenges. ",0
230,"global trends that affect computer technology, software, and services include:,      offshoring, or  the  continuing  movement  of  manufac-turing of high-value components (and whole systems) from the industrialized West to regions such as Asia,    outsourcing—moving  functions  (such  as  technical support)  from  a  company’s  home  country  to  areas where  suitable  labor  forces  are  cheaper  ,,      removal of traditional intermediaries such as brokers and agents, with some of their functions being taken over by software ,,      decentralized  networks  (of  which  the  Internet  itself is  the  most  prominent  example)  and  the  tendency  of information  to  flow  freely  and  quickly  despite  barri-ers such as censorship,      virtualization—creation   of   work   groups   or   whole companies  that  are  distributed  across  both  space and time (24 hours), coordinated by the Internet and mobile communications ,,      increasing use of open-source and collaborative mod-els  of  software  and  information  development  ,,      blurring  of  the  distinction  between  consumers  and producers  of  information ",0
231,"Computer-related businesses must also deal with the effects of  globalization  on  the  market  for  hardware,  software,  and services.   Lower-cost  offshore  manufacturing  has  helped contribute to making many computer systems and peripher-als  into  commodity  items.   This  certainly  benefits  consum-ers (consider the ubiquitous $100 or less computer printer).  However,  it  becomes  more  difficult  to  extract  a  premium for a brand as opposed to a generic name.  Some companies have responded by relentless efforts to maximize efficiency in manufacturing , while a few others have maintained a reputation for style or innovation .   Consumers  have  increasingly  objected, however, to the difficulty in dealing with offshore technical support. While  the  power  of  the  Internet  has  opened  many  new ways  of  reaching  potential  customers  around  the  world, dealing   with   a   global   marketplace   brings   considerable added complications, such as the need to deal with different regulatory systems .  In some areas  there is also the problem of unauthor-ized  copying  of  software  and  media  products . ",0
232,"google  Inc.   (NASDAQ  symbol: gOOg)  has  built  a  busi-ness  colossus  by  focusing  on  helping  users  find  what  they are  looking  for  on  the  Internet  while  selling  advertising targeted at those same users.  By 2006, “to google” could be found in dictionaries as a verb meaning to look up anyone or anything online. goog  le was founded by two Stanford students ,  who,  for  their  doctoral  thesis, had described a Web search algorithm that could give a bet-ter  idea  of  the  likely  relevance  of  a  given  site  based  on  the number  of  sites  that  linked  to  it.   The  two  students  imple-mented  a  search  engine  based  on  their  ideas  and  hosted  it on  the  Stanford  Web  site,  where  its  popularity  soon  irri-tated  the  university’s  system  administrators.   In  1998  their bus  iness  was  incorporated  as google,  Inc. ,  and  moved  to the  archetypal  Silicon  Valley  entrepreneur’s  location—a friend’s  garage.   However,  as  the  company  attracted  invest-ment  capital  and  grew  rapidly,  it  moved  to  Palo  Alto  and the  n its present home in mountain View. google’s  initial  public  stock  offering  was  in  2004,  and the   market’s   enthusiastic   response   made   many   senior employees  instant  millionaires.  google’s  steady  growth  in subsequent  years  has  kept  its  stock  in  demand,  reaching  a record  peak  of  $560  in  September  2007.   (In  2006 google was added to the S&P 500 Index. )",0
233,"prior  to  the  late  1970s,  most  computer  applications  (other than  some  scientific  and  experimental  ones)  did  not  use graphics.   However,  the  early  microcomputer  systems  such as the Apple II, Radio Shack TRS-80, and Commodore PET could all display graphics, either on a monitor or (with the aid  of  a  video  modulator)  on  an  ordinary  TV  set.   While primitive (low resolution; monochrome or just a handful of colors)  this  graphics  capability  allowed  for  a  thriving  mar-ket in games and educational software. The  earliest  video  displays  for  mainstream  PCs  pro-vid ed basic text display capabilities (such as the mDA, or monochrome  display  adapter,  with  25  lines  of  text  up  to 80  characters  per  line)  plus  the  ability  to  create  graphics by setting the color of individual pixels.  The typical low-endgraphics card of the early 1980s was the CgA (Color graphics  Adapter),  which  offered  various  modes  such  as 320  by  200  pixels  with  four  colors.   Computers  marketed forprofessional  use  offered  the  EgA  (Enhanced graph-ics  Adapter),  which  could  show  640  by  350  pixels  at  16 colors. ",0
234,"Broadly speaking, a graphics file consists of data that speci-fies the color of each pixel (dot) in an image.  Since there are many  ways  this  information  can  be  organized,  there  are  a variety  of  graphics  file  formats.  ",0
235,"In  a  bitmap  format  there  is  a  group  of  bits  (i. e.   a  binary value)  that  specifies  the  color  of  each  pixel.   Windows  pro-vides standard  bitmap  (BmP)  formats  for  1-bit  (2  colors or  monochrome),  4-bit  (16  colors),  8-bit  (256  colors),  or 24-bit  (16  million  colors).   The  Windows  bitmap  format  is also  called  a  DIB  (device-independent  bitmap)  because  the stored  colors  are  independent  of  the  output  device  to  be used  (such  as  a  monitor  or  printer).   The  relevant  device driver is responsible for translating the color to one actually used by the device.  Because it is “native” to Windows, BmP is widely used, especially for program graphics resources. ",0
236,"Bitmap  formats  have  the  advantage  of  storing  the  exact color  of  every  pixel  without  losing  any  information.   How-ever,  this  means  that  the  files  can  be  very  large  (from hundreds  of  thousands  of  bytes  to  several  megabytes  for Windows  screen  graphics).   BmP  and  other  bitmap  formats do  support  a  simple  method  of  compression  called  run-length encoding (RLE), where a series of identical pixels is replaced  by  a  single  pixel  and  a  count.   Bitmap  files  can  be further compressed through the use of utilities such as the popular Zip program ",0
237,"EPS (Encapsulated PostScript) is a vector-based rather than bitmap (raster) format.  This means that an EPS file consists not  of  the  actual  pixel  values  of  an  image,  but  the  instruc-tions for drawing the image (including coordinates, colors, and  so  on).   The  instructions  are  specified  as  a  text  file  in the versatile PostScript page description language.  This for-mat is usually used for printing, and requires a printer that supports PostScript (there are also PostScript renderers that run entirely in software, but they tend to be slow and some-what unreliable). ",0
238,"gIF, or graphics Interchange Format, is a bitmapped format promulgated  by  CompuServe.   Instead  of  reserving  enough space  to  store  a  large  number  of  colors  in  each  pixel,  this format  uses  a  color  table  that  can  hold  up  to  256  colors.  Each pixel contains a reference (index into) the color table.  This means that gIF works best with images that have rela-tively  few  colors  and  for  applications  (such  as  Web  pages) where compactness  is  important.  gIF  also  uses  compres-sion to achieve compactness, but unlike the case with JPEg it is a lossless compression called LZW.  There is also a gIF format that stores simple animations. ",0
239,"JPEg,  which  stands  for  Joint  Photographic  Experts group, is  widely  used  for  digital  cameras  because  of  its  ability  to highly  compress  the  data  in  a  color  graphics  image,  allow-ing  a  reasonable  number  of  high-resolution  pictures  to  be stored  in  the  camera’s  onboard  memory.   The  compression is “lossy,” meaning that information is lost during compres-sion ,.  At relatively low compression ratios (such as 10:1, or 10 percent of the original image size) changes  in  the  image  due  to  data  loss  are  unlikely  to  be         graphics formatsperceived by the human eye.  At higher ratios (approaching 100:1)  the  image  becomes  seriously  degraded.   JPEg’s  abil-ity to  store  thousands  of  colors  (unlike gIF’s  limit  of  256) makes  the  format  particularly  suitable  for  the  subtleties  of photography. ",0
240,PCx is a compressed bitmap format originally used by the popular PC Paintbrush program.  In recent years it has been largely supplanted by BmP and TIFF. ,0
241,"TIFF,  or  Tagged  Image  File  Format,  is  also  a  compressed bitmap  format.   There  are  several  variations  by  different vendors,  which  can  lead  to  compatibility  problems.   Imple-mentations  can  use  various  compression  methods,  gener-ally leading to ratios of 1. 5 to 1 to about 2 to 1. ",0
242,"While conventional pointing devices , are quite satisfactory  for  making  selections  and  even  manipulat-ing  objects,  many  artists  prefer  the  control  available  only through  a  pen  or  pencil,  which  allows  the  angle  and  pres-sure of the stylus tip to be varied, creating precise lines and shading.  A graphics tablet (also called a digitizing tablet) is a device that uses a specially wired pen or pencil with a flat surface (tablet).  Besides tracking the location of the pen and tra  nslating  it  into x/Y  screen  coordinates,  the  tablet  also has  pressure  sensors  (depending  on  sensitivity. the  tablet can recognize 256, 512, or 1024 levels of pressure).  In com-bination with buttons on the pen, the pressure level can be used  to  control  the  line  thickness,  transparency,  or  color.  In  addition,  the  driver  software  for  some  graphics  tablets includes additional functions such as the ability to program the  pen  to  control  features  of  such  applications  as  Adobe Photoshop. ",0
243,"This is a general term for features that reduce the growing environmental impact of the manufacture or use of comput-ers.   This  impact  has  several  aspects:  energy  consumption, resource  consumption,  e-waste,  and  pollution  and  green-house emissions. ",0
244,"Computers consume a variety of resources, starting with their manufacturing and packaging.  Resource consumption can be reduced  by  building  more  compact  units  and  by  designing components so they can be more readily stripped and recycled or  reused.   Adopting  reusable  storage  media  (such  as  rewrit-able  CDs),  recycling  printer  toner  cartridges,  and  changing office  procedures  to  minimize  the  generation  of  paper  docu-ments are also ways to reduce resource consumption. ",0
245,"In recent years the disposal of obsolete computers and other electronic  equipment  (“e-waste”)  has  been  both  a  grow-ing  concern  and  a  business  opportunity.   There  are  many toxic substances in electronics components, including lead, mercury,  and  cadmium.   Processing  e-waste  to  recover  raw materials is expensive, so greater emphasis has been placed on  disassembling  machines  and  reusing  or  refurbishing their  individual  components.  meanwhile,  many  communi-ties have banned disposing of e-waste in regular trash, and some have offered opportunities to drop off e-waste at no or minimal  charge.   States  such  as  California  have  also  insti-tuted  a  recycling  fee  that  is  collected  upon  sale  of  devices such as CRT monitors and televisions. ",0
246,"Fabrication of computer chips in more than 200 large plants around the world involves a variety of toxic chemicals and waste products.  The Silicon Valley alone is home to 29 toxic sites under the EPA’s Superfund Program.  The shift of much of semiconductor and computer component manufacturing to  countries  such  as  China  that  have  less  strict  pollution controls  has  also  exacerbated  what  has  become  a  global problem. Whether through regulation or enlightened self-interest, companies  that  want  to  reduce  future  emissions  can  use several strategies.  manufacturing equipment and processes can be modified so they create fewer toxic substances or at least  keep  them  from  getting  into  the  environment.   Non-toxic  (or  less  toxic)  materials  can  be  substituted  where possible—for  example,  use  of  ozone-depleting  chlorofluo-rocarbons (CFCs) as cleaning agents has been largely elimi-nated.   Finally,  waste  can  be  properly  sorted  and  disposed of, and recycled wherever feasible. Like  other  major  manufacturing  sectors,  the  computer industry  is  also  faced  with  the  need  to  reduce  the  amount of  the  greenhouse  gases  (particularly  CO2)  contributing  to global  warming.   This  mainly  means  further  reducing  the energy  consumption  of  new  PCs.   In  June  2007  a  number of  major  players,  including google,  Intel,  Dell,  Hewlett-Packard, microsoft, and Sun, established the Climate Savers Computing  Initiative.  going  beyond  Energy  Star,  the  pro-gram  is  expected  to  reduce  power  consumption  equivalent to 54 million tons of greenhouse gases annually—about the same  as  that  produced  by  11  million  cars  or  20  large  coal-fired power plants. ",0
247,"grid  or  cluster  computing  involves  the  creation  of  a  sin-gle  computer  architecture  that  consists  of  many  separate computers  that  function  much  like  a  single  machine.   The computers  are  usually  connected  using  fast  networks  ,.   The  purpose  of  the  arrangement can  be  to  provide  redundant  processing  in  case  of  system failures,  to  dynamically  balance  a  fluctuating  work  load, or  to  split  large  computations  into  many  parts  that  can  be performed  simultaneously.   This  latter  approach  to  “high-performance computing” creates the virtual equivalent of a very large and powerful machine. ",0
248,"grid  and  cluster  architectures  often  overlap,  but  the  term grid  tends  to  be  applied  to  a  more  loosely  coordinated structure  where  the  computers  are  dispersed  over  a  wider area  (not  a  local  network).   In  a  grid,  the  work  is  usually divided  into  many  separate  packets  that  can  be  processed independently without the computers having to share data.  Each task can be completed and submitted without waiting for the completion of any other task.  Clusters, or the other hand,  more  closely  couple  computers  to  act  more  like  a single large machine. ",0
249,"Popular groupware software suites such as Lotus Notes and microsoft Exchange generally offer at least some of the following features:,      e-mail coordination, including the creation of group or task-oriented mail lists,      shared  calendar,  giving  each  participant  information about all upcoming events,      meeting  management,  including  scheduling  (ensur-ing  compatibility  with  everyone’s  existing  schedule) and facilities booking,    scheduling  tasks  with  listing  of  persons  responsible for  each  task,  progress  (milestones  met),  and  check-ing off completed tasks,      real-time “chat” or instant message capabilities,      documentation systems that allow a number of people to  make  comments  on  the  same  document  and  see and respond to each other’s comments,    “whiteboard”  systems  that  allow  multiple  users  to draw  a  diagram  or  chart  in  real  time,  with  everyone able to see and possibly modify it",0
250,"Grove, Andrew S. (1936– )Hungarian-AmericanEntrepreneur.  Andrew grove  is  a  pioneer  in  the  semiconductor  industry and builder of Intel, the corporation whose processors now power the majority of personal computers.  grove was born András gróf on September 2, 1936, in Budapest to a Jewish family.  grove’s family was disrupted by the german occupa-tion of Hungary later in World War II.  Andrew’s father was conscripted into a work brigade and then into a Hungarian formation  of  the german  army.   Andrew  and  his  mother, maria,  had  to  hide  from  the  Nazi  roundup  in  which  many Hungarian Jews were sent to death in concentration camps. ",0
251,"Fault  tolerance  is  a  design  concept  that  recognizes  that  all computer-based systems will fail eventually.  The question is whether a system as a whole can be designed to “fail grace-fully. ”  This  means  that  even  if  one  or  more  components fail,  the  system  will  continue  to  operate  according  to  its design  specifications,  even  if  its  speed  or  throughput  must decrease. ",0
252,"There  are  a  number  of  ways  to  make  a  system  more  fault tolerant.  Individual components such as hard drives can be composed of multiple units so that the remaining units can take over if one fails ,.  If each key component has at least one backup, then there should be time to replace the primary before the backup also fails. Another way to achieve fault tolerance is to provide mul-tiple paths to successful completion of the task.  In fact, this is  how  packet-switched  networks  like  the  Internet  work ,.   If  one  communications  link  is  down  or  too congested, packets are given an alternative routing. Fault diagnosis software can also play an important role both in determining how to respond to a problem (beyond any automatic response) and for providing data that will be useful  later  to  system  administrators  or  technicians.   Some fault diagnosis systems can use elaborate rules , to pinpoint the cause of a fault and recommend a solution. ",0
253,"The  amount  of  fault  tolerance  to  be  provided  for  a  sys-tem depends on a number of factors:,    How important is it that the system not fail?,      How critical is a given component to the operation of the system?,      How  likely  is  it  that  a  given  component  will  fail? (mean time between failures, or mBTF),      How expensive is it to make the component or system fault tolerant?",0
254,"A  fiber  optic  (or  optical  fiber)  cable  transmits  photons (light)  instead  of  electrons.   Depending  on  the  diameter  of the cable, the light is guided either by total internal reflec-tion  or  as  a  waveguide  (manipulating  refraction).   These principles were known as early as the mid-19th century and began to be used in the 20th century for such applications as  dental  and  medical  illumination  and  in  experiments  in transmitting images for television. ",0
255,"Optical  fiber  in  its  modern  form  was  developed  in  the 1950s.   The  glass  fiber  through  which  the  light  passes  is surrounded  by  a  transparent  cladding  designed  to  provide the needed refractive index to keep the light confined.  The cladding  in  turn  is  surrounded  by  a  resin  buffer  layer  and often an outer jacket and plastic cover.  Fiber used for com-munication is flexible, allowing it to bend if necessary. Early  optical  fiber  could  not  be  used  for  practical  com-munication because of progressive attenuation (weakening) of the light as it traveled.  However, by the 1970s the attenu-ation  was  being  reduced  to  acceptable  levels  by  removing impurities from the fibers.  Today the light signals can travel hundreds of miles without the need for repeaters or amplifi-ers.  In the 1990s a new type of optical fiber (photonic crys-tal)  using  diffraction  became  available.   This  kind  of  fiber is  particularly  useful  in  applications  that  require  higher power signals. ",0
256,"Optical  fiber  has  several  advantages  over  ordinary  electric cable  for  communications  and  networking.   The  signals  can travel much farther without the need for a repeater to boost the  signal.   Also,  the  ability  to  modulate  wavelengths  allows optical fiber to carry many separate channels, greatly increas-ing the total data throughput.  Optical fiber does not emit RF (radio  frequency)  energy,  a  source  of  “cross  talk”  (interfer-ence) in electrical cable.  Fiber is also more secure than elec-trical cable because it is hard for an eavesdropper to tap. Today  fiber  is  used  for  most  long-distance  phone  lines and Internet  connections.  many  cable  television  systems are  upgrading  from  video  cable  to  fiber  because  of  its greater reliability and ability to carry more bandwidth and enhanced data services. ",0
257,"At bottom, information in a computer is stored as a series of bits,  which  can  be  grouped  into  larger  units  such  as  bytes or “words” that represent particular numbers or characters.  In  order  to  be  stored  and  retrieved,  a  collection  of  such binary  data  must  be  given  a  name  and  certain  attributes that  describe  how  the  information  can  be  accessed.   This named entity is the file. ",0
258,"The file system is the facility of the operating system that organizes files ,.  For example, on DOS and older Windows PCs, there is a file allocation table (FAT) that consists of a linked list of clusters (each cluster consists of a fixed number of sectors, varying with the overall size of the disk).  When the operating system is asked to access a file, it can go through the table and find the clusters belonging to that file, read the data and send it to the requesting applica-tio n.   modern  file  systems  further  organize  files  into  groups called folders or directories, which can be nested several lay-ers  deep.   Such  a  hierarchical  file  system  makes  it  easier  for users to organize the dozens of applications and thousands of files found on today’s PCs.  For example, a folder called Book might have a subfolder for each chapter, which in turn con-tains folders for text and illustrations relating to that chapter. ",0
259,"The  ultimate  organization  of  data  in  a  file  depends  on  the application.   A  typical  approach  is  to  define  a  data  record with  various  fields.   The  program  might  have  a  loop  that repeatedly  requests  a  record  from  the  file,  processes  it  in some  way,  and  repeats  until  the  operating  system  tells  it that  it  has  reached  the  end  of  the  file.   This  would  be  a sequential  access;  a  program  can  also  be  set  up  for ran-dom  access,  which  means  that  an  arbitrary  record  can  be requested  and  that  request  will  be  translated  into  the  cor-rect  physical  location  in  the  file.   The  two  approaches  can be combined in ISAm (Indexed Sequential Access method), where  the  records  are  stored  sequentially  but  fields  are indexed so a particular record can be retrieved. ",0
260,"The  growth  in  desktop  computing  since  the  1980s  has resulted  in  much  data  being  moved  from  mainframe  com-puters  to  desktop  PCs,  which  are  now  usually  linked  by networks.  While a network enables users to exchange files, there  remains  the  problem  of  storing  large  files  or  collec-tions of files (such as databases) that are too large for a typi-cal PC hard drive or that need to be accessed and updated by many users. The  common  solution  is  to  obtain  a  computer  with large,  fast  disk  drives  ,.   This  computer,  the file  server,  is  equipped  with  software  (often  included  with the  networking  package)  that  serves  (provides)  files  as requested  by  users  or  applications  on  the  other  PCs  on the  network.   (See  also client-seRveRcomputing. )  The specifics  of  configuring  the  server  for  optimum  efficiency, providing  adequate  security,  and  arranging  for  backup  or archiving varies with the particular network operating sys-tem  in  use  (the  most  popular  environments  are  Windows NT, Vista, and the various versions of UNIx and Linux)",0
261,"The  file  server  has  many  advantages  over  storing  the files needed by each user on his or her own PC.  By storing the files on a central server, ordinary users’ PCs do not need to have larger, more expensive disk drives.  Central storage also  makes  it  easier  to  ensure  that  backups  are  run  regularly. ",0
262,"File-sharing services allow participants to provide access to files  on  their  personal  computers,  such  as  music  or  video.  In turn, the user can browse the service to find and down-load material of interest.  The structure is generally that of a peer-to-peer (P2P) network with no central server. The  first  major  file-sharing  service  was  Napster.   This was  a  P2P  network  but  had  a  central  server  that  provided the  searchable  list  of  files  and  locations—but  not  the  files themselves,   which   were   downloaded   from   users’   PCs.  Napster  was  forced  to  close  in  2001  by  legal  action  from copyright  holders  ,.   A  new  but  unrelated  for-pay  service  opened  later under the same name. ",0
263,"many  services  today  use  the  popular  BitTorrent  file-shar-ing protocol.  A BitTorrent client (either the program of that name  or  another  compatible  one)  can  transmit  or  receive any  type  of  data.   To  share  a  file,  the  client  creates  a  “tor-rent”—a  small  file  that  contains  metadata  describing  the file and an assignment to a “tracker. ” The tracker is another computer  (node)  that  coordinates  the  distribution  of  the file.  Although this sounds complicated and a request takes longer  to  set  up  than  an  ordinary  HTTP  connection,  the advantage  is  that  once  set  up,  downloading  is  efficiently managed  even  for  files  for  which  there  is  high  demand.  The  downloading  client  connects  to  multiple  clients  that provide  pieces  of  the  desired  file.   Because  of  its  efficiency, BitTorrent allows for distribution of substantial amounts of data  at  low  cost,  particularly  since  the  system  “scales  up” automatically  without  having  to  provide  extra  resources.  BitTorrent  is  currently  being  used  for  a  variety  of  legally distributed  material,  including  video,  sound,  and  textual content ",0
264,"Because  of  their  frequent  use  to  share  copyrighted  music, video, or other material, a variety of organizations of copy-right  owners  have  sued  file-sharing  services  and/or  their users.   The  biggest  problem  for  the  courts  is  to  determine whether  there  is  “substantial  non-infringing  use”—that  is, the service is being used to exchange legal data. Some  file-sharing  services  have  been  accused  of  dis-tributing  malware  (viruses  or  spyware)  or  of  being  used to  distribute  material  that  is  illegal  per  se  (such  as  child pornography). In  response  to  litigation  threats,  file-sharing  services have tended to become more decentralized, and some have features  that  increase  anonymity  of  users  , or use encryption. ",0
265,"With  today’s  networked  PCs  and  the  use  of  e-mail  attach-ments  it  is  easy  to  send  a  copy  of  a  file  or  files  from  one computer  to  another,  because  networks  already  include  all the  facilities  for  doing  so.   Earlier,  many  PCs  were  not  net-worked  but  could  be  connected  via  a  dial-up  modem.   To established the connection, a terminal program running on one  PC  had  to  negotiate  with  its  counterpart  on  the  other machine,  agreeing  on  whether  data  would  be  sent  in  7-  or 8-bit  chunks,  and  the  number  of  parity  bits  that  would  be included  for  error-checking  ,.   The sending  program  would  inform  the  receiving  program  as to  the  name  and  basic  type  of  the  file.   For  binary  files (files  intended  to  be  interpreted  as  literal  binary  codes,  as with executable programs, images, and so on) the contents would be sent unchanged.  For text files, there might be the issue of which character set (7- bit or 8-bit ASCII) was being used, and whether the ends of lines were to be marked with a  CR  (carriage  return)  character,  an  LF  (linefeed),  or  bot",0
266,"Once the programs agree on the basic parameters for a file transfer,  the  transfer  has  to  be  managed  to  ensure  that  it completes  correctly.   Typically,  files  are  divided  into  blocks of data (such as 1K, or 1024 bytes each).  During the 1970s, Ward Christensen developed xmodem, the first widely used file transfer program for PCs running CP/m (and later, mS-DOS and other operating systems).  xmodem was quite reli-able because it incorporated a checksum (and later, a more advanced  CRC)  to  check  the  integrity  of  each  data  block.  If  an  error  is  detected,  the  receiving  program  requests  a retransmission. ",0
267,"The  role  of  the  computer  in  film  begins  well  before the  first  camera  rolls.   Writers  can  use  computers  to  write scripts,  while  specialized  programs  can  be  used  to  lay out  storyboards.   Using  3D  programs  somewhat  like  CAD (drafting)  programs,  set  designers  can  experiment  with the positioning of objects before deciding on a final design and  obtaining  or  creating  the  physical  props.   For  mattes (backgrounds  against  which  the  characters  will  be  shot  in a  scene),  a  computer-generated  scene  can  now  be  inserted directly  into  the  film  without  the  need  for  an  expensive, hand-painted backdrop. ",0
268,"There are many calculations or other processes that can be described using a specific series of states or conditions.  For example,  the  state  of  a  combination  lock  depends  not  only on what numeral is being dialed or punched at the moment, but  on  the  numbers  that  have  been  previously  entered.   An even simpler example is a counter (such as a car odometer), whose  next  output  is  equal  to  one  increment  plus  its  cur-rent  setting.   In  other  words,  a  state-based  device  has  an inherent “memory” of previous steps. ",0
269,"many programs and operating systems are structured as an endless  loop  where  an  input  (or  command)  is  processed, the  results  returned,  the  next  input  is  processed,  and  so on, until an exit command is received.  A mode or state can be  used  to  determine  the  system’s  activity.   For  example,  a program  might  be  in  different  modes  such  as  waiting  for input,  processing  input,  displaying  results,  and  so  on.   The program  logic  will  refer  to  the  current  state  to  determine what  to  do  next  and  at  some  point  the  logic  will transitionthe system to the next state in the sequence.  The validity of some kinds of programs, protocols, or circuits can therefore be proven by showing that there is an equivalent finite-state machine—and thus that all possible combinations of inputs have been accounted for. Finite-state machines have many other interesting appli-cations.  Simple organisms can be modeled as a set of states that  interact  with  the  environment .  The lower-level functions of robots can also be represented as  a  set  of  interacting  finite-state  machines.   Even  video game characters often use FSms to give them a repertoire of plausible behavior. ",0
270,"The  vulnerability  of  computer  systems  to  malicious  or criminal  attack  has  been  greatly  increased  by  the  growing number  of  connections  between  computers  (and  local  net-works)  and  the  worldwide  Internet  ,.   The  widespread  use of  permanent  broadband  connections  by  consumers  (such as  DSL  and  cable  modem  links)  has  increased  the  risk  to home  users.   Intruders  can  use  “port  scanning”  programs to  determine  what  connections  a  given  system  or  network has open, and can use other programs to snoop and steal or destroy sensitive data. ",0
271,"Typical firewall functions include:,    Examining incoming data  packets  and  blocking  those that  include  commands  to  examine  or  use  unauthor-ized ports or IP addresses,      Blocking  data  packets  that  are  associated  with  com-mon  hacking  techniques  such  as  “trojans”  or  “back-door” exploitationsThis diagram shows a finite-state representation of a ZIP code.  The arrows link each state (within a circle) to its possible successor.  In this simple example each digit must be followed by another digit until the fifth digit, which can either be followed by a blank (indi-cating a five-digit ZIP code) or four more digits for a 9-digit ZIP.         firewall,      Hiding  all  the  internal  network  addresses  on  a  local network,  presenting  only  a  single  address  to  the outside  world  (this  is  also  called  NAT,  or  Network Address Translation),      monitoring  particular  applications  such  as  ftp  (file transfer  protocol)  and  telnet  (remote  login),  restrict-ing them to certain addresses.  Often a special address called  a  proxy  is  established  rather  than  allowing direct  connections  between  the  outside  and  the  local network. ",0
272,"FireWire  is  a  high-speed  serial  interface  used  by  personal computers  and  digital  audio  and  video  equipment.   (The name FireWire is an Apple brand name, but it is used gener-ically.  Technically it is the IEEE 1394 Serial Bus. )FireWire was developed in the 1990s by the IEEE P1394 Working group  with  substantial  funding  from  Apple  and help  from  engineers  from  major  corporations  including IB m,  Digital  Equipment  Corporation  (DEC),  Sony,  and Texas  Instruments.   In  1993  it  was  hailed  as  the  “most  sig-nificant new technology” by Byte magazine. ",0
273,"Common uses for FireWire include connecting digital video (such as camcorder) devices, audio devices, and some data storage devices.  FireWire is favored over USB 2. 0 for many professional applications because of its higher speed and power distribution capabilities.  However, it is more expensive than USB 2. 0, which provides sufficient speed for many consumer peripherals such as digital cameras and printers. ",0
274,"A flag is a variable that is used to specify a particular condi-tion or status ,.  Usually a flag is either true or false.   For  example,  a  flag  Valid_Form  could  be  set  to  true before  the  input  form  is  processed.   If  the  validation  check for  any  data  field  fails,  the  flag  would  be  set  to  false.   After the  input  procedure  has  ended,  the  main  program  would check the Valid_Form flag.  If it’s true, the data on the form is  processed  (for  example,  continuing  on  to  the  payment process).  If the flag is false, the input form might be redis-played with errors or omissions highlighted. ",0
275,"In computers, the term ""flash mob"" typically refers to a group of individuals who organize a spontaneous and coordinated online activity, such as a social media campaign or an online protest.  Flash mobs often use social media platforms and other online tools to mobilize large groups of people quickly and efficiently. Flash mobs can take many different forms, ranging from coordinated tweets or Facebook posts to more elaborate online events such as virtual marches or protests.  They are often used to draw attention to a particular cause, raise awareness about an issue, or promote a message or idea. ",0
276,"Smart  mobs  are  similar  in  organization  to  flash  mobs  but tend  to  be  more  purposeful  and  enduring  forms  of  social organization.  The phenomenon was first described by How-ard  Rheingold  in  his  book Smart  Mobs:  The  Next  Social Revolution ,.  Rheingold describes several examples of smart mobs, including teenage “thumb tribes” in Tokyo and Helsinki, Finland (named for their use of  tiny  thumb-operated  keyboards  on  cell  phones).   Their typical  activities  included  organizing  impromptu  raves  or converging on rock stars or other celebrities. ",0
277,"A flash or “thumb” drive is a small data storage device that uses semiconductor flash memory rather than a disk drive.  It is connected to a digital device using the universal serial bus  ,.   Because  most  computers,  digital  cameras, and  other  digital  devices  have  USB  ports,  a  flash  drive  is  a convenient way to provide up to 16 gB (as of 2007) of low power,  rewritable  memory.   Flash  drives  first  appeared  in late 2000. Flash drives can use a separate USB cable (useful when several devices need to be connected to closely spaced USB ports)  or  simply  have  a  connector  that  plugs  directly  into the port.  many  people  who  regularly  work  with  several computers carry their backup data or even a complete oper-ating system (such as Linux) on a flash drive, perhaps con-nected to their keyring. ",0
278,"The  traditional  computer  display  uses  a  cathode  ray  tube (CRT)  like  that  in  a  television  set  ,.   The  flat-panel  display  is  an  alternative  used  in  most  laptop  com-puters  and  some  higher-end  desktop  systems.   The  most common type uses a liquid crystal display (LCD).  The dis-play consists of a grid of cells with one cell for each of the three colors (red, green, and blue) for each pixel. ",0
279,"Until  the  mid-1990s,  the  floppy  disk  or  diskette  was  the primary  method  for  distributing  software  and  providing removable  data  storage  for  personal  computers.   Diskettes first appeared in the late 1960s on IBm minicomputers, and became  more  widespread  on  a  variety  of  minicomputers and early microcomputers during the 1970s. The now obsolete 8-inch and 5-¼ inch disks were made from  mylar  with  a  metal  oxide  coating,  the  assembly  being housed in a flexible cardboard jacket (hence the term “floppy disk”).   The  more  compact  3. 5-inch  diskettes  first  widely introduced  with  the  Apple macintosh  in  1984  became  the standard  type  for  all  PCs  by  the  1990s.   These  diskettes  are no longer truly “floppy” and come in a rigid plastic case. ",0
280,"A flowchart is a diagram showing the “flow” or progress of operations  in  a  computer  program.   Flowcharting  was  one of  the  earliest  aids  to  program  design  and  documentation, and a plastic template with standard flowcharting symbols was a common programming accessory.  Today CASE (com-puter-aided  software  engineering)  systems  often  include utilities  that  can  automatically  generate  flowcharts  based on  the  control  structures  and  procedure  calls  found  in  the program code. The standard flowchart symbols include blocks of vari-ous  shapes  that  represent  input/output,  data  processing, sorting and collating, and so on.  Lines with arrows indicate the  flow  of  data  from  one  stage  or  process  to  the  next.   A diamond-shaped  symbol  indicates  a  decision  to  be  made by  the  program.   If  the  decision  is  an  “if”  ,  separate  lines  branch  off  to  the  alternatives.  If the decision involves repeated testing ,, the line returns  back  to  the  decision  point  while  another  line  indi-cates  the  continuation  of  processing  after  the  loop  exits.  Devices  such  as  printers  and  disk  drives  have  their  own symbols  with  lines  indicating  the  flow  of  data  to  or  from the device",0
281,"In computing, a font refers to a typeface that has a distinc-tive  appearance  and  style.   In  most  word  processing,  desk-top publishing, and other programs the user can select the point  size  at  which  the  font  is  to  be  displayed  and  printed (in traditional typography each point size would be consid-ered to be a separate font).  Operating systems such as Win-A flowchart uses a set of simple symbols to describe the steps involved in a data processing operation.  The parallelograms indi-cate an input/output operation (such as reading or writing a file).  The “decision diamonds” have yes and no branches depending on the result of a test or comparison. 00        flowchartdows  and macintosh  usually  come  with  an  assortment  of fonts, and applications can register additional fonts to make them available to the system. ",0
282,"There  are  two  basic  ways  to  store  font  data  in  the computer  system.   Bitmapped  fonts  store  the  actual  pat-tern of tiny dots that make up the letters in the font.  This has  the  advantage  of  allowing  each  letter  in  each  point size  to  be  precisely  designed.   The  primary  disadvantage is  the  amount  of  memory  and  system  resources  required to  store  a  font  in  many  point  sizes.   In  practice,  this  con-sideration  results  in  only  a  relatively  few  fonts  and  sizes being available. ",0
283,"Forth  has  a  very  simple  structure.   The  Forth  system  con-sists  of  a  collection  of words.   Each  word  is  a  sequence  of operations  (which  can  include  other  existing  words).   For example, the DUP word makes a copy of a data value.  Data is  held  by  a  stack.   For  example,  the  arithmetic  expression written  as  2  +  3  in  most  languages  would  be  written  in Forth  as  +  2  3.   When  the  +  operator  (which  in  Forth  is  a pre-defined  word)  executes,  it  adds  the  next  two  numbers it  encounters  (2  and  3)  together,  and  puts  the  sum  on  the stack (where in turn it might be fetched for further process-ing by the next word in the program ,.  This rep-resentation  is  also  called postfix  notation  and  is  familiar  to many users of scientific calculators. ",0
284,"A key feature of Forth is its extensibility.  Once you have defined a word, the new word can be used in exactly the same way  as  the  predefined  words.   The  various  forms  of defining words allow for great control over what happens when a new word  is  created  and  when  the  word  is  later  executed.   (In many ways Forth anticipated the principles of object-oriented programming, with words as objects with implicit construc-tors and methods.  A well-organized Forth program builds up from  “primitive”  operations  to  the  higher-level  words,  with the program itself being the highest-level word. )",0
285,"Fortran  (FORmula  TRANslator)  was  the  first  widely used  high-level  programming  language.   It  was  developed by a project begun in 1954 by a team under the leadership of I  Bm researcher John Backus.  The goal of the project was to create a language that would allow mathematicians, sci-entists, and engineers to express calculations in something close  to  the  traditional  notation.   At  the  same  time,  a  com-piler  would  have  to  be  carefully  designed  so  that  it  would produce  executable  machine  code  that  would  be  nearly  as efficient as the code that would have been created through the more tedious process of using assembly languages.  (See compileR and assembleR. )",0
286,"Fractals  and  the  related  idea  of  chaos  have  profoundly changed  the  way  scientists  think  about  and  model  the world.   Around  1960,  Benoit mandelbrot  noticed  that  sup-posedly random economic fluctuations were not distributed evenly but tended to form “clumps. ” As he investigated other sources of data, he found that many other things exhibited this  odd  behavior.   He  also  discovered  that  the  patterns  of distribution were “self-similar”—that is, if you magnified a portion of the pattern it looked like a miniature copy of the whol  e.   mandelbrot  coined  the  term fractal  (meaning  frac-tured, or broken up) to describe such patterns.  Eventually, a  number  of  simple  mathematical  functions  were  found  to exhibit such behavior in generating values. ",0
287,"many  computer  users  are  familiar  with  the  colorful  fractal patterns  generated  by  some  screen  savers.   There  are  hun-dreds  of  “families”  of  fractals  (beginning  with  the  famous mandelbrot  set)  that  can  be  color-coded  and  displayed  in endless  detail.   But  there  are  a  number  of  more  significant applications.   Because  of  their  ability  to  generate  realistic textures at every level of detail, many computer games and simulations  use  fractals  to  generate  terrain  interactively.  Fractals  can  also  be  used  to  compress  large  digital  images into  a  much  smaller  equivalent  by  creating  a  mathemati-cal  transformation  that  preserves  (and  can  be  used  to  re-cre  ate)  the  essential  characteristics  of  the  image.  military experts can use fractal analysis either to distinguish artifi-cial  objects  from  surrounding  terrain  or  camouflage,  or  to generate more realistic camouflage.  Fractals and chaos the-ory are likely to produce many surprising discoveries in the future, in areas ranging from signal analysis and encryption to economic forecasting. ",0
288,"Functional languages have generally been used for special-ized  purposes,  although  they  can  in  principle  perform  any task  that  an  imperative  language  can.   APL,  which  is  basi-cally a functional language, has devotees who appreciate its compact  and  powerful  syntax  for  performing  calculations ,.   LISP  and  its  variants  have  long  been  favored  for many  artificial  intelligence  applications,  particularly  natu-ral language processing, where its representation of data as lists and the facility of its list-processing functions seems a natural fit. Proponents of functional languages argue that they free the  programmer  from  having  to  be  concerned  with  explic-itly setting up and using variables.  In a functional language, problems can often be stated in a more purely mathematical way.   Further,  because  functional  programs  are  not  orga-nized  as  sequentially  executed  tasks,  it  may  be  easier  to implement  parallel  processing  systems  using  functional languages. However, critics point out that imperative languages are much  closer  to  how  computers  actually  work  (employing actual storage locations and sequential operation) and thus produce  code  likely  to  be  much  faster  and  more  efficient than that produced by functional languages. ",0
289,"At  bottom,  a  data  bit  in  a  computer  is  “all  or  nothing” (1  or  0).  most  decisions  in  computer  code  are  also  all  or nothing: Either a condition is satisfied, and execution takes one  specified  path,  or  the  condition  is  not  satisfied  and  it goes  elsewhere.   In  real  life,  of  course,  many  situations  fall between the cracks.  For example, a business might want to treat  a  credit  applicant  who  almost  qualifies  for  “A”  status different  from  one  who  barely  made  “B. ”  While  a  program could be refined to include many gradations between B and A, another approach is to express the degree of “closeness” (or certainty) using fuzzy logic. ",0
290,"eBayeBay Inc.  (NASDAQ symbol: EBAY) is the world’s largest online auction and shopping site.  The first appearance of the auction service was in 1995 as AuctionWeb, part of the personal Web site of Pierre Omidyar ,.  Omidyar was surprised at how rapidly the auction service (which was initially free) grew.  After he imposed a modest listing fee, Omidyar found himself receiving thousands of dollars in small checks, and decided that online auctions could become a full-time business. ",0
291,"An e-book is a book whose text is stored in digital form and can be read on a PC or a handheld reading device.  Since most books today are created on word processors and typesetting systems, it is easy for a publisher to create an electronic version.  Older books that exist only in printed form can be scanned and converted to text. ",0
292,"A digital library is to e-books what a conventional library is to printed books.  Sometimes called an electronic library or virtual library, digital libraries can be created in a variety of ways.  Printed books can now be scanned and digitized rapidly.  Google has said that it can scan 3,000 volumes a day using a proprietary system.  (This is not necessary, of course, for books that were originally created in digital form",0
293,"Advantages of digital libraries include the following:166? ? ? ? e-books and digital libraries, There is never a shortage of copies or the need for a reader to wait for access. ,? Many digital libraries allow full searching of the text of all volumes.  Libraries can also use a common data format (such as “Open Archives. ”) to make their material searchable throughout the Internet. ,? Many older, hard-to-find books can be made more “discoverable” and accessible",0
294,"The most popular e-commerce sectors today include the selling of books, music and movies, travel-related services, electronics, clothing, luxury goods, and medications.  (In 2006, online buyers actually spent more money on clothing than on computers and related products. ) A number of other online activities can be considered part of e-commerce, although they are usually not included in retailing statistics ",0
295,"One continuing obstacle to the growth of e-commerce has been consumers’ concerns about the theft or misuse of personal information gathered as part of the shopping process.  This can involve either fake Web sites , or legitimate businesses that sell information about customers without their knowledge or consent ,.  According to a report from Gartner Research, more than $900 million in e-commerce sales during 2006 was lost because of consumers’ security concerns, and about a billion dollars more in sales was lost because customers decided not to buy online at all",0
296,"In the early 1950s, knowledge of computing tended to have and hoc nature.  On the practical level, computing staffs tended to train newcomers in the specific hardware and machine-level programming languages in use at a particular site.  On the theoretical level, programmers in scientific fields were likely to come from a background in electronics, electrical engineering, or similar disciplines. As it became clear that computers were going to play an increasingly important role, courses specific to computing were added to curricula in mathematics and engineering.  By the late 1950s, however, leading people in the computing field had become convinced that a formal curriculum in computer science was necessary for further advance in an increasingly sophisticated computing arena .  By the early 1960s, efforts at the University of Michigan, University of Houston, Stanford, and other institutions had resulted in the creation of separate graduate departments of computer science.  By the mid-1960s, the National Academy of Sciences and the President’s Science Advisory Committee had both called for a major expansion of efforts in computer science education to be aided by federal funding.  During the 1970s and 1980s, mathematical and engineering societies (in particular the Association for Computing Machinery (ACM) and Institute for Electrical and Electronic Engineering (IEEE) worked to established detailed computer science curricula that extended to undergraduate study.  By 2000, there were 155 accredited  programs in computer science in the United States. ",0
297,"BASIC was invented at Dartmouth College in 1963–1964 by John G.  Kemeny and Thomas E.  Kurtz, both professors of mathematics, assisted by a group of undergraduate student programmers.  Computers then were huge, slow, and expensive; there were no personal computers.  Their goal was to bring easy and accessible computing to all students, not just science or engineering students.  The method they chose called for developing a time-shared operating system, which would allow many users simultaneously.  (This operating system was developed entirely by Dartmouth undergraduate students. ) The new language, BASIC, easy to learn and easy to use, was an essential part of this effort.  BASIC was thus developed originally for a large multipleuser, time-shared system and not for personal computers, which did not appear widely until the early 1980s.  It has been asked whyBASICwas invented.  Couldn’t an existing language have been used for the purpose? The answer to the second question is no, which also answers the first question.  Other computer languages did exist in 1963, although there were not nearly as many as there are today.  The principal ones were FORTRAN and Algol; most of the others are long since forgotten.  Some of the common languages used today—C, C++, and Java—had not even been conceived.  FORTRAN and Algol were each considered briefly.  These languages were designed for production use on big machines or for scientific research, using punched cards.  But neither was suitable for use by beginners, neither was particularly well suited for a time-shared environment, and neither permitted speedy handling of short programs.  Kemeny and Kurtz had experimented with other simple computer languages as early as 1956, but with only modest success.  So, in 1963, when they began building a time-shared system for students, they quickly realized that a new language had to be invented—BASIC. ",0
298,"Timing attacks were publicized by Paul Kocher in 1996.  They attack the implementation of cryptosystems by measuring observable differences in the timing of the algorithm based on the particular value of the key.  They then use statistical methods to determine the bits of key by observing many operations using the same key.  Timing attacks typically require a significant number of chosen ciphertexts.  Related attacks can use any measure of differences in the performance of the encryption and decryption functions such as power consumption and heat dissipation.  Timing attacks and related attacks can be protected against to some degree by “blinding” the devices performing encryption and decryption computations so that all computations have the same performance, regardless of the particular key and message being used.  However, this can have a substantial performance cost, as it requires all computations to have worst-case performance.  Such attacks can also be protected against by designing systems so that they will not act as an “oracle” by decrypting and returning all and any messages that come their way, thereby preventing an attacker from obtaining the necessary data to carry out the attack.  However, this is not always possible without interfering with the purpose of the system. ",0
299,"The  arithmetic  logic  unit  is  the  part  of  a  computer  system that actually performs calculations and logical comparisons on data.  It is part of the central processing unit (CPU), and in  practice  there  may  be  separate  and  multiple  arithmetic and logic units ,. The ALU works by first retrieving a code that represents the operation to be performed (such as ADD).  The code also specifies the location from which the data is to be retrieved and  to  which  the  results  of  the  operation  are  to  be  stored.  (For example, addition of the data from memory to a num-ber  already  stored  in  a  special  accumulator  register  within the CPU, with the result to be stored back into the accumu-lator. )  The  operation  code  can  also  include  a  specification of the format of the data to be used (such as fixed or float-ing-point  numbers)—the  operation  and  format  are  often combined into the same code. In  addition  to  arithmetic  operations,  the  ALU  can  also carry  out  logical  comparisons,  such  as  bitwise  operations that compare corresponding bits in two data words, corresponding to Boolean operators such as AND, OR, and xOR ,. The  data  or  operand  specified  in  the  operation  code  is retrieved as words of memory that represent numeric data, or  indirectly,  character  data  ,.  Once the operation is per-formed,  the  result  is  stored  (typically  in  a  register  in  the CPU).   Special  codes  are  also  stored  in  registers  to  indicate characteristics  of  the  result  (such  as  whether  it  is  positive, negative,  or  zero).   Other  special  conditions  called  excep-tions  indicate  a  problem  with  the  processing.   Common exceptions include overflow, where the result fills more bits than are available in the register, loss of precision (because there  isn’t  room  to  store  the  necessary  number  of  decimal places),  or  an  attempt  to  divide  by  zero.   Exceptions  are typically  indicated  by  setting  a  flag  in  the  machine  status register.  ",0
300,"An  applet  is  a  small  program  that  uses  the  resources  of  a larger program and usually provides customization or addi-tional  features.   The  term  first  appeared  in  the  early  1990s in  connection  with  Apple’s  AppleScript  scripting  language for the macintosh operating system.  Today Java applets rep-resent the most widespread use of this idea in Web develop-ment ,. Java  applets  are  compiled  to  an  intermediate  repre-sentation  called  bytecode,  and  generally  are  run  in  a  Web browser  ,.   Applets  thus  represent  one of  several  alternatives  for  interacting  with  users  of  Web pages  beyond  what  can  be  accomplished  using  simple  text markup  ,. An  applet  can  be  invoked  by  inserting  a  reference  to its  program  code  in  the  text  of  the  Web  page,  using  the HTmL applet element or the now-preferred object element.  Although  the  distinction  between  applets  and  scripting code  (such  as  in  PHP)  is  somewhat  vague,  applets  usually run  in  their  own  window  or  otherwise  provide  their  own interface,  while  scripting  code  is  generally  used  to  tailor the  behavior  of  separately  created  objects.   Applets  are  also rather  like  plug-ins,  but  the  latter  are  generally  used  to provide  a  particular  capability  (such  as  the  ability  to  read or  play  a  particular  kind  of  media  file),  and  have  a  stan-dardized facility for their installation and management ,. Some  common  uses  for  applets  include  animations  of scientific or programming concepts for Web pages support-ing  class  curricula  and  for  games  designed  to  be  played using  Web  browsers.   Animation  tools  such  as  Flash  and Shockwave are often used for creating graphic applets. To  prevent  badly  or  maliciously  written  applets  from affecting  user  files,  applets  such  as  Java  applets  are  gen-erally  run  within  a  restricted  or  “sandbox”  environment where, for example, they are not allowed to write or change files on disk.  ",0
301,"In  personal  computers  a  chipset  is  a  group  of  integrated circuits that together perform a particular function.  System purchasers  generally  think  in  terms  of  the  processor  itself (such  as  a  Pentium  III,  Pentium  IV,  or  competitive  chips from AmD  or  Cyrix).   However  they  are  really  buying  a system  chipset  that  includes  the  microprocessor  itself  , and often a memory cache (which may be part of the microprocessor or a separate chip—see cache) as  well  as  the  chips  that  control  the  memory  bus  (which connects  the  processor  to  the  main  memory  on  the  moth-erboard—see bus. )  The  overall  performance  of  the  system depends not just on the processor’s architecture (including data  width,  instruction  set,  and  use  of  instruction  pipe-lines)  but  also  on  the  type  and  size  of  the  cache  memory, the memory  bus  (RDRAm  or  “Rambus”  and  SDRAm)  and the  speed  with  which  the  processor  can  move  data  to  and from memory. In addition to the system chipset, other chipsets on the motherboard are used to support functions such as graphics (the AgP,  or  Advanced graphics  Port,  for  example),  drive connection  (EIDE  controller),  communication  with  exter-nal devices ,, and connections to expansion cards (the PCI bus). At  the  end  of  the  1990s,  the  PC  marketplace  had  chip-sets  based  on  two  competing  architectures.   Intel,  which originally  developed  an  architecture  called  Socket  7,  has switched  to  the  more  complex  Slot-1  architecture,  which is  most  effective  for  multiprocessor  operation  but  offers the advantage of including a separate bus for accessing the cac  he memory.  meanwhile,  Intel’s  main  competitor,  AmD, has  enhanced  the  Socket  7  into  “Super  Socket  7”  and  is offering  faster  bus  speeds.   On  the  horizon  may  be  com-pletely  new  architecture.   In  choosing  a  system,  consumers are locked into their choice because the microprocessor pin sockets used for each chipset architecture are different. ",0
302,"A  class  is  a  data  type  that  combines  both  a  data  structure and  methods  for  manipulating  the  data.   For  example,  a string  class  might  consist  of  an  array  to  hold  the  charac-ters in the string and methods to compare strings, combine strings,  or  extract  portions  of  a  string  ,. As  with  other  data  types,  once  a  class  is  declared, objects  (sometimes  called  instances)  of  the  class  can  be created  and  used.   This  way  of  structuring  programs  is called   object-oriented   programming   because   the   class object  is  the  basic  building  block  ,. Object-oriented  programming  and  classes  provide  sev-eral advantages over traditional block-structured languages.  In  a  traditional  BASIC  or  even  Pascal  program,  there  is no  particular  connection  between  the  data  structure  and the  procedures  or  functions  that  manipulate  it.   In  a  large program  one  programmer  might  change  the  data  structure without  alerting  other  programmers  whose  code  assumes the  original  structure.   On  the  other  hand,  someone  might write  a  procedure  that  directly  manipulates  the  internal data rather than using the methods already provided.  Either transgression can lead to hard-to-find bugs. With  a  class,  however,  data  and  procedures  are  bound together,  or  encapsulated.   This  means  that  the  data  in  a class  object  can  be  manipulated  only  by  using  one  of  the methods  provided  by  the  class.   If  the  person  in  charge of  maintaining  the  class  decides  to  provide  an  improved implementation  of  the  data  structure,  as  long  as  the  data parameters  expected  by  the  class  methods  do  not  change, code  that  uses  the  class  objects  will  continue  to  function properly. ",0
303,"access Control Matrix means preparing a system permitting to bring together user and the system’s available resources.  Preparing a matrix requires the following steps: A broad level of groups of resources that share the same or similar security objectives is established based on requirements for Confidentiality, Integrity, Availability, User Accountability, Authentication and Audit (Compliance) (CIA UAA).  The resources include files, directories, applications, databases, hosts services, processes and others including those protected by the operating system or by other mechanisms.  Users are grouped according to common security needs into functional teams and a group owner is identified who is responsible for group management If necessary, naming standards for resources can be defined based on the findings under 1 & 2 above, thereby further facilitating resource groupings.  Now a decision can be made on the degree of access such as read, write, execute, take ownership, access control, delete, purge, modify, file scan ,.  Balancing central security control and group administration decides about the involvement of security administration.  A corporation’s access control matrix should not become a static document but, instead, regular review and improvement is a must.  This matrix is closely linked to a Role-Based Access management system, whereby user roles (e. g. , nurse versus private or general physician) have roles to perform requiring access and modification of data or addition of new information regarding an electronic patient file. ",0
304,"Active OS Fingerprinting is conducted in order to detect the target’s Oper ating System.  It is usually done by sending specially crafted network packets and comparing them against known responses.  Each operating system re sponds to different packet differently and even response to the ping command can give a good indication of the target OS.  Several methods have been developed and fingerprints are widely available on the internet.  The tool of the trade for this is a tool called Network Mapper (nmap).  While Active OS Fingerprinting is more accurate than Passive OS Fingerprinting, it has a significant disadvantage.  Being active, means that attackers have to send packets to the host, hence, Active versus passive Fingerprinting means risking discovery ,. ",0
305,"is a method used for dynamic web sites/pages thatincludes one or more scripts (small embedded programs) that are processedon a web server before the page is sent to the user.  An ASP is somewhat similarto a server-side include or a common gateway interface (CGI) application inthat it involves programs that run on the server, usually tailoring a page forthe user.  Typically, the script in the web page at the server uses input receivedas the result of the user’s request for the page to access data from a database,thereafter it builds or customizes the page on the fly before sending it to therequestor. ",0
306,"are similar to Java applets in that ActiveX controls may be includedwithin a web page.  The control is downloaded and executed on the browser’scomputer in the form of a pre-compiled executable.  Unfortunately, ActiveXdoes not enforce any form of security management technology.  Hence, ActiveX control has the same level of control of the client computer as theuser that is executing the browser.  ActiveX controls are specific to MicrosoftInternet Explorer (MSIE). ActiveX controls are elements that can be added to web pages, therebyproviding them with more functionality (e. g. , animation, video, and threedimensional browsing).  The controls are written in programming languagessuch as Visual Basic, C or C++.  They are written in a different code than theone used for the web pages itself such as HTML.  They could, however, beinfected with malicious code (Malware). Whatever risks are associated with running native executables on a computer also apply to ActiveX.  How security of ActiveX controls is handled isat the user’s discretion who runs the browser.  Without appropriate training this may be risky, justifying disabling this functionality within webbrowsers by setting security settings to ‘high,’ ActiveX is prevented fromrunning",0
307,"Advisory and Notification Markup Language (ANML) is an Extensible Markup Language (XML) -based specification for describing advisories and other types of notifications.  ANML is currently being developed to help in solving the inconsistent use of terminology by software vendors in their advisories.  The hope is that ANML will make it easy for applications to read these advisories.  This will make way for the necessary tools to automatically update systems.  Although ANML will have its biggest impact for security advisories, it can be used for any type of notification.  Some examples include bug-fixes, feature enhancements, upgrade availability, and many more.  More information can be found in Appendix 1 (Opensec) and Appendix 6 under ADML",0
308,"pronounced AL-go-rith-um is a procedure or formula for solving a problem.  The word derives from the name of the mathematician, Mohammed ibn-Musa al-Khwarizmi, who was part of the royal court in Baghdad and who lived from about 780 to 850.  Al-Khwarizmi’s work is the likely source for the word algebra as well.  Algorithm is any well-defined procedure (does not have to be computa tional) that takes some value or set of values as input and produces some value or set of values as output.  It is a technique that comes with a guarantee.  The technique for ratio nal function integration is an algorithm because it always produces the an swer, without exception.  Differentiation is an algorithm—given an elemen tary function, you can always find its derivative.  Algorithms can be fast or slow, but the important thing is the guarantee.  In some sense, an algorithm is a ‘predictable heuristic,’ whereby one cannot tell if a heuristic will work, before one tries it, but one knows in advance what the output of an algorithm will be.  In computing terms, algorithm is a sequence of computational steps that transform the input to the output.  A computer program can be viewed as an elaborate algorithm.  In mathe matics and computer science, an algorithm usually means a small procedure that solves a recurrent problem.  In this context the term Virus Algorithm means a set of operations or a procedure designed to create a virus problem.  In the context of Digital Signatures or Encryption, the algorithm describes how the signature or text is encrypted using mathematical formulas Another example is the Condensation algorithm (Conditional Density Propagation) that allows quite general representations of probability.  The simplicity of the Condensation algorithm also allows the use of non-linear motion models more complex than those commonly used in Kalman filters.  Using the statistical technique of importance sampling it is possible to build a Condensation tracker which runs in real time tracking a dancer or a hand as they move",0
309,Apache is a freely available multiplatform web server.  It is currently the most commonly used server on internet connected sites.  Its genesis was in early 1995 when developers of some high visibility web sites decided to pool their patches and enhancements to the NCSA / 1. 3 server to create a patchy server.  The project has since gained considerable momentum. ,0
310,"ASCII (American Standard Code for Information Interchange) is a series of standards used to identify simple characters (numbers, letters, etc. ) where each character is represented by a numerical code.  Each character is assigned a 7-bit code, whereby the bit has a minimum unit of data, 0 (zero) or 1.  ASCII files can be created using simple text editors.  ASCII is a universal computer code for English letters and characters.  Computers store all information as binary numbers, regardless of what make or brand the computer is.  ASCII also refers to a protocol for copying files from one computer to another over a network, in which neither computer checks for any errors that might have been caused by static or other problems.  The difficulty with ASCII is twofold: 1) 2) all special fonts and elements used in a document typed using one software (e. g. , WordPerfect) will be lost when the file / document is saved in ASCII format and than reloaded by somebody else into WordPerfect (or even Microsoft Word for that matter), and special characters such as those used in German cannot be transferred by ASCII (e. g. , ü is transferred as a blank and an ue must be typed instead).  Besides German, for many languages (e. g. , Chinese and Japanese), this represents a real problem.  For the above reasons, a new source called Unicode was developed ,.  As 8 bit codes only allow for 256 separate characters, a single 8-bit en coding is not suitable for all text requirements.  Even western alphabets have many ‘characters’, and once languages of China, and Braille are included there are many thousands of characters that are needed.  Of course, within a given domain, an implicit assumption of a particular character set could be made.  This leads to multiple 8-bit codes.  But it doesn’t work for those character sets where more than 256 characters are needed in the same context.  Both 7-bit ASCII and the various 8-bit extensions of it can be considered to be subsets of Unicode.  One does not need to know the above level of detail, but it is important to see why ASCII is insufficient, and why a standard is required for character encodings ,. ",0
311,"Asymmetrical Digital Subscriber Line (ADSL) allows voice, video anddata to be transmitted over a single telephone line at up to 6. 144 megabitsper second (Mbps) in a single direction, with significantly slower speedsin the other direction.  Accordingly, it is very appropriate for downloading large amounts of data but for certain applications, such as videoconferencing, other technologies work better [see also Very-High-bit-rateDigital Subscriber Line (VDSL), Discrete Multi-Tone (DMT) modulation,Quadrature Amplitude Modulation (QAM-VDSL)]. ",0
312,"Attack is a single unauthorized access attempt, or unauthorized use attempt,regardless of success.  Success may or may not result in the alteration, releasing or denying of data.  The likelihood of success depends on the effectivenessof implemented security measures that reduce the risk and probability for athreat resulting in a compromised system ,.  Computernetworks make it easier to start attacks and speed their dissemination, or forone anonymous individual to reach vast numbers of people at virtually nocost. Defending against Attacks means engineering in a world ruled by Satan’sLaw.  The differences between Attacks and Accidents are intent, intelligence,and control.  Things go wrong because there is a malicious and intelligentadversary trying to force things to go wrong ,Table 3A outlines a taxonomy of the attacks that can be launched againstinformation resources and infrastructure.  ",0
313,Authentication results in positively verifying the identity of a user‚ device‚ or another entity in an information system.  This is often a prerequisite for allowing access to resources offered by a system.  One element that contributes to the reliability of individual authentication is good password management practices.  In an area of high risk‚ stronger authentication may be required such as: Asymmetric Keys—see Access Control Biometrics—see Biometrics Cryptographic Tokens—see Cryptography Digital Certificates—see Digital Certificate Smart Cards—see Smart Card One-Time Password Generators—where password can be used once only.  Stronger authentication relies on combining one or more of the following: password / Pin or something else user knows‚ token or other means‚ that is something user has‚ and finally‚ biometrics or other technique enabling system to identify who the person is.  The above means may be used to verify the true source of a message or data.  But all approaches have their weak spots and combinations of measures are usually more effective.  In case of electronic voting‚ the term refers to verification that an electronic ballot really comes from the person it claims to have been initiated by‚ and not from an imposter. ,0
314,Backdoor is a hidden feature of an application prepared by its designer ormaintainers.  The backdoor gives the programmer special privileges for running the application.  These privileges are not available to the user. The motivation for such holes is not always sinister; some operating systems‚ for example‚ come out of the box with privileged accounts intended foruse by field service technicians or the vendor’s maintenance programmers. But we are not sure if such an account should be called backdoor – in ourunderstanding‚ backdoor is a hidden feature.  Yes‚ these accounts were sometimes hidden‚ Microsoft NT 3. 51 and then they could be called backdoors‚but generally they are not hidden. ,0
315,Batch Files are files usually describing automated sequence of commands. These are used in mainframe operations where even special languages weredeveloped for creation of batches.  Shell scripts known in the UNIX worldcan also be considered in some cases as batch files.  In MS-DOS and MSWindows context‚ these files are characterized by a . BAT extension.  Thesetext files contain MS-DOS commands‚ one on each line of the file that areprocessed in sequential order‚ for example when the computer is booted (e. g. ‚Autoexec. bat). ,0
316,"Binders are programs permitting hackers to ‘bind’ two or more programstogether resuling in a single . EXE file.  Hence‚ harmless . EXE animations‚e-greetings or other such files could have a Trojan horse inserted , ,. The only way to stop an executable from harming a PC or system is to run itin a proactive ‘sandbox’ environment and monitor its behavior for maliciousactivity in real-time. ",0
317,"Biometrics identification systems are based on the recognition of unique morphological characteristics of each individual‚ so that only the physical presence of the person will allow access to the system.  Numerous biometric parameters are available such as: facial‚ iris‚ fingerprints‚ hand and finger geometry‚ signature and voice recognition.  Although iris recognition has stronger security credentials‚ fingerprints are often preferred due to the maturity of the technology ,.  As Table 4A outlines‚ Retina Scan‚ Iris Scan and Hand Geometry are most accurate and both‚ the False Rejection Rate (FRR) and the False Acceptance Rate (FAR) are quite low for these Authentication measures based on Biometrics.  In Table 4B other measures are outlined that can be used for Access Control.  Often a system may use a Magnetic Swipe Card and a Keypad Entry System to give an employee access to a restricted office area or a database.  Some of the time a system such as the ones outlined in Table 4B may also be combined with one of those described and explained in Table 4C.  Unfortunately‚ all the one’s in Table 4C do have‚ however‚ some severe weaknesses that limit their Reliability and Validity.  In particular the FRR and FAR may not be satisfactory.  For instance‚ during May 2002 a Japanese group of researchers reported that using fake fingerprints fashioned from gelatin were able to fool biometric fingerprint readers 80% of the time.  The researchers also devised a way to create fake fingerprints from fingerprints left on glass surfaces.  What is interesting is that the material needed to make this work‚ was purchased at a neighborhood store and cost less than Euro 11.  This attack is a classic replay (or forgery) attack.  Unfortunately‚ replays are not unique to fingerprints but are a fundamental Vulnerability of all Biometrics ,.  The American Civil Liberties Union (ACLU) published a report during May 2002 about the inaccuracy of tests using facial recognition technology at the Palm Beach (FL) International Airport.  The technology failed to correctly identify faces more than half of the time.  The recognition rate went down when people wore glasses‚ turned their heads‚ or were moving. ",0
318,"Bluetooth is short-range radio that can be stuck on anything‚ including a mobile phone or computer as well a ticket barrier‚ or a trolley full of supermarketgoods.  In turn‚ users can use the technology to connect various devices (e. g. ‚computer and stereo) or pay at the train waving their Bluetooth enabled technology (e. g. ‚ card or mobile phone) to have the fare added to their monthlybill ,. Bluetooth was named after a century Danish King‚ Harald Bluetooth. The standard was founded by Ericsson in 1994 but it was joined by Nokia‚IBM and Intel in 1998 to form the Bluetooth Special Interest Group thatcampaigns for further Bluetooth adoption ,. ",0
319,Blue Screen of Death (BSOD) is a term used in relations to Microsoft Windows operating systems.  Some crashes of these systems appear as a bluesscreen with white hexadecimal text (part of memory dump).  Many usersconsider this a useless information‚ however it can be often used to detectmisbehaving programs (often not from Microsoft) causing these fatal crashes,0
320,Broadband is high-speed internet access.  Broadband may be delivered to private households with Asymmetrical Digital Subscriber Line (ADSL)‚ DigitalSubscriber Line (DSL)‚ Cable TV lines‚ Fixed Wireless Access or even satellite supported broadband in more remote areas,0
321,bulletin Board System(s) (BBS) are a type of online computer service thatfunctions as an electronic notice board.  Users can read or post messages‚download programs‚ and play online games.  Some functions of a BBS aresimilar to that of the internet‚ but on a smaller scale. ,0
322,"Business Continuity Plan (BCP) is a plan to ensure that the essential business functions of the organization are able to continue (or re-start) in the event of unforeseen circumstances; normally a disaster of some sort ,.  The BCP follows the Disaster Recovery Plan ,.  BCP identifies the critical people and their roles and functions while the BCP is in force‚ information‚ systems‚ and other infrastructures (e. g. ‚ telephones‚ servers‚ SMS gateways) that are required to enable the business to operate again fully.  The BCP lays out a detailed plan which‚ if called upon‚ should be executed to assure minimum additional disruption after a massive computer Virus Attack or disaster.  Companies that do not have clear business continuity plans remain very vulnerable.  Good recovery from a disaster is‚ however‚ contingent upon people‚ processes and training.  Finally‚ management awareness may be the main weakness in business continuity planning.  Doing the basics well is they key‚ whereby if one says one will take his or her data off site within 24 hours of a calamity‚ one must make sure that this is being done.  Hence‚ fire drills or ‘dress rehearsals’ by simulating various disaster scenarios will permit the organization to be in a better position‚ while recovering faster after a disaster.  Hence‚ business continuity and disaster recovery plans need to be exercised so that kinks may be worked out before the crisis hits. ",0
323,Disk Cache is a temporary storage place that a computer can use to storea file after reading it from the hard drive.  For instance‚ telling a computerto open a MP3 file‚ it may take several seconds for it to locate and read thefile into memory from the PCs hard drive.  However‚ if the computer storesthe file in the Disk Cache‚ the next time one wishes to open the same file‚data can be retrieved from the disk cache rather than loading it from the harddrive. One definition refers to a Disk Cache specifically as: hard disk-based memory used to store accessed web pages.  This technique enables the browserload the stored pages from the cache rather than from the network.  That iswhy clicking the ‘Back’ button on a browser usually retrieves a page nearlyinstantaneously.  The virtual memory system that comes with Microsoft Windows is also another example of disk caching to increase performance,0
324,File Cache is used to store the locations of frequently used files for quickreference.  When open a file that is stored on a hard drive‚ the computer willfirst check for the file name (and location) in the file cache.  If it finds thisinformation‚ the computer can jump immediately to the correct place on ahard drive without having to search in the file allocation table (a type of tableof contents for a computer’s hard drive).  Because read/write heads on a harddrive have to physically move across the disk to search for them‚ it takes timefor the computer to search the hard drive for files‚ that is why using the cacheis fasterISPs may also be required to pay copyright fees when caching in the nearfuture. ,0
325,Web Cache is closely related to Disk Caching and occurs between webservers and a client or many clients.  It watches requests for HTML pages‚images and files to come by‚ saving a copy for itself.  Then‚ if there is anotherrequest for the same object‚ it will use the copy that it has‚ instead of askingthe origin server for it again. This type of caching may‚ however‚ infringe upon copyrights and publishers and music distributors are trying to find a solution for this problem,0
326,omputer Security Incident Response Team (CSIRT) is a service organization that is responsible for receiving‚ reviewing‚ and responding tocomputer security Incident reports and activity. A CSIRT usually performs these services for a defined constituency or agroup of stakeholders.  These could a firm or a g,0
327,"certification Authority is a trusted third party clearing house issueing DigitalCertificates and Digital Signatures.  For firms‚ the certificates include thecorporation’s name‚ a serial number‚ and an expiration date.  A public key isalso enclosed to allow others to decrypt the message.  The digital signature ofthe certificate-issuing authority is also part of this‚ permitting the recipientto verify that the certificate is valid ,. The challenge is that in some countries‚ the government may require thatthe Key Recovery facilities must be exercised within its national borders. For instance‚ how this may affect citizens who have a Digital Signature todo e-government business with their local government when wanting to doa transaction with the EU or EC or another EU member state’s agency stillneeds to be addressed.  Will these other governments accept that the KRfacility is exercised in Denmark instead of‚ for instance‚ France?",0
328,"checksum is a mathematical method whereby the individual binary values ofa string of storage locations on a computer’s hard drive are summed up, andthe total is then retained for future reference.  On subsequent accesses, thesumming procedure is repeated, and the total compared to the one derivedpreviously. A difference indicates that an element of stored data has changed duringthe intervening period.  Agreement provides a high degree of assurance (butnot total assurance) that data have not been changed during the interveningperiod. A checksum is also used to verify that a network transmission has beensuccessful.  If the counts agree, it is safe to assume that the transmission wascompleted correctly ,. ",0
329,"Click Wrap Agreement checks if a user meets certain requirements, by clicking on the I Agree button, the user has agreed to complete terms of use agreement including another confirmation that the user meets certain requirements(e. g. , working at an educational institutions or being located in a country theproduct can and may be used).  To complete the agreement, the user mayagain be required to scroll to the bottom of the agreement and click on the “IAgree” icon [see also End User License Agreement (EULA), Jurisdiction]. This is also sometimes called a Shrink Wrap Agreement, whereby rippingoff the shrink wrap around the software implies that the user will take advantage of the software according to the conditions stipulated by the vendor",0
330,"Command and Conrol Warfare (C2W) is an integrated use of electronicwarfare, military deception, operations security, and also physical destruction.  These activities are supported by intelligence in order to deny information or degrade or destroy adversary command and control capabilities. Friendly C2 activities are protected C2W is an application of information operations in military operations and a subset of information warfare.  Hence,it can be offensive and defensive raising ethical questions for non militaryapplications if ever used. ",0
331,"Computer Metal Oxide Semiconductor (CMOS) is a section of the RAMmemory containing important data, such as date and time of the computerclock, and its configuration settings.  A battery that is housed on the computermotherboard powers the CMOS.  While a virus can overwrite information onthe CMOS, it can neither infect nor reside in it. ",0
332,"Confidentiality is the property that data or information is not made availableor disclosed to unauthorized, parties (e. g. , individuals, organizations andprocesses).  Hence, a health insurer cannot get access to medical files as suchbut only to information pertaining to a particular bill submitted electronicallyfor reimbursement",0
333,"cookies are data files that are stored on a hard disk by some web sites. Whenever visiting a Website, it tries to access this cookie enabling usersto enter the site according to one’s preferences (e. g. , first page shows news,email and horoscope).  Cookies may also store one’s password to access a siteunless the user prevents the browser from doing so, a must for any securityminded user.  Additional information is collected from subsequent browsingon the web site, to further personalize one’s browsing experience on this website (e. g. , customer profile). Cookies are accessible to a particular Website only and cannot be used byother Websites to gain information about the user",0
334,"Critical Infrastructure Protection (CIP) tries to coordinate efforts to protectcritical infrastructure that may include but not be limited to informationtechnology and utility grids (e. g. , electricity).  Critical Infrastructure mayinclude essential services such as telecommunication, finance/banking, foodproduction, transport and logistics, energy and utilities as well as criticalgovernment services. One also should consider that in many countries much of the CriticalInfrastructure (e. g. , electricity, gas distribution and shipping) is privatelyowned. Critical Infrastructure Protection may also include Emergency Preparedness.  Its mission is usually to enhance the safety and security of citizens orusers in their physical and cyber environments. Mandates could be such as:to provide a comprehensive approach to protecting critical infrastructure—thekey physical and cyber components of the energy and utilities, communications,services, transportation, safety and government sectors; andto be a government’s primary agency for ensuring national civil emergencypreparedness–for all types of emergencies. ",0
335,"Critical Information Infrastructure (CII) may include the information technology component (e. g. , hardware, software and data) of essential ser[1]vices, such as telecommunication, finance/banking, food production, trans[1]port and logistics, energy and utilities as well as critical government services .  One also should consider that in many countries much of the CII for essential services such as electricity supply lines are is privately owned.  CII may be vulnerable to an outside dimension that comes from disruption through natural disasters, accidents and mismanagement as well as deliberate Attack with criminal intent.  An inside dimension is Safety and System-related issues including Complexity. ",0
336,"cyberpunks embody behaviors similar to what we define as Crackers andphone phreaks but, in addition, they use technology to damage, destroy orcapitalize on the data they find (e. g. , for profit).  Most importantly, cyberpunksuse their know how to gain more information which, in turn, increases theirinfluence, power and potential threat upon others’ information world.  Mostof this activity is done at somebody else’s expense by, for example, breakingand entering into an organization’s voice mail system.  Gaining unauthorizedaccess and using IT resources and process while not being authorized to doso ,. Similar to the things we learned in Economics 101, demand and supply ofgoods are related.  Accordingly, cyberpunks can be compared to “fences” inNew York who sell stolen merchandise at very “attractive” prices.  As longas there is a consumer who is willing to go home with a “good” deal, even ifit is at the expense of the victim whose apartment / car was broken into, thenthe supply will continue.  If we stop purchasing from fences supply will drop;if cyberpunks find no demand for their illegally acquired information and,therefore fail to sell their goods, supply will be reduced ,. Finally, providing individual cyberpunks with five minutes of fame doesfurther reinforce the cool image or status the individual is striving for withinhis or her social group.  Lack of attaining fame, status or other benefits (e. g. ,being on a TV show) will also help reduce the supply of cyberpunks. ",0
337,"Cyclic Redundancy Check (CRC) is a technique used to determine fileintegrity through the generation of a CRC algorithm.  The CRC algorithmcalculates the CRC of the file in question.  This way a protection system canbe established, whereby if something changes, the CRC also changes.  Thispermits the detection of any possible modifications",0
338,"Darknets are a collection of networks and technologies used to share digitaland other content or objects (e. g. , software programs, songs, movies, andbooks) and have substantial non-infringing uses.  The darknet is not a separatephysical network but an application and protocol layer riding on existingnetworks. Darknets offer distributed object storage, whereby today’s Darknets do notrely upon any centralized server or service—a peer just needs the IP addressof one or a few participating peers to be able to reach any host on the Darknet. Also important is that with the open protocol, anyone can write a clientapplication for a particular Darknet.  Second, Gnutella is not really “run”by anyone: it is an open protocol and anyone can write a Gnutella clientapplication. Members-only darknets are popping up to protect file sharing from prying eyes.  Inn",0
339,"Data Encryption Standard (DES) has been the most popular encryptionalgorithm of the past twenty-five years.  Originally developed at IBM Corporation, it was chosen by the US’ National Bureau of Standards (NBS) asthe government-standard encryption algorithm in 1976.  Since then, it has become a domestic and international encryption standard, and has been used inthousands of applications.  Concerns about its short key length have doggedthe algorithm since the beginning, and in 1998 a brute-force machine capableof breaking DES was built.  Today, modifications to DES, such as triple-DES,ensure that it will remain secure for the foreseeable future",0
340,"Demilitarized Zone (DMZ) is a separate and shielded or ‘cut off’ systemfrom the main corporate network, containing technical equipment such asthe Webpage.  This further protects the main system from being accessed byexternal parties or via the InternetThe term comes from the buffer zone that was set up between North Koreaand South Korea following their war in the early 1950s.  A DMZ is not a singlesecurity component; it signifies a capability.  Within the DMZ one can find:firewalls,choke and access routers, and alsofront-end and back-end servers. Essentially, the DMZ provides multi-layer filtering and screening to completely block off access to the corporate network and data.  And, even wherea legitimate and authorised external query requests corporate data, no directconnection will be permitted from the external client, only a back-end serverwill issue the request (which may require additional authentication) fromthe internal corporate network.  How much data may be accessible from theoutside is also depending on asset values that may be represented by thesedata",0
341,"is considered to take place only when accessto a computer or network resource is intentionally blocked or degraded as aresult of malicious action taken by another user.  While these attacks coulddamage data directly or permanently they do not have to.  However, theyintentionally compromise the availability of the resources.  An attacker carriesout a denial-of-service attack by making a resource inoperative, by taking upso much of a shared resource that none of the resource is left for other users,or by degraded the resource so that it is less valuable to users.  Those sharedresources are reached through processes and can include other processes,shared files, disk space, percentage of CPU, modems, etc.  DoS is an attack based on the principles of the TCP Three-way Handshakeexploitation.  It was very popular in late 1990s.  Flooding system by speciallycrafted packets from one host could disable its networking capabilities andin some cases even force it to reboot.  Majority of the exploitable holes werepatched and this form of attack is nowadays nearly impossible. ",0
342,"Digital Certificates is the electronic version of an ID card that establishesone’s credentials and authenticates a person’s connection when performinge-commerce or e-government transactions. To obtain Digital Certificate an organization or individual must apply toa Certifications Authority which is responsible for validating and ensuringthe authenticity of requesting organization.  The Certificate will identify thename of the organization, a serial number, the validity date (“from / to”) andthe organization’s Public Key where encryption to / from that organizationis required. In addition, the Digital Certificate will also contain the Digital Signature ofthe Certification Authority to allow any recipient to confirm the authenticityof the Digital Certificate. A global standard (X.  509 Public Key Infrastructure for the Internet) defines the requirements for Digital Certificates and the major Certificate Authorities conform to this.  Such standards, and the integrity of the CertificateAuthorities are vital for the establishment of ‘digital trust’, without whiche-Commerce will never attain its full potential. ",0
343,"Digital Rights Management (DRM) (sometimes also called Digital Information Management) lets a document owner define how recipients can handledocuments in terms of forwarding, copying, and printing them.  Also allowsthe DRM holder to determine expiration dates for those permissions.  A document owner can also designate sections of a document that only certainpeople can change, force the use of revision marks for changes, and force theuse of certain formatting and styles.  Microsoft has integrated the same typeof functionality into Office Excel 2003 and Office Outlook 2003. To enable users to take advantage of DRM features in Office 2003, Windows Rights Management Services (RMS) for Windows Server 2003 hasto be implemented first on the network.  RMS is based on the ExtensibleRights Markup Language (XrML), which is a method for defining rights andpolicies. ",0
344,"Digital Signature uses public-key algorithm to generate a digital signature,which is a block of data used to create some authentication ,.  When a judge sees a digital signature, he or she does not knowanything about the signatory’s intentions.  He doesn’t know if the personagreed to the document as one being presented with a notarized signature. Nor do we know if the signatory ever saw the signed document.  The problemis that:while a digital signature authenticates the document up to the point of the signingcomputer,it does not authenticate the link between that computer and the individual. Digital signatures prove, mathematically, that a secret value known as theprivate key was present in a computer at the time the person’s signature wascalculated.  It is a small step from that inferring that the individual enteredthat key into the computer at the time of signing.  But it is a much largerstep to assume that the individual actually intended a particular documentto be signed.  And without a tamperproof computer trusted by the signingindividual, one can expect “digital signature experts” to show up in courtcontesting a lot of digital signatures. ",0
345,"Public Key (PKI) is used to verify that the signature was really generatedusing the corresponding private key.  Public keys are often registered witha third party and can be downloaded so the person can check if the key isgenuine.  (See also Digital Signatures). Public Key Infrastructure is needed as a means of generating and managing the encryption keys is required.  It is the use and management of cryptographic keys-a public key and a private key-for the secure transmission andauthentication of data across public networks.  Vendor systems do, however,differ (Key Recovery, Encryption-Authorization). ",0
346,Digital Watermark is a unique identifier being part of a digital document. The watermark is invisible to the human eye but a computer can analyze the document and extract the hidden data.  The watermark cannot beremoved. The primary use of such marks is to allow different marks to be used whenthe document is copied to different persons and thereby establish an AuditTrail should there be any leakage of information. ,0
347,"Disaster Recovery Plan deals with the immediate crisis and tries to securecritical IT infrastructure by preventing further spread or continuation of thecrisis such as a computer virus or a denial of service attack ,. The Disaster Recovery Plan is needed to make sure that when the disasterstrucks, people are being taken to safety quickly, or the further spread of aVirus or Trojan horse can be prevented.  Also, with the help of the BusinessContinuity Plan, systems can go online again immediately if need be or elsewithin the time frame planned and agreed upon beforehand ,",0
348,"Distributed Denial-of-Service Attack (DDoS) is a distributed version ofDoS.  Many hosts are used to send packets to a target host.  This way the hostis flooded by a high amount of traffic and similar results to DoS are achieved,. Administrators should check any systems connected to the internet frequently for the presence of DDoS software that could be used to attack othernetworks by following the steps as outlined in Table 8. Moreover, unnecessary ports should be closed.  For instance, during August 2003 MSBlaster took advantage of known vulnerable network ports inWindows, ports that should have been closed. ",0
349,"The Domain Name System (DNS) is the Internet's system for mapping alphabetic names to numeric Internet Protocol (IP) addresses like a phone book maps a person's name to a phone number.  For example, when a Web address (URL) is typed into a browser, a DNS query is made to learn an IP address of a Web server associated with that name. Using the www. example. com URL, example. com is the domain name, and www is the hostname.  DNS resolution maps www. example. com into an IP address (such as 192. 0. 2. 1).  When a user needs to load a webpage, a conversion must occur between what a user types into their web browser (www. example. com) into an IP address required to locate the www. example. com site. The DNS system is an open worldwide network of database name servers that include 13 authoritative name servers that serve the DNS root zone level, known as ""root servers"".  A root server (also called a DNS root nameserver) receives a DNS query that includes a domain name (e. g.  www. thousandeyes. com), and responds by directing that request to a top-level domain (TLD) nameserver, based on the TLD of that domain such as . com, . net, and . org.  It directly responds to requests for DNS records in the root zone by returning an appropriate list of the authoritative TLD name servers for the appropriate TLD that can resolve the initial DNS lookup request for an IP address of that domain name. ",0
350,"A dynamic link library (DLL) is a collection of small programs that larger programs can load when needed to complete specific tasks.  The small program, called a DLL file, contains instructions that help the larger program handle what may not be a core function of the original program. An example of those tasks might be communicating with a specific device, such as a printer or scanner to process a document.  DLL files that support specific device operations are known as device drivers. DLL contains bits of code and data, like classes and variables, or other resources such as images that the larger program can use. In addition to being a generic term for dynamic link libraries, Dynamic Link Library is also the name of Microsoft's version of the shared library concept for Windows.  A shared library can exist in any operating system (OS). ",0
351,"e-Government is the public’s efforts to bring dealings with the governmentonline, thereby enabling citizens to conduct most of their businesses (e. g. ,ordering a passport) online.  Accordingly, while the UK wants every government transaction to be offered online by 2005, Denmark has chosen toprovide all of its citizens with digital signatures to enable them to do all theirtransactions online ,. Pushing e-Government initiatives requires satisfactory IT security of information, data and protection of people’s privacy which can be a challenge. For instance in 2002, Canada’s Auditor General released a report in which itstated that government sites do not seem to do well as far as privacy and datasecurity are concerned.  His 2003 report acknowledged some improvementsbut did not yet give e-Government sites a passing grade.  Quite likely, othergovernments may have to improve on this score as well. As Table 9 suggests, putting down a policy about e-government and ITinitiatives is important.  The hard work follows thereafter.  Providing everycitizen with a digital signature is a start but, without addressing the issuesin Table 9, it is unlikely to improve service for citizens.  Accordingly, governments will have to learn how to manage the changed relationships withtheir citizens and business thanks to e-government.  Also, Identity Theft maybecome an issue thanks to increased use of digital signatures by citizensinteracting with the government. ",0
352,"eMail or Electronic Mail is an electronically transmitted message which arrives as a computer file on a user’s PC or the corporation’s server.  Originallyconceived as a simple means of sending short messages from one computer toanother, the Simple Mail Transfer Protocol (SMTP) was introduced withoutsecurity in mind. Whilst standards have been agreed for the attachment of files to eMailmessages, be aware that such files can contain Malicious Code such as a Virus. eMail is insecure because:Message can purport to have been sent from an individual, whilst it was sentfrom somebody else, possible misrepresentation;The From field may have been modified to indicate a sender that is fallacious ordoes not exist;Because there is no Authentication, the eMail can be opened by anyone unlessit is encrypted ,;When sending eMail, the sender has no guarantee that the recipient has receivedit after passing through multiple computer nodes to get to the final destination, hence, without requesting safe receipt there is little guarantee ,; andeMail is not a legal document unless sent using a digital signature but by 2002,EU member states and the USA (some states) have begun accepting documents withDigital Signatures as legally binding documents",0
353,"Encryption is the process of conversion of an easily understood format (text)into another format apparently lacking sense because it is encoded.  Theencrypted text is called ciphertext ,. In the Encryption context, Algorithms use a key to control encryption anddecryption (See Algorithm), two categories are used:Symmetric (or Secret-key): Uses the same key for encryption & decryption, orthe latter key is easily derived from the encryption key ,. Asymmetric (or Public key): A different key is used for encrypting and decrypting a message; accordingly, the decrypting key cannot be derived from theencrypting key.  (See also Algorithms, Tables 11A and 11B). A majority of cryptographic products are software rather than hardwarebased.  Moreover, many are communications-oriented rather than data storageoriented; they heavily tend towards secure electronic mail, IP security (IPsec),and Virtual Private Network applications. In 1999 a report identified 805 hardware and / or software products incorporating cryptography manufactured in 35 countries outside the UnitedStates.  Most were manufactured in these countries in that order:United Kingdom, followed by Germany, Canada, Australia, Switzerland,Sweden, the Netherlands, and Israel. Other countries accounted for slightly more than a quarter of the world’stotal of encryption products.  Table 11A outlines some of the algorithms thathave been used based on the DES standard. Table 11B outlines the various standards that were submitted for testingand examination.  These finalists were part of the tests being undertaken toselect the Advanced Encryption Standard (AES) Rijndael",0
354,"nd User License Agreement (EULA) is a legally binding contract betweenthe developer or publisher of a software program (or application) and thepurchaser of that software.  The purchaser does not own the software but has merely a right to use it in accordance with the license agreement. During software installation, the EULA is usually shown and one is required to Accept or Refuse the terms ,. In some cases, the EULA is written on the outside of the packaging withthe breaking of the seal to the CD, indicating acceptance of the EULA. In all cases, the EULA is the contract which users ignore at their peril;and whilst most EULAs contain broadly similar clauses and restrictions, itis important to confirm these before committing your organization. ",0
355,"Extensive Mark-up Language (XML) is the first building bloc for a Semantic Web.  Invisible to the human viewer, XML tags can be used to describehow information on a page is structured, allowing visiting computers to readand act on it without human invention.  XML describes data in terms of its content.  In that respect XML is a markuplanguage that has significant potential for the capture and onward processingof data directly from web pages.  The real significance of this is that Businessto Business data transfer is greatly facilitated by XML as neither party needs.  to write interfaces to each other’s systems; they merely need to be able toaccept and process XML.  Unlike Hyper Text Mark-up Language (HTML) which is a single predefined language‚ XML is a metalanguage.  Hence‚ it is a language for describing other languages.  Therefore‚ visiting computers need to be familiar with thespecific XML language before then can interpret the web page or document. Hence‚ a computer can refer to an XML “schema” located elsewhere on theweb. ",0
356,Extranet helps the organization to link the outside world such as suppliersand customers with a private intranet.  While this is similar to the internet‚access is controlled and restricted to particular groups‚ similar to the Intranet. Accordingly‚ an Extranet web server can be accessed by all the participantsinvolved in a project (e. g. ‚ various engineers and the firm developing a newproduct)‚ but not by anyone else.  In this example‚ the Extranet providesproject management functions for the work in progress and the work teamsinvolved.  The security is increased; however‚ breaches can still occur (e. g. ‚cyberpunk getting hold of a password or access to a server who‚ with apassword‚ is given access to the Extranet) ,0
357,Burden of Proof is the necessity or duty of affirmatively proving a factor facts in a dispute.  Hence‚ it is important to provide Reliable and Validevidence‚ especially if the victimized party (e. g. ‚ home user) wants to proofthat he or she has suffered damages through a computer virus.  In turn‚ anorganization requires a systematic approach to develop the costs (e. g. ‚ Tables2A–2D) to arrive at the quantified losses it suffered from,0
358,False Negative occurs when an actual intrusive action happened but thesystem fails to detect it or simply allows it to pass as a non-intrusive behavior.  Similarly if a virus was activated by a user action on the PC butthe software failed to detect it‚ a False Negative occurred ,0
359,"Firewall consists of a set of related programs‚ located at a network gateway server.  A firewall is a combination of hardware and software used toimplement a security policy governing the network traffic between two ormore networks.  Some of that traffic may be under the administrator’s control(e. g. ‚ organization’s networks) and some of which may be out of the systemadministrator’s control (e. g. ‚ the Internet). Firewalls determine whether to block or allow network traffic by lookingat TCP7IP packet headers to determine if these are in accordance with predetermined security policy.  However‚ a firewall does neither have the capabilityto recognize malicious code (Malware‚ Virus)‚ nor any means for preventingits transfer to a target system. A Firewall usually protects the resources of a private network from usersfrom other networks.  A network firewall commonly serves as a primary lineof defense against external threats to an organization’s computer systems‚networks‚ and critical information.  Firewalls can also be used to partition anorganization’s internal networks‚ reducing the risk from insider attacks. There are three types of firewalls:Pack Filter—filters the contents of the IP packet header‚ therefore‚ limited tothe source and destination address as well as the TCP/UDP port number.  Filterdoes not check content of message/data;Circuit Filter or Circuit Level Gateway—applies packet filtering but verifiesinformation based on TCP or UDP packet header information as well.  Hence‚it can make a better decision if the individual packets form part of a valid TCPsequence.  Creates a handshake‚ and once that takes place passes everythingthrough until the session is ended.  Still it has no knowledge of which user isrequesting access to services; andApplication Filter—uses proxies to apply filter rules based on the data contentand sometimes the user.  A dedicated program called a ‘proxy’ or ‘proxy server’is used to effect the application filter policy rules.  Commonly used applicationfilter is the web proxy.  It can be used to restrict the internal (or Intranet) webpages that are published out to the internet ,. ",0
360,Firmware often takes the form of a device which is attached to‚ or built into‚a computer‚ such as a ROM chip.  It performs some software function but isnot a program in the sense of being installed and run from the computer’sstorage media.  Hence‚ it is located in the middle of the conceptual continuumbetween hardware and software. ,0
361,Challenge-Handshake Authentication Protocol (CHAP) is used to periodically verify the identity of the peer using a 3-way handshake.  This isdone upon initial link establishment‚ and MAY be repeated anytime after thelink has been established. After the Link Establishment phase is complete‚ the authenticator sendsa “challenge” message to the peer which is a random byte-sequence as wellas the identifier‚ a randomly generated number.  The peer responds with avalue calculated using a “one-way hash” function.  The authenticator checksthe response against its own calculation of the expected hash value.  If thevalues match‚ the authentication is acknowledged; otherwise the connectionSHOULD be terminated. CHAP provides protection against playback attack through the use of anincrementally changing identifier and a variable challenge value.  The use ofrepeated challenges is intended to limit the time of exposure to any singleattack.  The authentication can occur between client and server‚ in largerinfrastructures‚ however‚ the administration of keys or passwords can bequite a big effort. CHAP also supports the use of central server that acts as a central certification unit according to such as‚ the Remote Access Dial-in User Serviceprinciple or is in control of the frequency and timing of the challenges. ,0
362,Hexadecimal is a numerical system using base 16 (as opposed to the usualbase 10).  Hexadecimal is a useful way to express binary computer numbersin which a byte is normally expressed as having 8 bits; with 2 hexadecimalcharacters representing eight binary digits—aka a byte. ,0
363,honeynet is a network of hosts that is specifically designed to be compromised.  It is further refinement of a Honeypot.  Highest security precautionsare taken and such networks are heavily monitored.  The purpose of suchnetworks is to study Hackers’ behaviors‚ tools‚ motives and possible contrameasures. ,0
364,Honeypot is a system or host specifically designed to be compromised. Such a system seems to be attractive to Hackers‚ but at the same momentthese are heavily monitored.  The purpose of such a host is to study attacker’sbehavior‚ security tools and possible contra measures. ,0
365,"Host means a computer that communicates across the network (e. g. ‚ information system‚ PC or other IT hardware).  But a host can also be a PC‚information system or other IT software or hardware that affords subsistenceor lodgment to an infectious agent under normal conditions ,.  Some malicious code or viruses pass successive stages in alternatehosts of different systems. ",0
366,Hyper Text Mark-up Language (HTML) is a descriptive language used forthe transmission of information‚ graphics‚ sounds and animation between aclient web browser and the web server using HTTP protocol. HTML tags‚ or mark-up‚ used in web pages dictate how the informationwill be displayed‚ such as a headline here or italics there.  However‚ they giveno clue as to the text’s meaning. ,0
367,Hyper Text Transfer Protocol (HTTP) is used for the transmission of information‚ graphics‚ sounds and animation between a client web browser andthe web server.  It is defined in RFC2616 and updated with RFC2817. ,0
368,"Secure Hyper Text Transfer Protocol (HTTPS) uses HTTP but additionally activates web server security‚ in the form of Secure Socket Layer (SSL). This means that the communications between the client and the (host) webserver are encrypted and‚ additionally‚ that the host web server may be validated by the client using a Digital Certificate on the server ,. The URL for such web sites indicates that they are secure by the use of‘https://address’ (rather than http://address). Some may list this term as shttp instead of https. ",0
369,"Integrated Services Digital Network (ISDN) allows voice, video and datato be transmitted over a single telephone line at speeds depending upon thenetwork provider.  In the mid 1990s ISDN offered speed of up to four timesas fast as conventional 28. 8 Kbps modems (up to 128 kilobits per second orKbps. There are two basic types of ISDN service: Basic Rate Interface (BRI)and Primary Rate Interface (PRI).  BRI consists of two 64 kb/s B channelsand one 16 kb/s D channel for a total of 144 kb/s.  This basic service isintended to meet the needs of most individual users. PRI is intended for users with greater capacity requirements.  Typically thechannel structure is 23 B channels plus one 64 kb/s D channel for a total of1536 kb/s. In Europe, PRI consists of 30 B channels plus one 64 kb/s D channel fora total of 1984 kb/s. ",0
370,"Internet Protocol Version 6 (IPv6) is the “next generation” protocol designed by the Internet Engineering Task Force (IETF) to replace the currentversion Internet Protocol, IP Version 4 (“IPv4”). Most of today’s internet uses IPv4, which is now twenty years old.  IPv4has been remarkably resilient in spite of its age, but it is beginning to haveproblems.  Most importantly, there is a growing shortage of IPv4 addresses,which are needed by all new machines added to the Internet. IPv6 fixes a number of problems in IPv4, such as the limited number ofavailable IPv4 addresses.  It also adds many improvements to IPv4 in areassuch as routing and network autoconfiguration.  IPv6 is expected to graduallyreplace IPv4, with the two coexisting for a number of years during a transitionperiod ,. ",0
371,"Internet Service Provider (ISP) describes a firm that provides access to theInternet, plus a range of standard services such as email and the hosting(running) of personal and corporate web sites. ISPs usually charge a tariff for their services although income can bederived from various sources of advertising and portal activities. ",0
372,Java Applets are Java applications with some security-related limitationsimposed on them.  These applets are downloaded to the local computer orpc.  They run within a Java ‘Sandbox’ created by a web browser.  This limitsthe applet’s ability to perform some functions that could be considered risky. Java applets cannot:execute arbitrary system commands;write to the local file system outside of strictly designated areas (but this is browserdependent); andopen network connections to sites other than the one from which the appletoriginated,0
373,"Java Sandbox contains specific provisions to deal with potentially dangerousapplets, and tries to execute them in a protected mode that severely limits theeffects they can have on other applications and the host computer.  The Javadesign team spent time worrying about malicious executables and how theycan be prevented from running amok, hence the Sandbox was invented. ",0
374,"JavaScript is an interpreted scripting language, similar in capability toMicrosoft’s Visual Basic, or SUN Microsystems’ Perl scripting language. Java script is interpreted, not compiled, and therefore slower to execute thancompiled code.  It is easier to maintain and fast enough for small applications.  Security risks associated with JavaScript are generally limited toDenial-of-Service (DoS) , attacks, such as excessive loadon the processor, or annoyance attacks ,. ",0
375,"Keystroke Monitoring is audit trail software or a specially designed device,recording every key struck by a user and every character of the response thatthe information system returns to the user. ",0
376,"Logic Bombs (also called Fork Bombs) is a program or portion thereoffthat triggers or causes an application or operating system to perform whena certain logical event occurs.  The code may be used to recursively spawncopies of itself, thereby eventually eating all the process table entries andeffectively locking up the system ,. A specific date, key combination, or internal counter are some of themost commonly used triggers that produce effects ranging from on-screendisplays to the blocking of the system or the deletion of files and programs. For instance, every day at 17:00 hours it mails out a message to 10 addressesout of a person’s email address database ,. A Virus may also no longer distribute itself after a certain date.  For instance,the Sobig virus family expired September 10, 2003 indicating that the writerwas primarily interested in studying the virus’ effects on networks.  However,this might change in the near future. ",0
377,"Master Boot Record (MBR) is the program stored in the boot sector of thebootable media.  MBR viruses normally enter the system through a floppydisk.  All floppy disks have a boot sector, regardless of whether they arebootable or not.  If the computer is started with a floppy disk in drive “A:”,the operating system reads the information contained in its boot sector andchecks to see if it is bootable.  If there is a virus in this sector, the computerreads this virus code first, and therefore, the virus is able to infect the computer",0
378,"Moore’s Law is a theorem that states that the amount of information storableon a given amount of silicon has roughly doubled every year since the technology was invented.  First mentioned in 1964 by semiconductor engineerGordon Moore, co-founder of Intel in 1968, this held until the late 1970s,at which point the doubling period slowed to 18 months.  Some are nowsuggesting that it is taking again 24–30 months. ",0
379,"Open Source is code that is put into the public domain.  Hence, any usercan take advantage of it usually for free.  Most importantly, if the code ispopular many people may examine it and, therefore, assuring that securitybugs are published quickly.  Very nicely for users, fixes are usually availablein relatively short time thereafter",0
380,"Operating System are computer programs that that are primarily or entirelyconcerned with controlling the computer and its associated hardware, ratherthan with processing work for users.  Computers can operate without application software, but they cannot run without an operating system.  Examplesare DOS, MaxOS, Linux and Unix. ",0
381,Parallel Processing occurs when a computer uses more than one processor‚either to be able to perform more than one task at the same time or to improveprocessing speed by breaking down one larger task between different processors.  Parallel processing is not quite the same as ‘Multi-Tasking’ since‚by definition‚ a single processor cannot do two things at once.  It just seemsthat way to the user because the two things are handled one after the otherso very quickly. ,0
382,Password is usually a string of alphanumeric characters a user will type onhis or her keyboard with lower and/or uppercase letters as well and hopefullywithout vocals such as (19mlnMY82].  Ideally‚ a computer or a network hasits own dictionary of common passwords‚ whereby a password previouslyused or included in the dictionary is refused by the system. Passwords must never (ever) be written down.  Neither should one useone’s first‚ last or a family member’s name nor one containing all digits or aword contained in a dictionary.  Below are some additional issues that needto be remembered. ,0
383,Ubiquitous Computing is a term put forward by Mark Weiser in Ubiquitous Computing is a term put forward by Mark Weiser in 1988‚ describing it as information technology’s next wave after the mainframe and PC.  In this new world‚ what Weiser called “calm technology” will reside around us‚ interacting with users in natural ways to anticipate their needs and supply the information they want wherever they are.  Pervasive Computing requires that computing devices are ubiquitious and if not invisible‚ at least handy for users and easy to take wherever one wishes to go (e. g. ‚ small cellular used for tracking location of person).  PervasiveComputing envisions environments richly lathered with computation‚ communication and networked devices‚ mobile users interacting with their environment using speech and vision‚ with secure access to personal or publicdata.  Pervasive Computing environments will not simply be standalone vehicles for number crunching‚ rather they will immerse their users in a triad of nvisible computation‚ communication and devices‚ working in concert to satisfy userrequirements according to the facilities available in the environment. As such Pervasive Computing is a world saturated with computing andcommunication on the road to Ubiquitous Computing.  The latter also gracefully integrates social and end-user issues including but not limited to SocialInformatics and Social Securematics. ,0
384,"PHP is an HTML-embedded Web scripting language.  Accordingly‚ PHPcode can be inserted into the HTML of a Web page.  When a PHP page isaccessed‚ the PHP code is read or “parsed” by the server the page resides on. The output from the PHP functions on the page are typically returned asHTML code‚ which can be read by the browser.  Because the PHP code istransformed into HTML before the page is loaded‚ users cannot view the PHPcode on a page.  This make PHP pages secure enough to access databases andother secure information. A lot of the syntax of PHP is borrowed from other languages such as C‚Java and Perl ,.  However‚ PHP has a number of uniquefeatures and specific functions as well.  The goal of the language is to allowWeb developers to write dynamically generated pages quickly and easily. PHP is also helpful for creating database-driven Web sites. ",0
385,Ping stands for Packet Internet (or Inter-Network) Groper and is a packet(small message) sent to test the validity/availability of an IP address ona network.  The technical term for ‘ping’ is the Internet Control MessageProtocol.  Maliciously sending large volumes of ‘Pings’ to cause difficultiesfor anyone else attempting to access that address is known as Smurfing. ,0
386,Port Address Translation (PAT) is communication technology used byrouters to allow multiple users in a local network to access—with their ownIP address—the Internet or corporate networks via a single public address. PAT actually translates multiple private IP addresses to a single public address‚ or to a public sub network‚ recognized by the IP service provider.  Thisfunction can reduce operating costs‚ increase security and simplify Internetaccess. If an attacker wants to access ports connected to the 3Com OfficeConnectRemote 812 ADSL router the router will not allow this connection.  However‚firmware versions V1. 1. 9 and V1. 1. 7 had a confirmed vulnerability that ifa connection is made to a redirected port using PAT and then to any portnot redirected using PAT‚ the router allows the successive connections to anyport.  The problem exists with TCP and with UDP. ,0
387,"Port Scan gives a list of all ports that are actually listening.  The netstat command can be run locally to determine the open ports but an external port scanagainst the system is usually also needed.  If the results of netstat differ fromthe port scanning results‚ validation of why each port is open‚ and what isrunning on each port is needed.  Ports that cannot be validated or justifiedshould be closed.  The final list should be recorded and used to audit the portson a regular basis‚ thereby making sure no extraneous ports appear ,. Blocking ports is not a substitute for a comprehensive security solution.  Anattacker may have gained access via other means such as a dial-up modem‚ atrojan e-mail attachment‚ or a person who is an organization insider.  Hence‚the attacker can exploit these ports if not properly secured on every hostsystem in the firm",0
388,Pretty Good Privacy (PGP) is a software package permitting users to useencryption when exchanging messages‚ widely available.  Export versionsof PGP are different than versions used in the USA and Canada ,0
389,"Protocol is a set of formal rules describing how to transmit data, especiallyacross a network.  Low level protocols define the electrical and physicalstandards to be observed, bit- and byte-ordering and the transmission anderror detection and correction of the bit stream.  High level protocols dealwith the data formatting, including the syntax of messages, the terminal tocomputer dialogue, character sets, sequencing of messages etc.  Protocols are,therefore, a set of rules that define how communications should take place,as they are common formulas that enable two separate computers to ‘speak’with and ‘understand’ each other. The first group of protocols includes the Transmission Control Protocol/Internet Protocol (TCP/IP) family, used for communication via Internet. Other examples are such as HTTP that is used for Web page communications,being a subset of TCP/IP",0
390,"Radio Frequency Identification (RFID) technology was used by Gillette infield tests for tracking packets of razors through its supply chain.  Michelinplans to use vulcanizing to attach an RFID to a tire. Benetton is another clothing manufacturer that tested RFID tags in someproducts, such as, by weaving the technology into the collar tags of clothesthat cost at least $15 to keep track of them as they ship.  Benetton pulled backfrom this RFID trial after a consumer group announced a global boycott ofthe clothing manufacturer. The advantage of RFIDs over barcodes is that information can be collectedwithout a line of sight to the tag, hence, a pallet of goods or a razor can justbe scanned by passing through a radio field. With location tracking chips in mobile phones and toll payment cards thetechnology can always provide information about where the product and itsuser are currently located. This does not look good for data protection and Privacy and consumeradvocacy groups have complained loudly and continue fighting for citizen’srights to privacy. ",0
391,"Remote Access Tools (RATS) enables a person to access a server remotelyby using, for instance, a telephone modem or wireless access.  RAT candrift in and out of legitimacy according to the prevailing legal climate andthe ensuing degree of nervousness displayed by the legal department ,. Nonetheless, generally RATS are defined as being malicious programsthat run invisibly on host PCs.  They can permit an intruder remote accessand if successful give him or her also control of the machine or PC.  Theseprograms are usually installed for stealth installation and the programs mayby hidden via a Trojan Horse that is embedded in a game or another program. These are usually small files with a size of between 10 KB–30 KB. An attacker may try to hide this program using a so-called Binder tocombine a RAT with legitimate executables, thereby enabling the RAT toexecute in the background while the legitimate application runs as well,thereby keeping the victim unaware of these undesirable activities. Best known RATs are Back Orifice and SubSeven that can capture screen,sound and video content.  These Trojans are key loggers, remote controllers,FTP servers, and HTP servers.  Telnet servers and password finders are alsopart of their capabilities and RATS may also rogue mechanisms that hidethe Trojans by using encrypted communication.  Worse is if they also containprofessional-looking APIs, thereby making it feasible for other attackers toinsert additional features. ",0
392,"Rijndael Algorithm was finally chosen as the winner because it was thefastest of them all.  The algorithm was broken shortly after its adoption byMike Boyle and Chris Salter as well as by Phillip Rogaway within daysof its publication as the accepted AES.  However, academic breaks are theones that force you to change things in the design process; practical breaksforce you to change things in fielded equipment.  This work is clearly of theacademic-break variety ",0
393,"Role-Based Access Control (RBAC) means that access decisions are basedon the roles the user has as part of tasks to be performed ,. The central notion of RBAC is that users do not have discretionary access to enterprise objects.  Instead, access permissions are administrativelyassociated with roles, and users are administratively made members of appropriate roles.  This idea greatly simplifies management of authorizationwhile providing an opportunity for great flexibility in specifying and enforcing enterprise- specific protection policies.  Users can be made members ofroles as determined by their responsibilities and qualifications and can beeasily reassigned from one role to another without modifying the underlyingaccess structure.  Roles can be granted new permissions as new applicationsand actions are incorporated, and permissions can be revoked from roles asneeded. ",0
394,"Router is an interconnection device similar to a bridge but serves packets orframes containing certain protocols.  Hence, Routers link Local Area Networks (LANs) at the network layer. But with Home Users installing their own routers, Denial of Serive Attacks can become a nightmare.  For instance, in May 2003, the Universityof Wisconsin–Madison found that it was the recipient of a continuous largescale flood of inbound Internet traffic destined for one of the campus’ publicNetwork Time Protocol (NTP) servers.  The flood traffic rate was hundredsof-thousands of packets-per-second, and hundreds of megabits-per-second. The only recourse available to the University was to go to its ISP andasking it to null-route all this traffic to the university servers, meanwhilepaying a huge increase in bandwidth costs—an increase that seriously hurtthe university’s IT budget, and would have quickly depleted it they ISP didnot respond by null-routing the traffic. ",0
395,"Script Kiddie is a derogatory term used by “real” Hackers and security professionals for less skilled hackers.  These Hackers usually use scripts and toolsfrom internet without appropriate knowledge.  They often target Unix hostswith tools that are for targeting of MS Windows hosts and vice versa.  It hasbeen observed that these hackers are not having much knowledge and evensimple system commands may pose a significant challenge to them.  Unfortunately, these types of Hackers can cause as much harm as “real” Hackerscan. ",0
396,"Sector means pie-shaped slices and track that are concentric rings on a disk. A combination of two or more sections on a single track makes a clusteror block, the minimum unit used to store information.  The number of sectorsper track determines the size of each cluster.  In turn, the number of clusterson a disk’s surface decides the storage capacity of the disk. The sector is the smallest unit that can be accessed on a disk and it has asize of 512 Bytes. ",0
397,"Secure Hash A process which reduces a message of arbitrary length to afixed length fingerprint which is very unlikely to be the same for any othermessage.  The word “secure” indicates that the algorithm has been chosen sothat it is not possible to forge a message which to have given hash value, norto create two similar messages with the same hash value. ",0
398,"Secure Socket Layer (SSL) means that the communications between theclient and the (host) web server are encrypted and, additionally, that the hostweb server may be validated by the client using a Digital Certificate on theserver ,.  SSL is a protocol developedby Netscape.  The most common application of SSL is https for ssl-encryptedhttp.  Nowadays, many other protocols use advantages of SSL, such as POP,SMTP, and NNTP. ",0
399,Signature Based Intrusion Detection Systems are matching collected dataagainst known set of signatures of known attacks.  This means that thesesystems are only as good as their patter/finger print/signature database. These databases have to be continuously updated and maintained to assure their currency.  This is a tedious task and there are recently attemptsto create central repository and unified descriptive language for thesesystems. ,0
400,Smurfing is an attack that exploits features of the IP protocol within theTCP/IP protocol used for internet communications.  A smurf attack causes a victim’s computer to become completely ‘waylaid’ with answering fictitious network requests (‘Pings’) that it grinds to ahalt and prevents anyone else from logging on.  ,0
401,Sniffer is software designed to look at and/or to collect traffic on the wire. Sniffing tools are built into majority of UNIX type systems and there is wealthof free tools available on the internet for other platforms.  Majority of themare designed around the libpcap library and tcpdump tool.  Some IntrusionDetection Systems have option of being run as a sniffer too. ,0
402,"Social Engineering is a technique where persuasion and/or deception are usedto gain access to the systems.  This is typically implemented through humanconversation or other interaction.  Typical example of this is an attack wherea hacker pretends to be a high positioned IT executive traveling on companybusiness and having problems to connect to the organization’s informationsystem through its remote access point. The person may gradually succeed in persuading the Help Desk operatorto tell him or her all the necessary details for the connection set-up.  Later onattacker calls again, complaining that his password for some reason does notwork and persuades the Help Desk to change it to a password of her or hisliking.  Hence, the person may gain unauthorized access ,. The term can also be applied to exploiting the victim’s good intentions andlack of in-depth technical knowledge in order to inspire fear and confusion. This process is often considered in the context of ‘memetics’, which dealswith the transfer of memes (the ‘unit of cultural inheritance’) from brain tobrain. Where social engineering is linked to an IT security issue or hoax, it nearlyalways trades on technophobia ",0
403,"P2P Spoofing means that an attacker could have anonymously tricked aninnocent P2P user into downloading a contraband file from another user onthe P2P network.  This can occur by having an attacker modify search requestsand search results in transit by, for instance, placing the innocent user’s IPaddress in the search result packet that lists a contraband file.  Another waymight be whereby a search request for music has a hop count set to 255.  Theflawed P2P application permits the attacker to set the hop count value backto zero, thereby making the innocent user be seen as the originator.  Changingthe search string church hymns to Rolling Stones makes an authority assumethat the innocent user went out to search for illegal content. The difficulties for authorities is to prove that the apparent offender wasthe actual offender, in turn this would require that the agency proofs that therewere no malicious users on the P2P network at the time while the apparentoffender’s P2P application had no implementation flaws that are possible. ",0
404,"Steganography (“hidden writing”) is a form of data hiding.  Unlike cryptography, which creates an unreadable version of a message for anyone withoutthe key, Steganography conceals even the existence of secret messages.  Oneof the earliest forms of Steganography is spread-spectrum radio transmissions, in which parts of a message (even parts of individual bits) are senton pseudo-randomly varying radio frequencies; without the right equipment,the signal is merely electronic white noise. Steganography circumvents restrictions on encryption and preventsSIGINT (signals intelligence) personnel from detecting encrypted traffic. Digital watermarks and copy-protection schemes are forms of Steganography used to reduce the ease of illegal copying of intellectual property and totrace the origin of files ,. There are many tools available for embedding secret messages in otherfiles; for instance, MP3STEGO modifies MP3 audio files to hold messages. Steganalysis tools look for tell-tale patterns in changed data when lookingfor concealed messages. ",0
405,"Stream Cipher encrypts in small units, often a bit or a byte at a time.  Unlike abasic block cipher, a stream cipher will have output corresponding to a giveninput will depend on where in the message it occurs.  The simplest type ofstream cipher uses a complicated function, which retains state, to generatea pseudo-random sequence which is then combined with the input using asimple operation such as bytewise addition. ",0
406,"Just as the way business is organized and conducted has been profoundly changed by information and communications technology, the operation of government at all levels has been similarly affected.  The term e-government (or electronic government) is a way of looking at these changes as a whole and of considering how government uses (or might use) various computer applications. The use of information technology in government can involve changes in the organization and internal communications of government departments, changes in how services are delivered to the public, and providing new ways for the public to interact with the agency. Internally, government agencies have many of the same information management and sharing needs as private enterprises ,.  However, government agencies are likely to have to adapt their information systems to account for complex, specialized regulations (both those the agency administers and others it is subject to).  The standards of openness and accountability are generally different from and stricter than those that apply to private organizations. ",0
407,"Eiffel is an interesting programming language developed by Bertrand Meyer and his company Eiffel Software in the 1980s.  The language was named for Gustav Eiffel, the architect who designed the famous tower in Paris.  The language and accompanying methodology attracted considerable interest at software engineering conferences. Eiffel fully supports (and in some ways pioneered) programming concepts found in more widely used languages today ,.  Syntactically, Eiffel emphasizes simple, reusable declarations that make the program easier to understand, and tries to avoid obscure or lower-level code such as compiler optimizations. ",0
408,"Eiffel’s proponents note that it is more than a language: It is designed to provide consistent ways to revise and reuse program components throughout the software development cycle.  The current implementation of Eiffel is available for virtually all platforms and has interfaces to C, C++, and other languages.  This allows Eiffel to be used to create a design framework for reusing existing software components in other languages.  Eiffel’s consistent object-oriented design also makes it useful for documenting or modeling software projects",0
409,"There are a variety of ways to electronically register, store, and process votes.  In recent years older manual systems (paper ballots or mechanical voting machines) have been replaced in many areas with systems ranging from purely digital (touch screens) to hybrid systems where marked paper ballots are scanned and tabulated by machine.  However, voting systems have been subject to considerable controversy, particularly following the Florida debacle in the 2000 U. S.  presidential election. The criteria by which voting systems are evaluated include:, how easy it is for the voter to understand and use the system, accessibility for disabled persons, whether the voter’s intentions are accurately recorded , the ability to make a permanent record of the vote, prevention of tampering (physical or electronic), provisions for independent auditing of the votes in case of disputeThe degree to which a given system meets these criteria can vary considerably because of both design and implementation issues. ",0
410,"The earliest form of voting system consisted of paper ballots marked and tabulated entirely by hand.  The first generation of “automatic” voting systems involved mechanical voting machines (where votes were registered by pulling levers).  Next came two types of hybrid systems where votes were cast mechanically but tabulated automatically.  These systems used punch cards , or “marksense” or similar systems where the voter filled in little squares and the ballots were then scanned and tabulated automatically. The ultraclose and highly disputed 2000 U. S.  presidential election “stress-tested” voting systems that most people had previously believed were reasonably accurate.  The principal problems were the interpretation of punch cards that were not properly punched through (so-called dimpled or hanging chads) and the fact that some ballot layouts proved to be confusing or ambiguous.  Two types of voting systems have been proposed as replacements for the problematic earlier technology",0
411,"This type of system uses a screen display that can be directly manipulated by the voter ,.  In the most common type, called DRE (direct-recording electronic), a computer program interprets and tabulates the vote as it is cast, storing an image in a removable memory unit and (usually) printing out a copy for backup.  After voting is complete, the memory module can be sent to the central counting office.  (Alternatively, votes can be transmitted over a computer network in batches throughout the day. ) In a few cases, voting has also been implemented through secure Internet sites. ",0
412,"Concern about potential tampering with computers has led many jurisdictions to begin to replace touchscreen systems with optical-scan systems, where the voter marks a sturdy paper ballot.  (About half of U. S.  counties now use optical-scan systems. ) The advantage of optical systems is that the voter physically marks the ballot and can see how he or she has voted, and after tabulation the physical ballots are available for review in case of problems.  However, optical-scan ballots must be properly marked using the correct type of pencil, or they may not be read correctly.  Unlike the touchscreen, it is not possible to give the voter immediate feedback so that any errors can be corrected.  Optical-ballot systems may cost more because of paper and printing costs for the ballots, which may have to be prepared in several languages.  However this cost may be offset by not having to develop or validate the more complicated software needed for all-electronic systems. ",0
413,"Electronic mail is perhaps the most ubiquitous computer application in use today.  E-mail can be defined as the sending of a message to one or more individuals via a computer system, open the file, and look for messages.  In 1971, however, the ARPANET (ancestor of the Internet—see internet) was used by researchers at Bolt Beranek and Newman (BBN) to send messages from a user at one computer to a user at another.  The availability of e-mail helped fuel the growth of the ARPANET through the 1970s and beyond. connection. Development and ArchitectureThe simplest form of e-mail began in the 1960s as a way that users on a time-sharing computer system could post and read messages.  The messages consisted of text in a file that was accessible to all users.  A user could simply log into the",0
414,"When people think of a computer, they generally think of a general-purpose computing system housed in a separate box, for use on the desk or as a laptop or hand-held device.  However, the personal computer and its cousins are only the surface of a hidden web of computing capability that reaches deep into numerous devices used in our daily lives.  Modern cars, for example, often contain several specialized computer systems that monitor fuel injection or enhance the car’s grip on the road under changing conditions.  Many kitchen appliances such as microwaves, dishwashers, and even toasters contain their own computer chips.  Communications systems ranging from cell phones to TV satellite dishes include embedded computers.  Most important, embedded systems are now essential to the operation of critical infrastructure such as medical monitoring systems and power transmission networks.  (The potential vulnerability of embedded systems to the Y2K date-related problems was a major concern in the months leading up to 2000, especially because many embedded systems might have to be replaced rather than just reprogrammed.  In the event, it turned out that there were relatively few date-dependent systems and only minor disruptions were experienced",0
415,"One consequence of the universal computer concept , is that in principle any computer can be programmed to imitate the operation of any other.  An emulator is a program that runs on one computer but accurately processes instructions written for another ,.  For example, fans of older computer games can now download emulation programs that allow modern PCs to run games originally intended for an Apple II microcomputer or an Atari game machine.  Emulators allowing Macintosh and Linux users to run Windows programs have also achieved some success. In order to work properly, the emulator must set up a sort of virtual model of the target microprocessor, including appropriate registers to hold data and instructions and a suitably organized segment of memory.  While carrying out instructions in software rather than in hardware imposes a considerable speed penalty, if the processor of the emulating PC is much faster than the one being emulated, the emulator can actually run faster than the original machine. ",0
416,"In the earliest programming languages, any part of a program could access any other part simply by executing an instruction such as “jump” or “goto. ” Later, the concept of the subroutine helped impose some order by creating relatively self-contained routines that could be “called” from the main program.  At the time the subroutine is called, it is provided with necessary data in the form of global variables or (preferably) parameters, which are variable references or values passed explicitly when the subroutine is called.  When the subroutine finishes processing, it may return values by changing global variables or changing the values of variables that were passed as parameters",0
417,"The use of encryption to disguise the meanings of messages goes back thousands of years (the Romans, for example, used substitution ciphers, where each letter in a message was replaced with a different letter).  Mechanical cipher machines first came into general use in the 1930s.  During World War II the German Enigma cipher machine used multiple rotors and a configurable plugboard to create a continuously varying cipher that was thought to be unbreakable.  However, Allied codebreakers built electromechanical and electronic devices that succeeded in exploiting flaws in the German machine (while incidentally advancing computing technology).  During the cold war Western and Soviet cryptographers vied to create increasingly complex cryptosystems while deploying more powerful computers to decrypt their opponent’s messages. ",0
418,"This concept refers to the organization of data processing and communications across an entire corporation or other organization.  Historically, computing technology and infrastructure often developed at different rates in the various departments of a corporation.  For example, by the 1970s, departments such as payroll and accounting were making heavy use of electronic data processing (EDP) using mainframe computers.  The introduction of the desktop computer in the 1980s often resulted in operations such as marketing, corporate communications, and planning being conducted using a disparate assortment of software, databases, and document repositories.  Even the growing use of networking often meant that an enterprise had several different networks with at best rudimentary intercommunication",0
419,"Much publicity has been given to figures such as Microsoft founder and multibillionaire Bill Gates, who turned a vest-pocket company selling BASIC language tapes into the dominant seller of operating systems and office software for PCs.  Historically, however, the role of key entrepreneurs in the establishment of information technology sectors repeats the achievements of such 19th- and early 20th-century technology pioneers as Thomas Edison and Henry Ford.  There appear to be certain times when scientific insight and technological capability can be translated into businesses that have the potential to transform society while making the pioneers wealthy",0
420,"Ergonomics is the study of the “fit” between people and their working environment.  Because computers are such a significant part of the working life of so many people, finding ways for people to maximize efficiency and reduce health risks associated with computer use is increasingly important. Since the user will be looking at the computer monitor for hours on end, it is important that the display be large enough to be comfortably readable and that there be enough contrast.  Glare on the monitor surface should be avoided.  It is recommended that the monitor be placed so that the top line of text is slightly below eye level.  A distance of about 18 inches to two feet (roughly arm’s length) is recommended.  There has been concern about the health effects of electromagnetic radiation generated by monitors.  Most new monitors are designed to have lower emissions",0
421,"Transmitting data involves the sending of bits (ones and zeros) as signaled by some alternation in physical characteristics (such as voltage or frequency).  There are a number of ways in which errors can be introduced into the data stream.  For example, electrical “noise” in the line might be interpreted as spurious bits, or a bit might be “flipped” from one to zero or vice versa.  Generally speaking, the faster the rates at which bits are being sent, the more sensitive the transmission is to effects that can cause errors. While a few wrong characters might be tolerated in some text messages or graphics files, binary files representing executable programs must generally be received perfectly, since random changes can make programs fail or produce incorrect results.  Data communications engineers have devised a number of methods for checking the accuracy of data transmissions",0
422,"An important characteristic of quality software is its ability to handle errors that arise in processing (also called run-time errors or “exceptions”).  Before it is released for general use, a program should be thoroughly tested with a variety of input ,.  When errors are found, the soundness of the algorithm and its implementation must be checked, as well as the program logic ,.  Interaction between the program and other programs (including the operating system) as well as with hardware must also be considered.  (See bugsand debugging. )However, even well-tested software is likely to encounter errors.  Therefore a program intended for widespread use must include instructions for dealing with errors, anticipated or otherwise.  The process of error handling can be divided into four stages: validation, detection, communication, and amelioration. ",0
423,"An expert system is a computer program that uses encoded knowledge and rules of reasoning to draw conclusions or solve problems.  Since reasoning (as opposed to mechanical calculation) is a form of intelligent behavior, the field of expert systems (also called knowledge representation or knowledge engineering) is part of the broader field of AI",0
424,"An expert system has two main components, a knowledge baseand an inference engine.  The knowledge base consists of a set of assertions (facts) or of rules expressed as if .  .  .  then statements that specify conditions that, if true, allow a particular inference to be drawn ,.  The inference engine accepts new assertions or queries and tests them against the stored rules.  Because satisfying one rule can create a condition that is to be tested by a subsequent rule, chains of reasoning can be built up.  If the reasoning is from initial facts to an ultimate conclusion, it is called forward chaining.  If a conclusion is given and the goal is to prove that conclusion, there can be backward chaining from the conclusion to the assertions",0
425,"Today the term data is associated in many peoples’ minds mainly with computers.  However, data (as in “given facts” or measurements) has been used as a term by scientists and scholars for centuries.  Just as with a counting bead, a notch in a stick, or a handwritten tally, data as stored in a computer (or on digital media) is a representation of facts about the world.  These facts might be temperature readings, customer addresses, dots in an image, the characteristics of a sound at a given instant, or any number of other things.  But because computer data is not a fact but a representation of facts, its accuracy and usefulness depends not only on the accuracy of the original data, but on its context in the computer",0
426,"Abstract data types are used to describe a “generic” type of data, specifying how the data is stored and what operations can be performed on it ,. DFor example, an abstract stack data type includes a structure for storing data (such as a list or array) and a set of operations, such as “pushing” an integer onto the stack and “popping” (removing) an integer from the stack.  (For the process of combining data and operations into a single entity, see encapsulation. ) Abstract data types can be implemented directly in object-oriented programming languages",0
427,"One advantage of using abstract data types is that it separates a structure and functionality from its implementation.  In designing the abstract stack type, for example, one can focus on what a stack does and its essential functions.  One avoids becoming immediately bogged down with details, such as what sorts of data items can be placed on the stack, or exactly what mechanism will be used to keep track of the number of items currently stored.  This approach also avoids “featuritis,” the tendency to see how many possible functions or features one can add to the stack object.  For example, while it might be useful to give a stack the ability to print out a list of its items, it is probably better to wait until one needs such a capability than to burden the basic stack idea with extra baggage that may make it more cumbersome or less efficient. An abstract data type or its embodiment",0
428,"There are a variety of ways in which data (facts or measurements about the world) can be turned into a digital representation suitable for manipulation by a computer.  For example, pressing a key on the keyboard sends a signal that is stored in a memory buffer using a value that represents the ASCII character code for the key pressed.  Moving the mouse sends a stream of signals that are proportional to the rotation of the ball which in turn is calibrated into a series of coordinates and ultimately to a position on the screen where the cursor is to be moved.  Digital cameras and scanners convert the varying light levels of what they “see” into a digital image",0
429,"The data acquisition system begins with a transducer, which is a device that converts a physical phenomenon (such as heat) into a proportional electrical signal.  Transducers include devices such as thermistors, thermocouples, and pressure or strain gauges.  The output of the transducer is then fed into a signal conditioning circuit.  The purpose of signal conditioning is to make sure the signal fits into the range needed by the data processing device.  Thus the signal may be amplified or its voltage may be adjusted or scaled to the required level.  Another function of signal conditioning is to isolate the incoming signal from the computer to which the acquisition device is connected.  This is necessary both to protect the delicate computer circuits from possible “spikes” in the incoming signal and to prevent “noise” (extraneous electromagnetic signals created by the computer itself) from distorting the signal, and thus the ultimate measurements.  Various sorts of filters can be added for this purpose. ",0
430,"Data acquisition systems are essential to gathering and processing the detailed data required by scientific and engineering applications.  The automated control of chemical or biochemical processes requires the ability of the control software to assess real-time physical data in order to make timely adjustments to such factors as temperature, pressure, and the presence of catalysts, inhibitors, or other components of the process.  The highly automated systems used in modern aviation and increasingly, even in ground vehicles, depend on real-time data acquisition.  It is not surprising, then, that data acquisition is one of the fastest-growing fields in computing",0
431,"Database administration is the management of database systems ,.  Database administration can be divided into four broad areas: data security, data integrity, data accessibility, and system development. ",0
432,"With regard to databases, ensuring data security includes the assignment and control of users’ level of access to sensitive data and the use of monitoring tools to detect compromise, diversion, or unauthorized changes to database files ,.  When data is proprietary, licensing agreements with both database vendors and content providers may also need to be enforced. ",0
433,"Data integrity is related to data security, since the completeness and accuracy of data that has been compromised can no longer be guaranteed.  However, data integrity also requires the development and testing of procedures for the entry and verification of data (input) as well as verifying the accuracy of reports (output).  Database administrators may do some programming, but generally work with the programming staff in maintaining data integrity.  Since most data in computers ultimately comes from human beings, the training of operators is also important. Within the database structure itself, the links between data fields must be maintained (referential integrity) and a locking system must be employed to ensure that a new update is not processed while a pending one is incomplete ,. Internal procedures and external regulations may require that a database be periodically audited for accuracy.  While this may be the province of a specially trained information processing auditor, it is often added to the duties of the database administrator",0
434,"Accessibility has two aspects.  First, the system must be reliable.  Data must be available whenever needed by the organization, and in many applications such as e-commerce, this means 24 hours a day, 7 days a week (24/7).  Reliability requires making the system as robust as possible, such as by “mirroring” the database on multiple servers (which in turn requires making sure updates are stored concurrently).  Failure must also be planned for, which means the imple-130? ? ? ? database administrationmentation of onsite and offsite backups and procedures for restoring data ",0
435,"A database management system consists of a database (a collection of information, usually organized into records with component fields) and facilities for adding, updating, retrieving, manipulating, and reporting on data. ",0
436,"Broadly speaking, data communications is the transfer of data between computers and their users.  At its most abstract level, data communications requires two or more computers, a device to turn data into electronic signals (and back again), and a transmission medium.  Telephone lines, fiber optic cable, network (Ethernet) cable, video cable, radio (wireless), or other kinds of links can be used.  Finally, there must be software that can manage the flow of data",0
437,"Data communications are the basis both for networks and for the proper functioning of servers that provide services such as World Wide Web pages, electronic mail, online databases, and multimedia content (such as audio and streaming video).  While Web page design and e-commerce are the “bright lights” that give cyberspace its character, data communications are like the plumbing without which computers cannot work together.  The growing demand for data communications, particularly broadband services such as DSL and cable modems, translates into a steady demand for engineers and technicians specializing in the maintenance and growth of this infrastructure ,. ",0
438,"The process of removing redundant information from data so that it takes up less space is called data compression.  Besides saving disk space, compressing data such as e-mail attachments can make data communications faster. Compression methods generally begin with the realization that not all characters are found in equal numbers in text.  For example, in English, letters such as e and s are found much more frequently than letters such as j or x. By assigning the shortest bit codes to the most common characters and the longer codes to the least common characters, the number of bits needed to encode the text can be minimized",0
439,"The developer of each application program that writes data files must define a format for the data.  The format must be able to preserve all the features that are supported by the program.  For example, a word processing program will include special codes for font selection, typestyles (such as bold or italic), margin settings, and so on. In most markets there are more than one vendor, so there is the potential for users to encounter the need to convert files such as word processing documents from one vendor’s format to another.  For example, a Microsoft Word user needing to send a document to a user who has WordPerfect, or the user may encounter another user who also has Microsoft Word, but a later version. ",0
440,"A modern enterprise database system can contain hundreds of separate data items, each with important characteristics such as field types and lengths, rules for validating the data, and links to various databases that use that item ,.  There can also be many different views or ways of organizing subsets of the data, and stored procedures (program code modules) used to perform various data processing functions.  A developer who is creating or modifying applications that deal with such a vast database will often need to check on the relationships between data elements, views, procedures, and other aspects of the system",0
441,"The process of analyzing existing databases in order to find useful information is called data mining.  Generally, a database, whether scientific or commercial, is designed for a data mining? ? ? ? 135particular purpose, such as recording scientific observations or keeping track of customers’ account histories.  However, data often has potential applications beyond those conceived by its collector",0
442,"A data structure is a way of organizing data for use in a computer program.  There are three basic components to a data structure: a set of suitable basic data types, a way to organize or relate these data items to one another, and a set of operations, or ways to manipulate the data. For example, the array is a data structure that can consist of just about any of the basic data types, although all data must be of the same type.  The way the data is organized is by storing it in sequentially addressable locations.  The operations include storing a data item (element) in the array and retrieving a data item from the array. ",0
443,"The data structures commonly used in computer science include arrays (as discussed above) and various types of lists.  The primary difference between an array and a list is that an array has no internal links between its elements, data structures? ? ? ? 137while a list has one or more pointers that link the elements.  There are several types of specialized list.  A tree is a list that has a root (an element with no predecessor), and each other element has a unique predecessor.  The guarantee of a unique path to each tree node can make the operations of inserting or deleting an item faster.  A stack is a list that is accessible only at the top (or front).  Any new item is inserted (“pushed”) on top of the last item, and removing (“popping”) an item always removes the item that was last inserted.  This order of access is called LIFO (last in, first out).  A list can also be organized in a first in, first out (FIFO) order.  This type of list is called a queue, and is useful in a situation where tasks must “wait their turn” for attention",0
444,"The preceding data types all hold single values.  However, most modern languages allow for the construction of data types that can hold more than one piece of data.  The arrayis the most basic structured data type; it represents a series of memory locations that hold data of one of the basic types.  Thus, in Pascal an array of integer holds integers, each taking up two bytes of memory. Many languages have composite data types that can hold data of several different basic types.  For example, the struct in C or the record in Pascal can hold data such as a person’s first and last name, three lines of address (all arrays of characters, or strings), an employee number (perhaps an integer or double), a Boolean field representing the presence or absence of some status, and so on.  This kind of data type is also called a user-defined data type because programmers can define and use these types in almost the same ways as they use the language’s built-in basic types. What is the difference between data types and data structures? There is no hard-and-fast distinction.  Generally, data structures such as lists, stacks, queues, and trees are more complex than simple data types, because they include data relationships and special functions (such as pushing or popping data on a stack).  However, a list is the fundamental data type in list-processing languages such as Lisp, and string operators are built into languages such as Snobol.  (See list processing, stack, queue, and tree. )Further, in many modern languages fundamental and structured data types are combined seamlessly into classes that combine data structures with the relevant operations ,",0
445,"data warehouseModern business organizations create and store a tremendous amount of data in the form of transactions that become database records.  Increasingly, however, businesses are relying on their ability to use data that was collected for one purpose (such as sales, customer service, and inventory) for purposes of marketing research, planning, or decision support.  For example, transaction data might be revisited with a view to identifying the common characteristics of the firm’s best customers or determining the best way to market a particular type of product.  In order to conduct such research or analysis, the data collected in the course of business must be stored in such a way that it is both accurate and flexible in terms of the number of different ways in which it can be queried.  The idea of the data warehouse is to provide such a repository for data",0
446,"A decision support system (DSS) is a computer application that focuses on providing access to or analysis of the key information needed to make decisions, particularly in business.  (It can be thought of as a more narrowly focused approach to computer assistance to management—see management information system. )The development of DSS has several roots reaching back to the 1950s.  This includes operational analysis and the theory of organizations and the development of the first interactive (rather than batch-processing) computer systems.  Indeed, the SAGE automated air defense system developed starting in the 1950s could be described as a military DSS.  The system presented real-time information (radar plots) and enabled the operator to select and focus on particular elements using a light pen.  By the 1960s more-systematic research on DSS was underway and included the provocative idea of “human-computer symbiosis” for problem solving ,. ",0
447,"Dell Computer (NASDAQ: DELL) is one of the world’s leading manufacturers and sellers of desktop and laptop computers ,.  By 2008 Dell had more than 88,000 employees worldwide. The company was founded by Michael Dell, a student at the University of Texas at Austin whose first company was PC’s Limited, founded in 1984.  Even at this early stage Dell successfully employed several practices that would come to typify the Dell strategy: Sell directly to customers (not through stores), build each machine to suit the customer’s preferences, and be aggressive in competing on price. In 1988 the growing company changed its name to Dell Computer Corporation.  In the early 1990s Dell tried an alternative business model, selling through warehouse clubs and computer superstores.  When that met with little success, Dell returned to the original formula.  In 1999 Dell overtook Compaq to become the biggest computer retailer in America. ",0
448,"The unusual computing term demon (sometimes spelled daemon) refers to a process (program) that runs in the background, checking for and responding to certain events.  The utility of this concept is that it allows for automation of information processing without requiring that an operator initiate or manage the process. For example, a print spooler demon looks for jobs that are queued for printing, and deals with the negotiations necessary to maintain the flow of data to that device.  Another demon (called chron in UNIX systems) reads a file describing processes that are designated to run at particular dates or times.  For example, it may launch a backup utility every morning at 1:00 a. m.  E-mail also depends on the periodic operation of “mailer demons. ”",0
449,"Design patterns are an attempt to abstract and generalize what is learned in solving one problem so that it can be applied to future similar problems.  The idea was first applied to architecture by Christopher Alexander in his book A Pattern Language.  Alexander described a pattern as a description of situations in which a particular problem occurs, with a solution that takes into account the factors that are “invariant” (not changed by context).  Guidance for applying the solution is also provided. For example, a bus stop, a waiting room, and a line at a theme park are all places where people wait.  A “place to wait” pattern would specify the problem to be solved (how to make waiting as pleasant as possible) and suggest solutions.  Patterns can have different levels of abstraction or scales on which they apply (for example, an intimate theater and a stadium are both places of entertainment, but one is much larger than the other). Patterns in turn are linked into a network called a pattern language.  Thus when working with one pattern, the designer is guided to consider related patterns.  For example, a pattern for a room might relate to patterns for seating or grouping the occupants. ",0
450,"The concept of patterns and pattern languages carries over well into software design.  As with architectural patterns, a software pattern describes a problem and solution, along with relevant structures ,.  Note that patterns are not executable code; they are at a higher level (one might say abstract enough to be generalizable, specific enough to be applicable). Software patterns can specify how objects are created and ways in which they function and interface with other objects.  Patterns are generally documented using a common format",0
451,"Traditionally documents such as advertisements, brochures, and reports were prepared by combining typed or printed text with pasted-in illustrations (such as photographs and diagrams).  This painstaking layout process was necessary in order to produce “camera-ready copy” from which a printing company could produce the final product. Starting in the late 1980s, desktop computers became powerful enough to run software that could be used to create page layouts.  In addition, display hardware gained a high enough resolution to allow for pages to be shown on the screen in much the same form as they would appear on the printed page.  (This is known by the acronym WYSIWYG, or “what you see is what you get. ”) The final ingredient for the creation of desktop publishing was the advent of affordable laser or inkjet printers that could print near print quality text and high-resolution graphics",0
452,"A fundamental problem in computer design is the control of devices such as disk drives and printers.  Each device is designed to respond to a particular set of control commands sent as patterns of binary values through the port to which the device is connected.  For example, a printer will respond to a “new page” command by skipping lines to the end of the current page and moving the print head to the start of the next page, taking margin settings into account.  The problem is this: When an applications program such as a word processor needs to print a document, how should the necessary commands be provided to the printer? If every application program has to include the appropriate set of commands for each device that might be in use, programs will be bloated and much development effort will be required for supporting devices rather than extending the functionality of the product itself.  Instead, the manufacturers of printers and other devices such as scanners and graphics tablets typically provide a program called a driver.  (A version of the driver is created for each major operating system in use. ) The driver serves as the intermediary between the application, the operating system and the low-level device control system.  It is sometimes useful to have drivers in the form of continually running programs that monitor the status of a device and wait for commands. ",0
453,"Also called digital money or e-cash, digital cash represents the attempt to create a method of payment for online transactions that is as easy to use as the familiar bills and coins in daily commerce ,.  At present, credit cards are the principal means of making online payments.  While using credit cards takes advantage of a well-established infrastructure, it has some disadvantages.  From a security standpoint, each payment potentially exposes the payer to the possibility that the credit card number and possibly other identifying information will be diverted and used for fraudulent transactions and identity theft.  While the use of secure (encrypted) online sites has reduced this risk, it cannot be eliminated entirely ,.  Credit cards are also impracticable for very small payments from cents to a few dollars (such as for access to magazine articles) because the fees charged by the credit card companies would be too high in relation to the value of the transaction. ",0
454,"The concept of digital convergence is an attempt to explore the implications of so many formerly disparate analog media now being available in digital form.  All forms of digital media have key features in common.  First, they are essentially pure information (computer data).  This means that regardless of whether the data originally represented still images from a camera, video, or film, the sound of a human voice, music, or some other form of expression, that data can be stored, manipulated, and retrieved under the control of computer algorithms.  This makes it easier to create seamless multimedia presentations ,.  Services or products previously considered to be separate can be combined in new ways. ",0
455,"The dashboard of a car is designed to present vital real-time information to the driver, such as speed, fuel supply, and engine status.  Ideally this information should be easy to grasp at a glance, allowing for prompt action when necessary.  Conversely, unnecessary and potentially distracting information should be avoided, or at least relegated to an unobtrusive secondary display. A digital dashboard is a computer display that uses similar concepts.  Its goal is to provide an executive or manager with the key information that allows him or her to monitor the health of the enterprise and to take action when necessary.  (A digital dashboard can also be part of a larger set of management tools—see decision support system. ",0
456,"The term digital divide was coined in the late 1990s amid growing concern that groups such as minorities, the elderly, and rural residents were not becoming computer literate and connecting to the Internet at the same rate as the young, educated, and relatively affluent. Nearly a decade later this perception of a chasm has diminished somewhat.  According to the Pew Internet & American Life project, as of 2006 about two-thirds (70 per-148? ? ? ? digital dashboardcent) of American adults were using the Internet, and the number has continued to increase, though more slowly (there is evidence of a “hard core” unconnected population).  Groups that lagged in Internet usage included Americans 65 years or older (35 percent), African Americans (58 percent), and persons without at least a high school education (36 percent). ",0
457,"By default, once information is digitized it is simply a pattern of bits that can be easily copied within the same or a different medium, using a variety of software or the built-in facilities of the operating system.  Of course the development of tape-recording technology in the mid-20th century already made it possible to copy audio recordings, and the later development of videotape and the VCR did the same for video.  However, while analog copying techniques lose some accuracy (or fidelity) with each generation of copying, digital files can be copied exactly each time.  It is equally easy to e-mail, upload, or otherwise distribute audio or video files. Legally, the creator of an original work can assert copyright—literally, the “right to copy” or to control when and how the work is distributed.  Digital rights management (DRM) refers to a variety of technologies that can be used to enforce this right by making it at least difficult for the purchaser of one copy of a work to copy and distribute it in turn.  (Similar technologies have also been used to prevent copying of software, which is, after all, just another pattern of bits—see copy protection and software piracy andcounterfeiting. )",0
458,"This concept involves the creation of a software system that runs programs and stores data across a number of different computers, an idea pervasive today.  A simple form is the central computer (such as in a bank or credit card company) with which thousands of terminals communicate to submit transactions.  While this system is in some sense distributed, it is not really decentralized.  Most of the work is done by the central computer, which is not dependent on the terminals for its own functioning.  However, responsibilities can be more evenly apportioned between computers ,. ",0
459,"Distributed computing is particularly suited to applications that require extensive computing resources and that may need to be scaled (smoothly enlarged) to accommodate increasing needs ,.  Examples might include large databases, intensive scientific computing, and cryptography.  A particularly interesting example is SETI@home, which invites computer users to install a special screen saver that runs a distributed process during the computer’s idle time.  The process analyzes radio telescope data for correlations that might indicate receipt of signals from an extraterrestrial intelligence ,. Besides being able to marshal very large amounts of computing power, distributed systems offer improved fault tolerance.  Because the system is decentralized, if a particular computer fails, its processes can be replaced by ones running on other machines.  Replication (copying) of data across a widely dispersed network can also provide improved data recovery in the event of a disaster",0
460,"The operation of the Internet requires that each participating computer have a unique address to which data packets can be routed ,.  The Domain Name System (DNS) provides alphabetical equivalents to the numeric IP addresses, giving the now familiar-looking Web addresses (URLs), e-mail addresses, and so on. The system uses a set of “top-level” domains to categorize these names.  One set of domains is based on the nature of the sites involved, including: . com (commercial, corporate), . edu (educational institutions), . gov (government), . mil (military), . org (nonprofit organizations), . int (international organizations), . net (network service providers, and so on). The other set of top-level domains is based on the geographical location of the site.  For example, . au (Australia), . fr (France), and . ca (Canada).  (While the United States has the . us domain, it is generally omitted in practice, because the Internet was developed in the United States)",0
461,"The Document Object Model (DOM) is a way to represent a Web document , as an object that can be manipulated using code in a scripting language ,.  The DOM was created by the World Wide Web Consortium (W3C) as a way to standardize methods of manipulating Web pages at a time when different browsers used different access models.  The full specification is divided into four levels (0 through 3).  By 2005, most DOMspecifications were supported by the major Web browsers. Using DOM, a programmer can navigate through the hierarchical structure of a document, following links or “descending” into forms and user-interface objects.  With DOM one can also add HTML or XML elements, as well as load, save, or format documents. ",0
462,"DSL (digital subscriber line) is one of the two most prevalent forms of high-speed wired access to the Internet ,.  DSL can operate over regular phone lines (sometimes called POTS or “plain old telephone service”).  DSL takes advantage of the fact that existing phone lines can carry frequencies far beyond the narrow band used for voice telephony.  When installing DSL, the phone company must evaluate the quality of existing lines to determine how many frequency bands are usable, and thus how much data can be transmitted.  Further, because the higher the frequency the shorter the distance the signal can travel, the available bandwidth drops as one gets farther from the central office or a local DSL access Multiplexer (DSLAM)",0
463,"A digital video recorder (DVR) records digital television broadcasts and stores them on a disk ,.  DVRs first appeared as commercial products in 1999 in Replay TV and TiVo, the latter becoming the most successful player in the field. A DVR works with digital signals and discs rather than tape used by the video cassette recorders (VCRs) that had become popular starting in the 1980s.  The digital recorder has several advantages over tape:, much larger capacity, limited only by hard drive size, instant (random) access to any recorded programming without having to go forward or backward through a tape, the ability to “time shift” within a live broadcast, including pausing and instant replay, the ability to skip over commercials, digital special effects",0
464,"The C programming language was developed in the early 1970s by Dennis Ritchie, who based it on the earlier languages BCPL and B.  C was first used on DEC PDP-11 computers running the newly developed UNIX operating system, where the language provided a high-level alternative to the use of PDP Assembly language for development of the many utilities that give UNIX its flexibility.  Since the 1980s, C and its descendent, C++, have become the most widely used programming languages. ",0
465,"Introduced in 2002, C# (pronounced “C sharp”) is a programming language similar to C++ and Java but simplified in several respects and tailored for use with Microsoft’s latest programming platform ,.  C# is a general-purpose language and is thoroughly objectoriented—all functions must be declared as members of a class or “struct,” and even fundamental data types are derived from the System. Object class ,. Compared with C++, C# is stricter about the use and conversion of data types, not allowing most implicit conversions (such as from an enumeration type to the corresponding integer—see data structures).  Unlike C++, C# does not permit multiple inheritance (where a type can be derived from two or more base types), thereby avoiding an added layer of complexity in class relationships in large software projects.  (However, a similar effect can be obtained by declaring multiple “interfaces” or specified ways of accessing the same class. )Unlike Java (but like C++), C# includes pointers (and a safer version called “delegates”), enumerations (enum types), structs (treated as lightweight classes), and overloading (multiple definitions for operators).  The latest version of the language, C# 3. 0 (introduced in 2007), provides additional features for list processing and functional programming ,",0
466,"The C++ language was designed by Bjarne Stroustrup at AT&T’s Bell Labs in Murray Hill, New Jersey, starting in 1979.  By that time the C language had become well established as a powerful tool for systems programming ,.  However Stroustrup (and others) believed that C’s limited data structures and function mechanism were proving inadequate to express the relationships found in increasingly large software packages involving many objects with complex relationships. ",0
467,"During the late 1980s and 1990s, C++ became a very popular language for a variety of applications ranging from systems programming to business applications and games.  The growth of the language coincided with the development of more powerful desktop computers and the release of inexpensive, easy-to-use but powerful development environments from Microsoft, Borland, and others.  Since these compilers could also handle traditional C code, programmers could “port” existing code and use the object-oriented techniques of C++ as they mastered them.  By the late 1990s, however, C++, although still dominant in many areas, was being challenged by Java, a language that simplified some of the more complex features of C++ and that was designed 68? ? ? ? C++particularly for writing software to run on Web servers and browsers ,.  For an alternative approach to creating a somewhat more “streamlined” C-type language, ",0
468,A basic problem in computer design is how to optimize the fetching of instructions or data so that it will be ready when the processor (CPU) needs it.  One common solution is to use a cache.  A cache is an area of relatively fast-access memory into which data can be stored in anticipation of its being needed for processing.  Caches are used mainly in two contexts: the processor cache and the disk cache,0
469,"The use of a processor cache is advantageous because instructions and data can be fetched more quickly from the cache (static memory chips next to or within the CPU) than they can be retrieved from the main memory (usually dynamic RAM).  An algorithm analyzes the instructions currently being executed by the processor and tries to anticipate what instructions and data are likely to be needed in the near future.  (For example, if the instructions call for a possible branch to one of two sets of instructions, the cache will load the set that has been used most often or most recently.  Since many programs loop over and over again through the same instructions until some condition is met, the cache’s prediction will be right most of the time. )",0
470,"A disk cache uses the same general principle as a processor cache.  Here, however, it is RAM (either a part of main memory or separate memory on the disk drive) that is the faster medium and the disk drive itself that is slower.  When an application starts to request data from the disk, the cache reads one or more complete blocks or sectors of data from the disk rather than just the data record being requested.  Then, if the application continues to request sequential data records, these can be read from the high-speed memory on the cache rather than from the disk drive.  It follows that disk caching is most effective when an application, for example, loads a database file that is stored sequentially on the disk",0
471,"Caching techniques can be used in other ways.  For example, most Web browsers are set to store recently read pages on disk so that if the user directs the browser to go back to such a page it can be read from disk rather than having to be retransmitted over the Internet (generally a slower process).  Web servers and ISPs (such as cable services) can also cache popular pages so they can be served up quickly",0
472,"The final stage in the development of the calculator would be characterized by the use of electronics to replace mechanical (or electromechanical) action.  The use of logic 70? ? ? ? calculatorcircuits to perform calculations electronically was first seen in the giant computers of the late 1940s, but this was obviously impractical for desktop office use.  By the late 1960s, however, transistorized calculators comparable in size to mechanical desktop calculators came into use.  By the 1970s, the use of integrated circuits made it possible to shrink the calculator down to palm-size and smaller.  These calculators use a microprocessor with a set of “microinstructions” that enable them to perform a repertoire of operations ranging from basic arithmetic to trigonometric, statistical, or business-related function",0
473,"Development of automotive technology has tended to be incremental rather than revolutionary.  The core “hardware” such as the engine and drive train has changed little over several decades, other than the replacement of carburetors with fuel injection systems, and some improvements in areas such as brake design.  On the other hand there have been significant improvements in safety features such as seat belts, air bags, and improved crash absorption barriers. In recent years, however, the incorporation of computers in automobile design , has led to a number of significant advances in areas such as fuel efficiency, traction/stability, crash response, and driver information and navigation.  Put simply, cars are becoming “smarter” and are making driving easier and safer",0
474,"Much future progress in car computing will depend on creating integrated networking between vehicles and the road.  An advanced navigation system could take advantage of real-time information being transmitted by the surrounding vehicles.  For example, a stalled car would transmit warning messages to other drivers about the impending obstacle.  Vehicles that sense an oil slick, ice, or other road hazard could also “mark” the location so it can be avoided by subsequent drivers.  Data about the speed and spacing of traffic could provide real-time information about traffic jams, possibly routing vehicles into alternative lanes or other roads to reduce congestion and travel time ",0
475,"Smart cars are vehicles equipped with advanced computing and communication technologies that enable them to collect, analyze, and share data to provide a wide range of features and services.  These technologies include:Advanced Driver Assistance Systems (ADAS): Smart cars are equipped with a variety of sensors and cameras that enable ADAS features such as lane departure warning, automatic emergency braking, adaptive cruise control, and blind spot monitoring.  These features use computer algorithms to analyze data from the sensors and provide alerts or take action to prevent accidents. Connectivity: Smart cars are connected to the internet and other vehicles, enabling them to share data about traffic conditions, road hazards, and weather conditions.  This connectivity also allows for over-the-air software updates, remote diagnostics, and other services. Infotainment: Smart cars offer advanced infotainment systems that provide features such as internet connectivity, voice commands, music streaming, and navigation.  These systems are often integrated with the vehicle's ADAS features to provide real-time information and alerts. Self-driving: Some smart cars are capable of autonomous driving, using a combination of sensors, cameras, and computer algorithms to navigate roads without human intervention.  These cars are still in development and are not yet widely available. Energy Efficiency: Smart cars often use advanced technologies such as regenerative braking, stop-start systems, and aerodynamic designs to improve fuel efficiency and reduce emissions. ",0
476,"During the late 1950s and 1960s, software rapidly grew more complex—especially operating system software and large business applications.  With the typical program consisting of many components being developed by different programmers, it became difficult both to see the “big picture” and to maintain consistent procedures for transferring data from one program module to another.  As computer scientists worked to develop sounder principles , it also occurred to them that the power of the computer to automate procedures could be used to create tools for facilitating program design and managing the resulting complexity.  CASE, or computer-aided software engineering, is a catchall phrase that covers a variety of such tools involved with all phases of development",0
477,"The earliest design tool was the flowchart, often drawn with the aid of a template that could be used to trace the symbols on paper ,.  With its symbols for the flow of execution through branching and looping, the flowchart provides a good tool for visualizing how a program is intended to work.  However large and complex programs often result in a sea of flowcharts that are hard to relate to one another and to the program as a whole.  Starting in the 1960s, the creation of programs for manipulating flow symbols made it easier both to design flowcharts and to visualize them in varying levels of detail. ",0
478,"Once a program has been designed and implementation is under way, CASE tools can help the programmers maintain consistency across their various modules.  One such tool (now rather venerable) is the data dictionary, which is a database whose records contain information about the definition of data items and a list of program components that use each item ,.  When the definition of a data item is changed, the data dictionary can provide a list of affected components.  Database technology is also applied to software design in the creation of a database of objects within a particular program, which can be used to provide more extensive information during debugging. ",0
479,"Analysis tools are computer software or web applications that are used to analyze data, identify patterns, and draw insights from large and complex datasets.  These tools are used by analysts, researchers, and data scientists to process and interpret data, in order to make informed decisions and predictions. Some popular analysis tools include:Statistical analysis software: These tools are used to perform statistical analysis on data, such as hypothesis testing, regression analysis, and clustering.  Examples include R, SAS, and SPSS. Data visualization software: These tools are used to create charts, graphs, and other visual representations of data, in order to help users understand and communicate data insights.  Examples include Tableau, Power BI, and QlikView. Business intelligence software: These tools are used to analyze data from various sources, such as sales data, customer data, and financial data, in order to make informed business decisions.  Examples include SAP BusinessObjects, IBM Cognos, and Oracle Business Intelligence. Machine learning software: These tools are used to train and deploy machine learning models, in order to make predictions and automate decision-making.  Examples include TensorFlow, Scikit-learn, and Keras. Big data tools: These tools are used to process and analyze large and complex datasets, such as those generated by social media, sensors, and IoT devices.  Examples include Apache Hadoop, Spark, and Cassandra. ",0
480,"In the late 1990s, a new consumer technology enabled users to create their own CDs with data or audio tracks.  The cheapest kind, CD-R (Compact Disk Recordable) uses a layer of a dyed material and a thin gold layer to reflect the laser beam.  Data is recorded by a laser beam hitting the dye layer in precise locations and marking it (in one of several ways, depending on technology).  The lengths of marked (“striped”) track and unmarked track together encode the data. ",0
481,"The DVD (alternatively, Digital Video Disc or Digital Versatile Disc) is similar to a CD, but uses laser light with a shorter wavelength.  This means that the size of the pits and lands will be considerably smaller, which in turns means that much more data can be stored on the same size disk.  A DVD disk typically stores up to 4. 7 GB of data, equivalent to about six CDs.  This capacity can be doubled by using both sides of the disk. The high capacity of DVD-ROMs (and their recordable equivalent, DVD-RAMs) makes them useful for storing feature-length movies or videos, very large games and multimedia programs, or large illustrated encyclopedias.  The development of high-definition television (HDTV) standards spurred the introduction of higher capacity DVD formats.  The competition between Sony’s Blu-Ray and HD-DVD (backed by Toshiba and Microsoft, among others) was resolved by 2008 in favor of the former.  BluRay offers high capacity (25GB for single layer discs, 50GB for dual layer). ",0
482,"Cellular automata theory has been applied to a variety of fields that deal with the complex interrelationships of components, including biology (microbe growth and population dynamics in general), ecology (including forestry), and animal behavior, such as the flight of birds.  (The cues that a bird identifies in its neighbors are like the input conditions for a cell in a cellular automaton.  The “output” would be the bird’s flight behavior. )The ability of cellular automatons to generate a rich complexity from simple components and rules mimics the development of life from simple components, and thus cellular automation is an important tool in the creation and study of artificial life.  This can be furthered by combining a set of cellular automation rules with a geneticalgorithm, including a mechanism for inheritance of characteristics.  Cellular automation principles can also be applied to engineering in areas such as pattern or image recognition",0
483,"Governments have always to varying degrees concerned themselves with the content of public media.  The growing use of the Internet for expressive activities , has prompted authoritarian governments such as that of China to attempt to block “objectionable” material both through filtering techniques , and through pressure on service providers.  Further, users identified as creators of banned content may be subjected to prosecution.  However because of the Internet’s decentralized structure and the ability of users to operate relatively anonymously, Internet censorship tends to be only partially effective ,. In the democratic West, Internet censorship generally applies to only a few forms of content.  Attempts to criminalize the online provision of pornography to minors in the 1996 Communications Decency Act have generally been overturned by the courts as excessively infringing on the right of adults to access such content.  However, a succession of bills seeking to require schools and libraries to install Web-filtering software culminated in the Children’s Internet Protection Act, which was upheld by the U. S.  Supreme Court in 2003. ",0
484,"By itself, a Web page coded in HTML is simply a “static” display that does not interact with the user (other than for the selection of links).  (See html, dhtml, and xhtm. ) Many Web services, including online databases and e-commerce transactions, require that the user be able to interact with the server.  For example, an online shopper may need to browse or search a catalog of CD titles, select one or more for purchase, and then complete the transaction by providing credit card and other information.  These functions are provided by “gateway programs” on the server that can access databases or other facilities. One way to provide interaction with (and through) a Web page is to use the CGI (common gateway interface).  CGI is a facility that allows Web browsers and other client programs to link to and run programs stored on a Web site.  The stored programs, called scripts, can be written in various languages such as JavaScript or PHP , and placed in a cgi-bin folder on the Web server. The CGI script is referenced by an HTML hyperlink on the Web page, such as<A HREF=“http://www. MyServer. com/cgi-bin/MyScript”>MyScript </A>",0
485,"While the attention of the first computer designers focused mainly on numeric calculations, it was clear that much of the data that business people and others would want to manipulate with the new machines would be textual in nature.  Billing records, for example, would have to include customer names and addresses, not just balance totals. The “natural” representation of data in a computer is as a series of two-state (binary) values, interpreted as binary numbers.  The solution for representing text (letters of the alphabet, punctuation marks, and other special symbols) is to assign a numeric value to each text symbol.  The result is a character code, such as ASCII (American Standard Code for Information Interchange), which is the scheme used most widely today.  (Another system, EBCDIC (Extended Binary-Coded Decimal Interchange Code) was used during the heyday of IBM mainframes, but is seldom used today. ",0
486,"Sophisticated string processing (such as parsing and pattern matching) tends to be awkward to express in traditional number-oriented programming languages.  Several languages have been designed especially for manipulating textual data.  Snobol, designed in the early 1960s, is best 82? ? ? ? characters and stringsknown for its sophisticated pattern-matching and pattern processing capabilities.  A similar language, Icon, is widely used for specialized string-processing tasks today.  Many programmers working with textual data in the UNIX environment have found that the awk and Perl languages are easier to use than C for extracting and manipulating data fields.  (See awk and Perl. )",0
487,"Many PC users have become acquainted with chatting through participating in “chat rooms” operated by online services such as America Online (AOL).  A chat room is a “virtual space” in which people meet either to socialize generally or to discuss particular topics.  At their best, chat rooms can develop into true communities whose participants develop long-term friendships and provide one another with information and emotional support ,. However, the essentially anonymous character of chat (where participants often use “handles” rather than real names) that facilitates freedom of expression can also provide a cover for mischief or even crime.  Chat rooms have acquired a rather lurid reputation in the eyes of the general public.  There has been considerable public concern about children becoming involved in inappropriate sexual conversation.  This has been fueled by media stories (sometimes exaggerated) about children being recruited into face-toface meetings with pedophiles.  AOL and other online services have tried to reduce such activity by restricting online sex chat to adults, but there is no reliable mechanism for a service to verify its user’s age.  A chat room can also be supervised by a host or moderator who tries to prevent “flaming” (insults) or other behavior that the online service considers to be inappropriate",0
488,"For people who find commercial online services to be too expensive or confining, there are alternatives available for just the cost of an Internet connection.  The popular Internet Relay Chat (IRC) was developed in Finland by Jarkko Oikarinen in the late 1980s.  Using one of the freely available client programs, users connect to an IRC server, which in turn is connected to one of dozens of IRC networks.  Users can create their own chat rooms (called channels).  There are thousands of IRC channels with participants all over the world.  To participate, a user simply joins a channel and sees all messages currently being posted by other users of the channel.  In turn, the user’s messages are posted for all to see.  While IRC uses only text, there are now enhanced chat systems (often written in Java to work with a Web browser) that add graphics and other features. There are many other technologies that can be used for conversing via the Internet.  Some chat services (such as Cu-SeeMe) enable participants to transmit their images ,.  Voice can also be transmitted over an Internet connection ,.  For a very pervasive form of “ad hoc” textual communication, see texting and instant messaging",0
489,"The famous Turing test , proposes that if a human is unable to reliably distinguish messages from a computer from those of another person, the computer program involved can at least be provisionally declared to be “intelligent. ” The advent of textual communication via the Internet , has afforded a variety of ways to attempt to meet this challenge.  Programs that mimic human conversational styles have come to be known as “chatterbots. ”The prototypical chatterbot was ELIZA, developed by Joseph Weizenbaum in the mid-1960s ,.  ELIZA mimicked a form of nondirective psychotherapy in which the therapist echoes or plays off of the client’s statements as a form of gentle encouragement and validation.  Thus if one types, “My father didn’t really like me,” ELIZA might reply, “Tell me more about your father. ” Although primitive, ELIZA once inadvertently fooled an executive into thinking he was exchanging messages with Weizenbaum.  Other classic chatterbots include Parry, designed to mimic a paranoid, and the story-generating Racter. ",0
490,"With simple rules but endless permutations, chess has fascinated millions of players for hundreds of years.  When mechanical automatons became fashionable in the 18th century, onlookers were intrigued by “the Turk,” a chessplaying automaton.  While the Turk was eventually shown to be a hoax (a human player was hidden inside), the development of the electronic digital computer in the mid-20th century provided the opportunity to create a true automatic chess player. In 1950 Claude Shannon outlined the two basic strategies that would be used by future chess-playing programs.  The “brute force” strategy would examine the possible moves for the computer chess player, the possible replies of the opponent to each move, the possible next moves by the computer, and so on for as many half moves or “plies” as possible.  The moves would be evaluated by a “minimax” algorithm that would find the move that best improves the computer’s position despite the opponent’s best play. ",0
491,"The earliest computer chess theorists such as Claude Shannon and Alan Turing saw the game as one potential way to demonstrate true machine intelligence.  Ironically, by the time computers had truly mastered chess, the artificial intelligence (AI) community had concluded that mastering the game was largely irrelevant to their goals.  AI pioneers Herbert Simon and John McCarthy have referred to chess as “the Drosophila of AI. ” By this they mean that, like the ubiquitous fruit flies in genetics research, chess became an easy way to measure computer prowess.  But what was it measuring? The dominant brute-force approach was more a measure of computing power than the application of such AI techniques as pattern recognition.  (There is, however, still some interest in writing chess programs that “think” more like a human player. ) In recent years there has been some interest in programming computers to play the Asian board game Go, where positional and structural elements play a greater role than in chess.  However, even the latest generation of Go programs seem to be relying more on a statistical approach than a deep conceptual analysis. ",0
492,"In personal computers a chipset is a group of integrated circuits that together perform a particular function.  System purchasers generally think in terms of the processor itself (such as a Pentium III, Pentium IV, or competitive chips from AMD or Cyrix).  However they are really buying a system chipset that includes the microprocessor itself , and often a memory cache (which may be part of the microprocessor or a separate chip—see cache) as well as the chips that control the memory bus (which connects the processor to the main memory on the motherboard—see bus. ) The overall performance of the system depends not just on the processor’s architecture (including data width, instruction set, and use of instruction pipelines) but also on the type and size of the cache memory, the memory bus (RDRAM or “Rambus” and SDRAM) and the speed with which the processor can move data to and from memory. In addition to the system chipset, other chipsets on the motherboard are used to support functions such as graphics (the AGP, or Advanced Graphics Port, for example), drive connection (EIDE controller), communication with external devices ,, and connections to expansion cards (the PCI bus). ",0
493,"In the telecommunications industry, “the last mile” refers to the connections and equipment that actually bring content to users’ homes and businesses.  One source of Cisco’s continued growth in the 2000 decade is the way it has addressed the consumer sector through strategic acquisitions.  In 2003, Cisco acquired Linksys, maker of home Internet routers and wireless access points.  In 2005, Scientific Atlanta—maker of cable modems, digital cable boxes, and other consumer equipment—also became a Cisco company. The company has also entered the area of Internet telephony , by teaming up with Skype to build a cordless phone that can connect to a computer to make phone calls over the Internet. Moving from hardware into software, Cisco in 2007 purchased Utah Street Networks, a San Francisco–based maker of software to link online communities , and operator of the Tribe. net Web site.  Around the same time, Cisco made a much larger buy, acquiring WebEx, maker of online collaboration software, for $3. 2 billion. In 2007 Cisco had revenue of $35 billion, with more than 63,000 employees. ",0
494,"A class is a data type that combines both a data structure and methods for manipulating the data.  For example, a string class might consist of an array to hold the characters in the string and methods to compare strings, combine strings, or extract portions of a string ,. As with other data types, once a class is declared, objects (sometimes called instances) of the class can be created and used.  This way of structuring programs is called object-oriented programming because the class object is the basic building block ,. Object-oriented programming and classes provide several advantages over traditional block-structured languages.  In a traditional BASIC or even Pascal program, there is no particular connection between the data structure and the procedures or functions that manipulate it.  In a large program one programmer might change the data structure without alerting other programmers whose code assumes the original structure.  On the other hand, someone might write a procedure that directly manipulates the internal data rather than using the methods already provided.  Either transgression can lead to hard-to-find bugs. ",0
495,"It is often more efficient to have a large, relatively expensive computer provide an application or service to users on many smaller, inexpensive computers that are linked to it by a network connection.  The term server can apply to both the application providing the service and the machine running it.  The program or machine that receives the service is called the client. A familiar example is browsing the Web.  The user runs a Web browser, which is a client program.  The browser connects to the Web server that hosts the desired Web site.  Another example is a corporate server that runs a database.  Users’ client programs connect to the database over a local area network (LAN).  Many retail transactions are also handled using a client-server arrangement.  Thus, when a travel or theater booking agent sells a ticket, the agent’s client program running on a PC or terminal connects to the server containing the database that keeps track of what seats are available",0
496,"There are several advantages to using the client-server model.  Having most of the processing done by one or more servers means that these powerful and more costly machines can be used to the greatest efficiency.  If more processing capacity is needed, more servers can be brought online without having to revamp the whole system.  Users, on the other hand, only need PCs (or terminals) that are powerful enough to run the smaller client program to connect to the server. Keeping the data in a central location helps ensure its integrity: If a database is on a server, transactions can be committed in an orderly way to ensure that, for example, the same ticket isn’t sold to two people.  A client-server model also offers flexibility to users.  Any client program that meets the standards supported by the server can be used to make a connection. ",0
497,"The transfer of data within the microprocessor and between the microprocessor and memory must be synchronized to ensure that the data needed to execute each instruction is available when the flow of execution has reached an appropriate point.  This synchronization is accomplished by moving data in intervals that correspond to the pulses of the system clock (a quartz crystal).  This is done by sending control signals that tell the components of the processor and memory when to send or wait for data.  Thus, if the microprocessor is the heart of the computer, the clock is the heart’s pacemaker.  Because most devices cannot run at the same pace as the processor, circuits in various parts of the motherboard create secondary control signals that run at various ratios of the actual system clock speed. ",0
498,"Common Business-Oriented Language was developed under the impetus of a 1959 Department of Defense initiative to create a common language for developing business applications that centered on the processing of data from files.  (The military, after all, was a “business” whose inventory control and accounting needs dwarfed those of all but the largest corporations. ) At the time, the principal business-oriented language for mainframe computers was FLOW-MATIC, a language developed by Grace Hopper’s team at RemingtonRand UNIVAC and limited to that company’s computers ,.  The first COBOL compilers became available in 1960, and the American National Standards Institute (ANSI) issued a standard specification for the language in 1968.  Expanded standards were issued in 1974 and 1985 (COBOL-74 and COBOL-85) with a new standard issued in 2002. ",0
499,"Short for “coder/decoder,” a codec is essentially an algorithm for encoding (and compressing) a stream of data for transmission, and then decoding and decompressing it at the receiving end.  Usually the data involved represents audio or video content ,.  Typically the data is being downloaded from a Web site to be played on a personal computer or portable player ,. A codec is described as “lossy” if some of the original information is lost in the compression process.  It then becomes a question of whether the loss in quality is perceived by the user as significant.  A codec that preserves all the information needed to re-create the original file is “lossless. ” For most purposes, the much greater size of the lossless version of a file is not worth the (often imperceptible) increase in quality or fidelity. A codec is usually used in connection with a “container format” that specifies how the encoded data is to be stored in a file.  Often a container can hold more than one data stream and even more than one kind of media (such as video and audio).  When one refers to a Windows WAV file, for example, one is actually referring to a container. ",0
500,"Cognitive science is the study of mental processes such as reasoning, memory, and the processing of perception.  It is necessarily an interdisciplinary approach that includes fields such as psychology, linguistics, and neurology.  The importance of the computer to cognitive science is that it offers a potential nonhuman model for a thinking entity.  The attempts at artificial intelligence over the past 50 years have used the insights of cognitive science to help devise artificial means of reasoning and perception.  At the same time, the models created by computer scientists (such as the neural network and Marvin Minsky’s idea of “multiple intelligent agents”) have in turn been applied to the study of human cognition ,",0
501,"With the exception of a few experimental systems, color graphics first became widely available only with the beginnings of desktop computers in the late 1970s.  The first microcomputers were able to display only a few colors (some, indeed, displayed only monochrome or grayscale).  Today’s PC video hardware has the potential to display millions of colors, though of course the human eye cannot directly distinguish colors that are too close together.  There are several important schemes that are used to define a “color space”—that is, a range of values that can be associated with physical colors. ",0
502,"One of the simplest color systems displays colors as varying intensities of red, green, and blue.  This corresponds to the electronics of a standard color computer monitor, which uses three electron guns that bombard red, green, and blue phosphors on the screen.  A typical RGB color scheme uses 8 bits to store each of the red, green, and blue components for each pixel, for a total of 24 bits (16,777,216 colors).  The 32-bit color system provides the same number of colors but includes 8 bits for alpha, or the level of transparency.  The number of bits per pixel is also called the bit depth or color depth",0
503,"CMYK stands for cyan, magenta, yellow, and black.  This four component color system is standard for most types of color printing, since black is an ink color in printing but is simply the absence of color in video.  One of the more difficult tasks to be performed by desktop publishing software is to properly match a given RGB screen color to the corresponding CMYK print color.  Recent versions of Microsoft Windows and the Macintosh operating system include a CMS (color matching system) to support color matching. ",0
504,"Although most color schemes now support thousands or millions of colors, it would be wasteful and inefficient to use three or four bytes to store the color of each pixel in memory.  After all, any given application is likely to need only a few dozen colors.  The solution is to set up a palette,which is a table of (usually 256) color values currently in use by the program.  (A palette is also sometimes called a CLUT, or color lookup table. ) The color of each pixel can then be stored as an index to the corresponding value in the palette",0
505,"Portability is the ability to adapt software or hardware to a wide variety of platforms (that is, computer systems or operating systems).  Developers want their products to be portable so they can adapt to an often rapidly changing marketplace.  A typical strategy for portability is to choose a language that is in widespread use and a compiler that is certified as meeting the ANSI or other standard for the language.  The program should be written in such a way that it makes as few assumptions as possible about hardwaredependent matters such as how data is stored in memory.  It is also sometimes possible to use standard frameworks that provide the same functions in several different operating systems such as Windows, Macintosh, and UNIX. ",0
506,"A compiler is a program that takes as input a program written in a source language and produces as output an equivalent program written in another (target) language.  Usually the input program is in a high-level language such as C++ and the output is in assembly language for the target machine ,. Compilers are useful because programming directly in low-level machine instructions (as had to be done with the first computers) is tedious and prone to errors.  Use of assembly language helps somewhat by allowing substitution of symbols (variable names) for memory locations and the use of mnemonic names for operations (such as “add for addition, rather than some binary instruction code).  An assembler is essentially a compiler that needs to make only relatively simple translations, because assembly language is still at a relatively low level. ",0
507,"Compilers are traditionally thought of as having a “front end” that analyzes the source code (high-level language statements) and a “back end” that generates the appropriate low-level code.  The front end processing begins with lexical analysis.  The compiler scans the source program looking for matches to valid tokens as defined by the language.  A token is any word or symbol that has meaning in the language, such as a keyword (reserved word) such as if or while.  Next, the tokens are parsed or grouped according to the rules of the language.  The result of parsing is a “parse tree” that resolves statements into their component parts.  For example, an assignment statement may be parsed into an identifier, an assignment operator (such as =), and a value to be assigned.  The value in turn may be an arithmetic expression that consists of operators and operands. ",0
508,"The process of code generation usually involves multiple passes that gradually substitute machine-specific code and data for the information in the parse tree.  An important consideration in modern compilers is optimization, which is the process of substituting equivalent (but more efficient) constructs for the original output of the front end.  For example, an optimizer can replace an arithmetic expression with its value so that it need not be repeatedly calculated while the program is running.  It can also “hoist out” an invariant expression from a loop so that it is calculated only once before the loop begins.  On a larger scale, optimization can also improve the communication between different parts (procedures) of the program. The compiler must attempt to “prove” that the change it is making in the program will never cause the program to operate incorrectly.  It can do this, for example, by tracing the possible paths of execution through the program (such as through branching and loops) and verifying that each possible path yields the correct result.  A compiler that is too “aggressive” in making assumptions can produce subtle program errors.  (Many compilers allow the user to control the level of optimization, and whether to optimize for speed or for compactness of program size. ) During development, a compiler is often set to include special debugging code in the output.  This code preserves potentially important information that can help the debugging facility better identify program bugs.  After the program is working correctly, it will be recompiled without the debugging code",0
509,"NLP, or Natural Language Processing, is a subfield of computer science and artificial intelligence that focuses on the interactions between computers and human language.  Specifically, NLP aims to develop algorithms and techniques that enable computers to analyze, understand, and generate natural language. The goal of NLP is to enable computers to perform a range of language-related tasks, such as text translation, sentiment analysis, speech recognition, chatbot development, and text summarization, among others.  NLP involves the use of techniques from linguistics, computer science, and statistics, among other fields, to enable machines to process and analyze large volumes of human language data. NLP is a rapidly evolving field that has seen significant advancements in recent years, thanks in large part to advances in machine learning and artificial intelligence.  As a result, NLP has become increasingly important in a wide range of industries, including healthcare, finance, e-commerce, and marketing, among others. ",1
510,"There are many applications of NLP across various industries and fields.  Some of the most common applications of NLP include:Text analysis and classification: NLP can be used to classify and analyze large volumes of text data, such as emails, social media posts, and customer reviews. Sentiment analysis: NLP can be used to analyze the sentiment of text data, such as whether a customer review is positive or negative. Speech recognition: NLP can be used to transcribe spoken language into text, which can be used for a variety of purposes, including voice assistants and customer service chatbots. Machine translation: NLP can be used to translate text from one language to another, enabling communication across language barriers. Chatbots and virtual assistants: NLP can be used to develop chatbots and virtual assistants that can communicate with users in natural language, answering questions and providing assistance. Content generation: NLP can be used to generate text content, such as news articles or product descriptions, based on input data. Text summarization: NLP can be used to summarize large volumes of text data, such as news articles or research papers, into shorter summaries. Named entity recognition: NLP can be used to identify and extract information about named entities, such as people, places, and organizations, from text data. These are just a few examples of the many applications of NLP, which has a wide range of potential uses across numerous industries and fields. ",1
511,"Netiquette is a term used to describe the proper etiquette and behavior expected from individuals when communicating and interacting with others online.  It involves adhering to certain rules and guidelines to ensure that online communication is respectful, courteous, and effective. Some examples of netiquette include using appropriate language and avoiding offensive or discriminatory language, avoiding all caps and excessive use of exclamation marks (as it can come across as shouting), being mindful of other people's time when sending emails or messages, respecting others' privacy and not sharing their personal information without their consent, and refraining from spamming or sending unsolicited messages. Netiquette is important as it helps to create a positive and respectful online environment, and ensures that people are able to communicate effectively and productively with each other online.  By following netiquette guidelines, individuals can also avoid misunderstandings, conflicts, and potentially harmful or offensive behavior. ",1
512,"Chuq von Rospach is a well-known author and speaker on the topic of netiquette (network etiquette).  In his writing, he has suggested a number of guidelines for good online behavior.  Here are some of his suggestions:Respect others: This means treating other people with respect and avoiding insults, flaming, or other forms of aggressive or hurtful behavior. Be concise: When posting messages online, try to be brief and to the point.  Avoid rambling or going off-topic. Stay on topic: When participating in online discussions, try to keep your contributions relevant to the topic at hand. Use appropriate language: Avoid using profanity or other offensive language, and be mindful of the fact that people from different backgrounds and cultures may be reading your posts. Be aware of privacy issues: Respect people's privacy by not sharing personal information about them without their consent. Use good manners: Remember to say ""please"" and ""thank you"" when appropriate, and avoid interrupting others. Be forgiving: People sometimes make mistakes online.  Try to be forgiving and give others the benefit of the doubt. Overall, Chuq von Rospach's suggestions for netiquette are designed to promote respectful, constructive online communication.  By following these guidelines, we can create a more positive and productive online community. ",1
513,"Advocates of net neutrality argue that all internet traffic should be treated equally, without discrimination or preference given to any particular website or online service.  Specifically, they argue that internet service providers (ISPs) should not be allowed to block, slow down, or otherwise manipulate internet traffic based on the content or source of that traffic. Here are some of the key arguments made by advocates of net neutrality:Protecting free speech: Net neutrality ensures that all voices on the internet have an equal opportunity to be heard.  If ISPs are allowed to block or slow down certain websites or services, this could restrict the ability of some individuals or groups to communicate online. Promoting innovation: Without net neutrality, ISPs could potentially favor their own products or services over those of their competitors.  This could stifle innovation and limit consumer choice. Preventing discrimination: Net neutrality ensures that all internet traffic is treated equally, regardless of the content, source, or destination of that traffic.  This prevents ISPs from engaging in discriminatory practices that could harm certain groups or individuals. Supporting small businesses: Net neutrality ensures that small businesses have an equal opportunity to reach customers online.  Without net neutrality, larger companies could potentially pay ISPs for faster internet access, giving them an unfair advantage over smaller competitors. Overall, advocates of net neutrality argue that this principle is essential for protecting free speech, promoting innovation, preventing discrimination, and supporting a level playing field for all online businesses and individuals. ",1
514,"Newsgroups emerged in the early days of the internet as a way for people to communicate and share information with each other.  The development of newsgroups can be traced back to the 1970s, when the first email systems were created.  At that time, email was used primarily by academics and researchers, who used it to share information and collaborate on projects. In the late 1970s, a new communication protocol called Usenet was developed, which allowed people to post and read messages in public discussion forums called newsgroups.  Usenet was decentralized, meaning that anyone with an internet connection could participate in discussions and post messages in newsgroups on a wide range of topics. As the internet grew in popularity in the 1990s, so did the number of people using newsgroups.  At their peak, there were tens of thousands of newsgroups covering a wide range of topics, from politics and religion to sports and entertainment.  Some newsgroups were moderated by volunteers who ensured that discussions remained civil and on-topic, while others were unmoderated and often became heated and contentious. In the late 1990s and early 2000s, the popularity of newsgroups began to wane as other forms of online communication, such as social media and instant messaging, emerged.  However, newsgroups continue to exist today, albeit with much smaller user bases than in their heyday.  Many newsgroups have migrated to web forums or email discussion lists, but the basic idea of public discussion forums where people can share information and ideas lives on. ",1
515,"Netnews, also known as Usenet, is a distributed online discussion system that allows users to read and post messages in public discussion forums called newsgroups.  Each newsgroup is dedicated to a specific topic, and users can browse, search, and participate in discussions related to that topic.  Here are the key features and structures of netnews and newsgroups:Hierarchy: Newsgroups are organized into a hierarchical structure that starts with major categories such as ""comp"" for computer-related discussions, ""sci"" for science-related discussions, ""soc"" for social discussions, and ""rec"" for recreational discussions.  Each major category is further divided into subcategories that reflect more specific topics. Message posting: Users can post messages to newsgroups by composing a message in their newsreader software and submitting it to a news server.  The message is then distributed to all servers that subscribe to that particular newsgroup. Message threading: Netnews uses a threading system that groups related messages together in a tree-like structure.  Users can easily follow the conversation by viewing messages in the order they were posted and seeing how they relate to each other. Subscription: Users can subscribe to newsgroups that interest them, and their newsreader software will download all new messages in those newsgroups automatically. Moderation: Some newsgroups are moderated by volunteers who review messages before they are posted to ensure they are relevant and appropriate for the newsgroup's topic.  Moderators can also remove messages that violate the group's rules. Archives: Netnews messages are archived on news servers, and users can search through archives to find old messages or topics of interest. Distribution: Netnews messages are distributed across a network of news servers, which synchronize with each other to ensure that all messages are available to all users. In summary, netnews and newsgroups offer a decentralized, distributed system for online discussion and information sharing, with a hierarchical structure, message threading, subscriptions, and archives.  While netnews and newsgroups have been largely superseded by other online communication platforms, they remain an important part of the early history of the internet and online communities. ",1
516,"Netnews, or Usenet, is a distributed system for exchanging news and information in the form of messages posted to newsgroups.  The system is decentralized, meaning that there is no central server or authority that controls it.  Instead, messages are distributed across a network of interconnected servers, which synchronize with each other to ensure that all messages are available to all users.  Here's how the distribution and reading of netnews works:Posting a message: Users can post messages to a newsgroup by composing a message in their newsreader software and submitting it to their local news server.  The message is then propagated to other servers that subscribe to the newsgroup, until it reaches all servers in the network. Replication and synchronization: Each news server stores a copy of all messages it receives, and periodically synchronizes with other servers to exchange new messages and updates.  This ensures that all servers have the same set of messages, and that users can access the latest messages from any server in the network. Reading messages: Users can browse newsgroups and read messages using their newsreader software.  The software retrieves messages from their local news server, and can also retrieve messages from other servers in the network if the local server does not have a copy of a particular message. Message threading: Netnews uses a threading system to group related messages together in a tree-like structure, based on their subject and message ID.  This makes it easy for users to follow a conversation or track the evolution of a discussion over time. Archiving: Messages are typically archived on news servers, and can be accessed by users even after they have been removed from their local server or the server they were originally posted to.  This allows users to search for and access older messages and discussions. Overall, the distribution and reading of netnews is based on a decentralized, peer-to-peer system that relies on the synchronization and replication of messages across a network of servers.  This allows users to access a wide range of topics and discussions from anywhere in the network, while also ensuring that messages are distributed and stored redundantly to prevent loss or corruption. ",1
517,"The idea of networks has been around for centuries, as people have always sought ways to connect with each other over long distances.  However, the modern computer network as we know it today began to take shape in the mid-20th century. In the 1950s, computers were large, expensive, and rare machines that were primarily used by governments, large corporations, and academic institutions.  These computers were often housed in large rooms or buildings and were operated by trained technicians. One of the earliest computer networks was the SAGE (Semi-Automatic Ground Environment) system, which was developed in the late 1950s by the United States Air Force.  The SAGE system was designed to provide early warning of potential air attacks by the Soviet Union.  It used a network of interconnected computers to track incoming enemy aircraft and coordinate defensive responses. In the 1960s, the US Department of Defense developed the ARPANET, which was the first wide-area network to use packet switching.  The ARPANET was initially designed as a way for researchers at different universities and research institutions to share computing resources and collaborate on research projects. Over the years, computer networks evolved and became more sophisticated, eventually leading to the creation of the World Wide Web in the 1990s.  Today, networks are an essential part of our daily lives, connecting people and devices all over the world and enabling us to access information and services instantly from almost anywhere. ",1
518,"The OSI (Open Systems Interconnection) model is a conceptual model that describes the communication process between computing systems.  It is composed of seven layers, each of which defines a specific function in the communication process.  The layers are:Physical layer: This layer is responsible for transmitting raw data between devices using physical connections like cables, wires, or radio signals.  It defines the electrical and physical specifications for the connection. Data link layer: This layer is responsible for transmitting data between devices on the same physical network.  It defines how data is organized into frames for transmission and how errors are detected and corrected. Network layer: This layer is responsible for routing data between different networks.  It determines the optimal path for data to travel and uses protocols like IP (Internet Protocol) to transmit data between networks. Transport layer: This layer provides reliable, end-to-end communication between devices.  It breaks down data into smaller packets for transmission and ensures that they are received correctly at the other end. Session layer: This layer establishes, manages, and terminates connections between devices.  It allows applications on different devices to establish a communication session and manage the exchange of data. Presentation layer: This layer is responsible for data representation and encryption.  It transforms data into a format that can be understood by the application layer and encrypts it for secure transmission. Application layer: This layer provides services to end-users, such as email, file transfer, and web browsing.  It defines the protocols and formats used by applications to communicate with each other. The OSI model is a reference model and does not specify any particular protocol or technology.  However, it provides a common framework that helps network designers and developers to understand the various functions and interactions between different layers of the network. ",1
519,"NAS stands for Network-Attached Storage.  It refers to a storage device that is connected to a network and allows multiple users and devices to access and share files and data over that network.  NAS devices typically have their own operating systems and can provide a wide range of features such as file sharing, backup and recovery, media streaming, and remote access. NAS devices can be used in both home and business settings to centralize and manage data storage across multiple devices and users.  They are often used as a more cost-effective and flexible alternative to traditional file servers, which require dedicated hardware and IT support.  NAS devices can range in size from small, single-drive units to large enterprise-level systems with multiple drives and advanced features. ",1
520,"A Storage Area Network (SAN) is a dedicated high-speed network that provides block-level access to data storage.  In a SAN, storage devices such as disk arrays, tape libraries, and other storage appliances are connected to servers through a series of switches and/or hubs. SANs are designed to provide high-speed, reliable access to large amounts of data, and are often used in enterprise environments where large amounts of data need to be stored and accessed by multiple servers and applications.  SANs provide centralized storage management, which allows administrators to allocate storage resources to servers and applications as needed. SANs typically use Fibre Channel or iSCSI protocols to connect storage devices to the network.  Fibre Channel is a high-speed, dedicated network technology that provides very low latency and high reliability.  iSCSI, on the other hand, is an IP-based protocol that allows storage devices to be connected to the network using standard Ethernet connections. SANs can provide a number of benefits, including increased performance, scalability, and availability of data.  They are often used in data centers and other large-scale IT environments to provide high-performance storage for critical applications and data. ",1
521,"Cyberpunk is a subgenre of science fiction that typically focuses on a future dystopian society dominated by advanced technology and corporate control.  The term was first used in the 1980s to describe a type of science fiction that features a gritty, urban setting, high-tech weaponry, and a distinct anti-authoritarian attitude. The central themes of cyberpunk often include a focus on the dangers of technology and the impact of artificial intelligence, virtual reality, and cybernetic enhancements on human society.  It often explores themes of social decay, economic inequality, and the struggle of individuals to find their place in a complex and oppressive world. Some well-known examples of cyberpunk include William Gibson's Neuromancer, Bruce Sterling's Schismatrix, and the movies Blade Runner and The Matrix.  Cyberpunk has also influenced other media, such as video games, music, and fashion. ",1
522,"Neuroprosthetics is a field of study that involves developing devices that can replace or restore the function of the nervous system.  These devices are designed to interface directly with the brain, spinal cord, or peripheral nerves and can be used to restore movement, sensation, or other functions that may have been lost due to injury or disease. At Duke University, researchers have been working on developing advanced neuroprosthetics that can help people with spinal cord injuries regain movement and mobility.  One of the key areas of focus has been on developing brain-machine interfaces (BMIs) that can translate brain activity into movement. In a recent study published in the journal Nature Medicine, researchers at Duke University demonstrated that a monkey with a spinal cord injury was able to regain the ability to walk with the help of a brain-controlled robotic exoskeleton.  The monkey had electrodes implanted in its brain that were able to decode signals related to movement and translate them into commands that controlled the robotic exoskeleton. The researchers also used a wireless connection to send feedback from the exoskeleton back to the monkey's brain, which allowed it to adjust its movements in real-time.  This approach has the potential to enable people with spinal cord injuries to regain movement and independence. Overall, the research carried out at Duke University and other institutions in the field of neuroprosthetics is aimed at developing new and innovative approaches to restoring function and improving the quality of life for people with neurological disorders or injuries. ",1
523,"Brain implants are electronic devices that are designed to interface with the brain and can be used to treat a variety of neurological disorders, including Parkinson's disease, epilepsy, and chronic pain.  In the future, it is likely that brain implants will become even more advanced and sophisticated, with new features and capabilities that will enhance their effectiveness and utility. Here are some potential future developments in brain implants:Improved Precision: One of the key challenges of brain implants is ensuring that they are precisely targeted to the specific areas of the brain that need to be treated.  In the future, it is likely that brain implants will become even more precise, with the ability to target specific cells or neural circuits. Wireless Connectivity: Many current brain implants require wires to connect to external devices.  In the future, it is possible that brain implants will be able to communicate wirelessly, which could make them less invasive and easier to use. Better Energy Efficiency: Many current brain implants require regular battery replacements, which can be invasive and inconvenient.  In the future, it is likely that brain implants will become more energy-efficient, potentially using novel power sources like body heat or motion. Enhanced Sensing: Current brain implants can detect neural activity, but in the future, it is possible that they will be able to sense additional signals like blood flow, temperature, or chemical changes in the brain.  This could provide more detailed information about brain function and enable more precise treatment. Integration with AI: As artificial intelligence (AI) technologies become more advanced, it is likely that brain implants will be able to integrate with these systems.  This could allow for real-time analysis of brain activity and enable more precise and effective treatment. Overall, the future of brain implants is likely to be characterized by continued innovation and development, with new technologies and capabilities that have the potential to transform the treatment of neurological disorders. ",1
524,"Integers are typically stored in memory as binary digits, or ""bits,"" which are either a 0 or a 1.  The size of the integer (i. e. , the number of bits used to represent it) can vary depending on the programming language and the computer architecture being used. In most modern computers, integers are typically stored using a fixed number of bits, such as 8 bits, 16 bits, 32 bits, or 64 bits.  The number of bits used to represent an integer determines its range of possible values.  For example, an 8-bit integer can represent values between 0 and 255, while a 32-bit integer can represent values between -2,147,483,648 and 2,147,483,647. When an integer is stored in memory, it is typically stored as a binary representation of the integer's value using a fixed number of bits.  The most significant bit (MSB) represents the sign of the integer (positive or negative), while the remaining bits represent the magnitude of the integer. For example, consider the 8-bit integer -23.  In binary, this value is represented as 11101001.  The leftmost bit (1) represents the sign (negative), while the remaining 7 bits (1101001) represent the magnitude of the integer (23).  When this value is stored in memory, the computer will reserve 8 bits of memory and store the binary value 11101001 at the corresponding memory address. ",1
525,"Floating point numbers are typically stored in memory using a standardized format, such as the IEEE 754 standard.  This format uses a combination of a sign bit, an exponent, and a mantissa to represent the value of the floating point number. The sign bit indicates whether the value is positive or negative.  It is represented using a single bit, with 0 indicating a positive value and 1 indicating a negative value. The exponent represents the power of 2 by which the mantissa should be multiplied.  It is represented using a fixed number of bits, which depends on the specific floating point format being used.  For example, in single-precision floating point format (which uses 32 bits), the exponent is represented using 8 bits. The mantissa represents the significant digits of the floating point number.  It is also represented using a fixed number of bits, which depends on the specific floating point format being used.  In single-precision floating point format, the mantissa is represented using 23 bits. To store a floating point number in memory, the computer first converts the number to the standardized floating point format, using the appropriate number of bits for the sign, exponent, and mantissa.  The resulting binary representation is then stored in memory at a specific memory address. When a program needs to use the floating point number, it reads the binary representation from memory, converts it back to the original floating point value, and performs the necessary calculations. ",1
526,"In object-oriented programming, inheritance is a mechanism that allows a new class to be based on an existing class, inheriting its properties and methods.  To illustrate this concept with an example, we can use the Microsoft Windows Dialog Box. A dialog box is a graphical user interface element that displays information to the user and prompts them for a response.  In Microsoft Windows, a dialog box is implemented as a class, which we can call the DialogBox class. Now, let's suppose we want to create a new type of dialog box called a ColorPickerDialogBox that allows the user to pick a color.  We can use inheritance to create this new class based on the DialogBox class. The ColorPickerDialogBox class will inherit all the properties and methods of the DialogBox class, such as the ability to display text, buttons, and input fields.  However, the ColorPickerDialogBox class will also have its own unique properties and methods that are specific to picking colors, such as a color palette and color preview box. By using inheritance, we can avoid duplicating code and make our program more efficient and easier to maintain.  We can also create more specialized classes that build upon existing classes, making our code more modular and extensible. ",1
527,"Polymorphism is a concept in object-oriented programming that allows different objects to be treated as if they were the same type.  It allows code to be more flexible and reusable by allowing different objects to be used interchangeably. To illustrate this concept with an example, let's consider a program that deals with different shapes, such as circles, squares, and triangles.  Each shape has its own unique properties and methods, such as its size, color, and the ability to calculate its perimeter. Now, let's suppose we want to create a method that takes a shape object as input and calculates its perimeter.  We can use polymorphism to create a single method that works for all types of shapes, rather than creating separate methods for each shape. ",1
528,"Online advertising, also known as internet advertising, is a form of digital marketing that uses the internet to deliver promotional marketing messages to consumers.  It encompasses a variety of advertising methods, including display ads, social media advertising, search engine marketing, email marketing, and mobile advertising. Online advertising allows businesses to reach a large audience of potential customers across the internet, regardless of their location or device.  It can be targeted to specific demographics, interests, and behaviors, making it a more effective and efficient way to reach a specific audience compared to traditional advertising methods. There are different pricing models for online advertising, including cost per click (CPC), cost per thousand impressions (CPM), and cost per acquisition (CPA).  Advertisers can track and measure the performance of their ads using metrics such as click-through rates, conversion rates, and return on investment (ROI). Online advertising has become a major part of the digital marketing landscape, with businesses of all sizes using it to promote their products and services to a global audience.  It is a constantly evolving industry, with new technologies and strategies emerging to help advertisers better target and engage with their audience. ",1
529,"There are several types of online ads that businesses and advertisers can use to reach their target audience.  Here are some of the most common types:Display ads: These are visual ads that appear on websites, often in the form of banners, images, or videos.  They can be targeted to specific audiences based on demographics, interests, and behaviors. Search engine ads: These are text ads that appear at the top or bottom of search engine results pages, based on the keywords that users search for.  They can be targeted to specific geographic regions and are priced on a pay-per-click (PPC) basis. Social media ads: These are ads that appear on social media platforms, such as Facebook, Twitter, and Instagram.  They can be targeted based on demographics, interests, and behaviors, and can include images, videos, and interactive elements. Video ads: These are ads that appear before, during, or after online video content, such as YouTube videos.  They can be targeted based on demographics and interests, and can include pre-roll, mid-roll, or post-roll ads. Native ads: These are ads that blend in with the content of a website or app, making them less disruptive to the user experience.  They can be targeted based on the context of the content, and can include articles, videos, or sponsored posts. Email ads: These are ads that appear in email newsletters or promotional emails.  They can be targeted based on the recipient's interests and behaviors, and can include images, text, or call-to-action buttons. In-app ads: These are ads that appear within mobile apps, often in the form of banners, interstitial ads, or rewarded ads.  They can be targeted based on the app's content or the user's behavior, and can include images, videos, or interactive elements. Each type of online ad has its own strengths and weaknesses, and choosing the right type of ad depends on the goals of the campaign and the target audience. ",1
530,"Affiliate marketing is a type of online marketing in which a business or advertiser pays a commission to a third-party affiliate for promoting and driving traffic to their products or services.  It is a performance-based marketing strategy that involves four main parties: the advertiser, the affiliate, the customer, and the network. In affiliate marketing, the advertiser provides the affiliate with promotional materials, such as banner ads or product links, to use in their marketing efforts.  The affiliate then promotes the advertiser's products or services to their audience, such as on their website or social media channels.  When a customer clicks on the affiliate's promotional link and makes a purchase, the affiliate earns a commission from the advertiser. Affiliate marketing can be beneficial for all parties involved.  Advertisers can reach new customers and increase their sales without having to invest in traditional advertising methods.  Affiliates can earn money by promoting products or services that align with their brand and audience.  Customers can discover new products or services that they may not have found otherwise. Affiliate marketing can be used in a variety of industries, including e-commerce, travel, finance, and more.  There are different types of affiliate marketing programs, including pay-per-click (PPC), pay-per-lead (PPL), and pay-per-sale (PPS).  It is important for both advertisers and affiliates to choose the right program and set clear expectations for commission rates, performance metrics, and marketing guidelines. Overall, affiliate marketing is a popular and effective way for businesses to reach new customers and increase their sales, while providing affiliates with a way to monetize their online presence and promote products or services they believe in. ",1
531,"Maintaining user interest in online advertising is essential for the success of any online advertising campaign.  Here are some tips on how to keep users engaged and interested in your online ads:Keep it relevant: Make sure your ads are relevant to the user's interests and needs.  Use targeting options to show ads to users who are likely to be interested in what you're offering. Keep it simple: Use clear and concise messaging that is easy to understand.  Avoid using too much text or complicated language. Use eye-catching visuals: Use high-quality images or videos that grab the user's attention and stand out from other ads on the page. Provide value: Offer something of value to the user, such as a discount, free trial, or informative content.  This will make the user more likely to engage with your ad. Personalize the experience: Use personalization techniques to make the ad feel more personalized and relevant to the user.  This can include using the user's name or showing ads based on their previous browsing history. Use interactive elements: Use interactive elements, such as quizzes, polls, or games, to engage the user and encourage them to interact with your ad. Test and optimize: Continuously test and optimize your ads to improve their performance.  This can include testing different ad formats, messaging, and targeting options to see what works best for your audience. By following these tips, you can maintain user interest in your online ads and increase the likelihood of users engaging with your brand and taking action, such as making a purchase or filling out a form. ",1
532,"Email fraud, also known as email scams, is a type of online fraud that uses email as the primary means of communication.  Here are some examples of email frauds:Phishing: This is a type of email fraud in which the sender pretends to be a trustworthy entity, such as a bank or a popular website, to trick the recipient into providing personal information or login credentials.  The email may include a link to a fake website that looks similar to the legitimate one, where the recipient is asked to enter their information. Business email compromise (BEC): This is a type of email fraud in which the sender poses as a high-level executive or business partner of the recipient's company and requests a transfer of funds or sensitive information.  The email may appear to be legitimate and may include the company's logo and other details. Lottery scam: This is a type of email fraud in which the sender claims that the recipient has won a large sum of money in a lottery or a sweepstakes.  The email may ask the recipient to provide personal information or pay a processing fee in order to claim the prize. Nigerian scam: This is a type of email fraud in which the sender claims to be a wealthy individual or a government official from Nigeria who needs the recipient's help in transferring large sums of money out of the country.  The email may promise a large reward for the recipient's help. Charity scam: This is a type of email fraud in which the sender claims to be a representative of a charitable organization and asks the recipient to donate money.  The email may include a link to a fake website that looks similar to the legitimate charity's website. It's important to be cautious when receiving emails from unknown senders or emails that seem suspicious.  It's always a good idea to verify the legitimacy of the email by contacting the sender directly or by checking with the company or organization they claim to represent. ",1
533,"Online casinos are websites or mobile apps that offer a variety of casino games, such as slots, blackjack, roulette, and poker, that can be played online using a computer, smartphone, or tablet.  Online casinos allow players to place real-money bets and potentially win real money prizes, similar to traditional brick-and-mortar casinos. Online casinos typically offer a wide variety of games, ranging from classic table games to modern video slots, and often feature attractive graphics and sound effects to enhance the gaming experience.  Many online casinos also offer bonuses and promotions to attract new players and retain existing ones, such as welcome bonuses, free spins, and loyalty rewards programs. To play at an online casino, players must register for an account and deposit money into their account using a variety of payment methods, such as credit cards, e-wallets, or bank transfers.  Once the account is funded, players can choose from a variety of games and place real-money bets. It's important to note that online gambling laws and regulations vary by country and jurisdiction, and players should ensure that online gambling is legal in their area before playing at an online casino.  Additionally, players should always gamble responsibly and set limits on their spending to avoid potential gambling addiction and financial problems. ",1
534,"Online poker is a digital version of the popular card game, poker, which can be played over the internet on a computer or mobile device.  Online poker follows the same rules and gameplay as traditional poker, but is played using virtual chips or real money. Players can choose from a variety of poker games, such as Texas Hold'em, Omaha, Seven Card Stud, and more, and can compete against other players from around the world.  Online poker sites typically offer a variety of tournaments and cash games with varying buy-ins and prize pools. Online poker can be played using a software client that is downloaded to a computer, or through a web-based platform that can be accessed using a web browser.  Players must register for an account on an online poker site, and deposit funds into their account using a variety of payment methods, such as credit cards, e-wallets, or bank transfers. Online poker has become increasingly popular over the years, due to its convenience, variety of games, and potential to win real money.  However, it's important to note that online gambling laws and regulations vary by country and jurisdiction, and players should ensure that online gambling is legal in their area before playing online poker.  Additionally, players should always gamble responsibly and set limits on their spending to avoid potential gambling addiction and financial problems. ",1
535,"Online games have come a long way since their inception in the late 1970s and early 1980s.  Here is a brief overview of the evolution of online games:Early online games (1970s-1980s): The first online games were text-based and played on university mainframe computers.  These early games, such as Adventure and Dungeon, were played by typing commands and reading text descriptions. Multiplayer games (1990s): In the 1990s, the first online multiplayer games emerged.  Games such as Doom, Quake, and Ultima Online allowed players to connect to servers and play with other players from around the world. Massively multiplayer online games (MMOs) (late 1990s-early 2000s): The late 1990s saw the emergence of massively multiplayer online games, such as EverQuest and World of Warcraft.  These games allowed thousands of players to play in the same virtual world, complete quests, and battle monsters together. Social gaming (mid-2000s): In the mid-2000s, social networking sites such as MySpace and Facebook popularized casual social games such as FarmVille and Mafia Wars.  These games were designed to be played with friends and were often free-to-play with in-app purchases. Mobile games (late 2000s-2010s): With the rise of smartphones and tablets, mobile gaming became increasingly popular.  Games such as Angry Birds, Candy Crush, and Pokémon Go were designed to be played on mobile devices and were often free-to-play with in-app purchases. Esports (2010s): In the 2010s, esports emerged as a popular competitive gaming industry.  Games such as League of Legends and Overwatch became popular esports titles, with professional teams and players competing in tournaments for large prize pools. Overall, the evolution of online games has been driven by advancements in technology and changes in consumer behavior.  Today, online games continue to evolve with the advent of new technologies such as virtual reality and the growing popularity of mobile gaming",1
536,"MMORPGs (Massively Multiplayer Online Role-Playing Games) are a type of online game that allows players to immerse themselves in a virtual world where they can interact with thousands of other players from around the world.  In MMORPGs, players take on the role of a character in a persistent, evolving game world that continues to exist even when they log out. MMORPGs typically feature a wide variety of gameplay elements, including combat, exploration, crafting, and social interaction.  Players can form guilds and alliances with other players, participate in player-versus-player battles, and undertake quests and missions to advance their characters' abilities and gain experience points. Popular MMORPGs include World of Warcraft, Final Fantasy XIV, Guild Wars 2, and Elder Scrolls Online.  MMORPGs often require a subscription fee or the purchase of in-game currency or items, although there are also free-to-play MMORPGs available. The success of MMORPGs can be attributed to the social and immersive experience they provide, allowing players to forge relationships with other players from around the world and explore vast, detailed virtual worlds.  However, it's important to note that MMORPGs can be time-consuming and addictive, and players should be careful to balance their gameplay with other aspects of their lives. ",1
537,"Online brokers are financial companies that provide individuals with an online platform to buy and sell various financial securities, such as stocks, bonds, mutual funds, exchange-traded funds (ETFs), and other investment products.  Online brokers allow investors to conduct their transactions over the internet without the need for a physical broker or financial advisor. These brokers typically offer a variety of investment products, tools, and resources to help investors make informed decisions about their investments.  They may also offer educational resources, research reports, and customer support to help their clients make the most of their investments. Some online brokers charge a fee for their services, while others operate on a commission-based model, earning a fee for every transaction made on their platform.  Many online brokers also offer mobile apps, allowing investors to manage their investments on the go. ",1
538,"There are several features that you should consider when evaluating job sites.  These features can help you determine whether a job site is reputable, user-friendly, and effective in helping you find the job opportunities that best match your skills and experience.  Some of the key features to consider include:Job listings: Look for job sites that have a large number of current job listings in your field or industry. Search filters: Make sure the job site has search filters that allow you to narrow down job listings by location, industry, job type, and other criteria. Resume uploading: Check whether the job site allows you to upload your resume and create a profile to showcase your skills and experience. Alerts and notifications: Look for job sites that offer email alerts or push notifications to let you know when new job opportunities are posted. Company research: Some job sites allow you to research companies and read reviews from current and former employees to get a better sense of their work culture and reputation. Networking opportunities: Consider job sites that offer networking opportunities, such as online forums or discussion boards where you can connect with other job seekers and industry professionals. User interface: Look for job sites that have a user-friendly interface that is easy to navigate and search. Mobile compatibility: Check whether the job site has a mobile app or is optimized for mobile browsing, as this can be helpful if you need to search for jobs on the go. Cost: Some job sites may charge a fee for access to certain features, so make sure you understand the cost structure before you sign up. By considering these features, you can evaluate job sites and choose the ones that are most likely to help you find the job opportunities that are right for you. ",1
539,"An ontology is a formal, explicit description of concepts and their relationships within a particular domain or area of knowledge.  It defines the types, properties, and interrelationships of the entities that exist in that domain, as well as the rules governing their behavior and interactions. In other words, an ontology is a structured way of representing knowledge and information, and it can be used to support a wide range of applications and systems, such as data integration, knowledge management, semantic search, and artificial intelligence. Ontologies can be developed using various formalisms and languages, such as RDF, OWL, and UML, and they can be designed for different purposes and domains, such as biology, medicine, finance, and engineering. ",1
540,"The implementation of an ontology involves several steps, which may vary depending on the specific ontology language, tools, and application domain.  Here is a general overview of the typical process:Define the ontology scope and requirements: Identify the domain or area of knowledge that the ontology will cover, and specify the purpose and goals of the ontology.  This step involves analyzing existing data sources, documents, and experts' knowledge to determine the relevant concepts, relationships, and properties that need to be represented in the ontology. Design the ontology structure: Decide on the ontology language and modeling approach to use, and create a conceptual model that defines the classes, properties, and relationships in the ontology.  This step involves using ontology editors or modeling tools to define the ontology schema, which may include axioms, constraints, and rules. Implement the ontology schema: Transform the conceptual model into a formal representation in the chosen ontology language, such as OWL or RDF.  This step involves using ontology editors or programming languages to define the ontology's classes, properties, and relationships and to ensure that the ontology conforms to the language syntax and semantics. Populate the ontology with data: Import or generate data that conforms to the ontology schema and annotate the data with ontology terms and concepts.  This step involves using data integration tools or APIs to map the data to the ontology classes and properties and to ensure that the data is consistent and aligned with the ontology's semantics. Validate and evaluate the ontology: Verify the correctness and completeness of the ontology schema and the data, and test the ontology against use cases and scenarios.  This step involves using reasoning engines or validation tools to detect errors, inconsistencies, or ambiguities in the ontology and to ensure that the ontology meets the intended goals and requirements. Deploy and maintain the ontology: Publish the ontology to a repository or server that allows other users or systems to access and use it, and maintain the ontology by updating it with new data or knowledge, refining its schema, or extending its scope. Overall, the implementation of an ontology requires a systematic and iterative approach that involves collaboration between domain experts, ontology designers, and software developers.  It also requires a good understanding of the ontology language and tools, as well as the application domain and data sources. ",1
541,"Open source software refers to computer software that is distributed with its source code publicly available to anyone to view, modify, and redistribute.  The term ""open source"" refers to the fact that the source code is open to the public, rather than being proprietary or closed. Open source software is typically developed collaboratively by a community of developers who contribute code and make improvements to the software over time.  This development process can result in software that is more reliable, secure, and flexible than proprietary software. Open source software can be used for a wide range of applications, from operating systems and programming languages to web servers and databases.  Some examples of popular open source software include the Linux operating system, the Apache web server, the MySQL database, and the Mozilla Firefox web browser. Open source software is often free to use, although there may be costs associated with support, customization, or maintenance.  Additionally, many open source software projects are supported by companies or organizations that offer commercial versions of the software with additional features or support. ",1
542,"An operating system (OS) is a software that manages the computer hardware and software resources and provides common services for computer programs.  The operating system acts as an intermediary between the computer hardware and the software applications that run on the computer. The operating system performs a wide range of tasks, including:Managing computer hardware resources, such as the CPU, memory, and storage devices. Providing a user interface for users to interact with the computer, such as a graphical user interface (GUI) or a command line interface. Managing system security, such as user authentication and access control. Managing system processes, such as starting and stopping applications and managing system resources. Providing device drivers to enable software applications to interact with hardware devices, such as printers and network adapters. Examples of popular operating systems include Microsoft Windows, macOS, Linux, and Android.  Each operating system has its own unique features and capabilities, and different operating systems are used for different types of devices and applications. ",1
543,"Designers of modern operating systems face a range of issues related to performance, security, usability, and compatibility.  Some of the key issues are:Security: One of the biggest challenges for modern operating systems is ensuring security against malware, viruses, and other cyber threats.  Designers need to implement robust security measures to protect against attacks and ensure that user data is safe. Performance: Modern operating systems need to be fast and responsive, with low latency and minimal resource usage.  This requires careful optimization of system resources, such as memory, CPU, and I/O operations. Compatibility: Operating systems need to be compatible with a wide range of hardware and software configurations.  Designers need to ensure that the operating system can work seamlessly with different hardware devices, including printers, scanners, and other peripherals. Usability: Operating systems should be intuitive and easy to use, with a consistent interface that users can navigate easily.  Designers need to ensure that the user experience is positive and that users can easily access the features and functions they need. Updates and maintenance: Operating systems need to be updated regularly to fix bugs and security vulnerabilities, and designers need to ensure that these updates are delivered seamlessly and without disrupting the user experience. Privacy: Operating systems should also protect the user's privacy and personal data.  Designers need to implement privacy settings and features that give users control over their personal data and limit access to sensitive information. Accessibility: Operating systems should also be designed to be accessible to users with disabilities.  Designers need to ensure that users with vision, hearing, or mobility impairments can use the operating system effectively. Overall, designers of modern operating systems face a range of complex challenges that require careful planning, testing, and optimization to deliver a reliable, secure, and user-friendly experience. ",1
544,"Levels of precedence in programming languages determine the order in which operators are evaluated in an expression.  The following are the typical levels of precedence followed by most programming languages, in order from highest to lowest:Parentheses: Any expressions contained within parentheses are evaluated first. Unary Operators: Unary operators, such as negation (-) and logical NOT (!), are evaluated next. Exponentiation: Exponentiation operators, such as the caret (^) in some languages, are evaluated next. Multiplication and Division: Multiplication (*) and division (/) operators are evaluated next, in left-to-right order. Addition and Subtraction: Addition (+) and subtraction (-) operators are evaluated next, in left-to-right order. Relational Operators: Relational operators, such as less than (<) and greater than (>), are evaluated next. Equality Operators: Equality operators, such as equals (==) and not equals (!=), are evaluated next. Logical AND: The logical AND operator (&&) is evaluated next. Logical OR: The logical OR operator (||) is evaluated last. It's important to note that some programming languages may have different levels of precedence or slightly different rules for evaluating expressions.  Therefore, it's essential to consult the documentation of a particular programming language to determine its specific order of precedence for operators. ",1
545,"Optical Character Recognition (OCR) is a technology that is used to convert printed or handwritten text into digital text.  OCR systems are designed to scan an image containing text and use algorithms to recognize and convert the text into a digital format that can be edited, searched, or processed by a computer. OCR technology typically works by analyzing the shapes of characters in an image and comparing them to a database of known character shapes.  The system then uses pattern recognition algorithms to match the characters in the image to the closest match in the database.  Once the characters have been identified, the OCR system converts them into digital text that can be edited or searched. OCR technology is used in a wide range of applications, including document scanning and conversion, automated data entry, and text recognition in images and videos.  OCR is commonly used to convert printed text from books, magazines, and newspapers into digital formats, and is also used in industries such as healthcare, finance, and legal services to digitize and process large volumes of text-based documents. ",1
546,"There are several ways that Optical Character Recognition (OCR) systems can identify characters in an image:Pattern recognition: OCR algorithms compare the shape of characters in an image to a database of known character shapes.  The system then uses pattern recognition algorithms to match the characters in the image to the closest match in the database. Feature extraction: OCR algorithms can also identify characters by extracting features such as lines, curves, and corners in the image.  The system then compares these features to a database of known character features to identify the characters in the image. Neural networks: OCR systems can also use artificial neural networks to identify characters.  These networks are trained using a large dataset of known characters, and then apply this knowledge to identify characters in new images. Edge detection: OCR algorithms can also identify characters by detecting the edges of characters in an image.  The system then uses these edges to identify the shape of the characters and convert them into digital text. Language-specific recognition: OCR systems can also be trained to recognize characters in specific languages, such as English, Chinese, or Arabic.  These systems use language-specific algorithms to identify characters based on the rules and conventions of the language. Overall, OCR systems use a combination of these techniques to identify characters in an image, depending on the complexity of the image and the requirements of the application.  The accuracy of OCR systems can vary depending on factors such as image quality, font type, and language, so it's essential to choose the right OCR system for your specific needs. ",1
547,"OS X (pronounced ""Oh Ess Ten"") is a graphical operating system developed and maintained by Apple Inc.  for its Macintosh line of personal computers.  It is based on the UNIX operating system and provides a user-friendly interface, advanced graphics capabilities, and powerful multimedia tools.  OS X was first released in 2001 and has since undergone several major revisions, with the latest version being macOS Monterey as of September 2021. OS X features a range of built-in applications for common tasks such as web browsing, email, and word processing, as well as support for third-party software.  It also includes features such as Spotlight search, which allows users to quickly find files and folders on their computer, and Time Machine, which provides automatic backups of important data. OS X has gained popularity among both consumers and professionals due to its stability, ease of use, and compatibility with a wide range of software and hardware.  Its robust security features, such as Gatekeeper and FileVault, also make it a popular choice for users concerned about data privacy and security. ",1
548,"macOS (formerly known as OS X) is a Unix-based operating system that comes bundled with a number of pre-installed software packages.  These packages are designed to provide users with a range of essential tools for productivity, creativity, and entertainment.  Some of the software packages offered by macOS include:iWork: a suite of productivity software that includes Pages (a word processor), Numbers (a spreadsheet application), and Keynote (a presentation tool). GarageBand: a digital audio workstation (DAW) that allows users to create and edit music. iMovie: a video editing application that allows users to create and edit videos. Final Cut Pro: a professional video editing application that is widely used in the film and television industry. Logic Pro: a digital audio workstation (DAW) that is widely used by musicians and music producers. Xcode: a software development environment that allows developers to create macOS, iOS, and other Apple platform applications. Safari: a web browser that allows users to browse the internet. Photos: a photo management application that allows users to store, edit, and organize their photos. Time Machine: a backup and recovery software that automatically backs up your Mac data on an external hard drive. Terminal: a command-line interface that allows users to interact with macOS using text commands. These are just some examples of the software packages offered by macOS.  There are many other third-party applications available for macOS as well, which can be downloaded from the Mac App Store or directly from software vendors' websites. ",1
549,"Parallel ports are a type of communication interface that allows data to be transmitted between a computer and a peripheral device.  They were commonly used in older computers to connect printers, scanners, and other external devices. A parallel port typically consists of a 25-pin female connector on the computer, and a matching male connector on the peripheral device.  The data is transmitted in parallel, meaning that multiple bits of data are sent at the same time over separate wires. Parallel ports were popular because they allowed for relatively fast data transfer rates, especially compared to older serial ports.  However, they have largely been replaced by newer and faster interfaces, such as USB and Ethernet. Parallel ports are still used in some specialized applications, such as industrial control systems and scientific equipment, but they are not commonly found on consumer-grade computers anymore. ",1
550,"Pattern recognition is the process of recognizing patterns, regularities, or structures in data, signals, images, sounds, or any other type of information.  It involves the use of algorithms and techniques to identify and classify these patterns based on certain criteria or characteristics. Pattern recognition is a fundamental component of many fields, including computer science, machine learning, artificial intelligence, psychology, and cognitive neuroscience.  Some examples of applications of pattern recognition include speech recognition, image processing, handwriting recognition, facial recognition, and anomaly detection. The goal of pattern recognition is to find meaningful patterns in data and use them to make predictions, decisions, or automate tasks.  It involves both supervised and unsupervised learning methods, and can also be combined with other techniques such as data mining and deep learning to improve accuracy and performance. ",1
551,"Personal digital assistant (PDA) devices originated from the convergence of several technologies, including electronic organizers, mobile phones, and handheld computing devices.  The first PDA devices were introduced in the 1990s, and they quickly gained popularity among professionals and consumers for their ability to manage schedules, contacts, and tasks. One of the earliest PDA devices was the Apple Newton, which was released in 1993.  It featured a touchscreen interface and handwriting recognition, allowing users to write notes and messages directly on the device.  However, the Newton was expensive and had limited functionality, and it was eventually discontinued in 1998. Other early PDA devices included the Palm Pilot, which was released in 1996 and quickly became a popular choice for its simple interface and compact size.  The Palm Pilot featured a stylus-based interface, allowing users to input text and interact with applications on the device. As technology advanced, PDAs began to incorporate more features, such as mobile phone capabilities, wireless connectivity, and multimedia playback.  This led to the development of smartphones, which combined the functionality of a PDA with the ability to make calls and access the internet. Today, smartphones have largely replaced standalone PDA devices, but the basic concepts of PDA technology continue to influence the development of mobile devices and other personal technology. ",1
552,"PDF stands for Portable Document Format, which is a file format used to present and exchange documents reliably, independent of software, hardware, or operating system.  The PDF format was developed by Adobe Systems in the early 1990s, and has since become a widely used file format for documents such as reports, manuals, and forms. Here are some of the key operations of PDF:Creation: PDF documents can be created using various software applications, including Adobe Acrobat, Microsoft Word, and many others.  To create a PDF, the user must select the ""Save as PDF"" or ""Print to PDF"" option, which converts the document into a PDF file. Viewing: PDF documents can be viewed using a variety of software applications, including Adobe Acrobat Reader, which is a free program that allows users to view, print, and annotate PDF files.  Other programs that can view PDFs include web browsers, such as Google Chrome and Mozilla Firefox. Editing: PDF documents can be edited using various software applications, including Adobe Acrobat, which provides a range of editing tools for modifying text, images, and other elements in a PDF document.  Other software programs, such as Microsoft Word and Google Docs, can also convert PDFs into editable formats for further modification. Security: PDF documents can be protected with various security features, such as passwords and encryption, to prevent unauthorized access or modification of the content. Conversion: PDF documents can be converted to other file formats, such as Microsoft Word or HTML, using software applications or online tools.  This allows users to edit or repurpose the content of a PDF document in different ways. Overall, PDF is a versatile file format that allows for reliable presentation and exchange of documents across different platforms and devices. ",1
553,"Perl is a high-level, general-purpose programming language that was originally developed in the late 1980s by Larry Wall.  The name ""Perl"" stands for ""Practical Extraction and Reporting Language,"" which reflects its origins as a tool for text processing and report generation. Perl has a syntax that is designed to be easy to read and write, and it supports a wide range of programming paradigms, including procedural, object-oriented, and functional programming.  It is a popular language for web development, system administration, and other tasks that involve text processing and automation. Some of the key features of Perl include:Regular expressions: Perl has powerful support for regular expressions, which allows for sophisticated text processing and manipulation. Built-in data structures: Perl has built-in support for arrays, hashes, and other data structures, which make it easy to work with complex data. Platform independence: Perl code can be run on a wide range of operating systems, including Unix, Linux, Windows, and macOS. Extensibility: Perl has a large library of modules that can be used to extend its functionality, as well as support for calling code written in other languages such as C. Community support: Perl has a large and active community of developers, who contribute to its development and provide support and resources for users. Overall, Perl is a versatile and powerful programming language that is well-suited for text processing, automation, and other tasks that involve working with complex data.  Its ease of use, flexibility, and community support make it a popular choice for many programmers. ",1
554,"Medical information sites are websites that provide information about various health conditions, diseases, treatments, and medical procedures.  These sites can be useful sources of information for patients, caregivers, and healthcare professionals.  Some examples of popular medical information sites include:WebMD: A comprehensive health information site that provides information on various health topics, including symptoms, treatments, medications, and wellness. Mayo Clinic: A nonprofit medical practice and medical research group that provides information on various health topics, as well as a symptom checker and a directory of healthcare providers. MedlinePlus: A service of the U. S.  National Library of Medicine that provides information on diseases, conditions, medications, and wellness, as well as a directory of healthcare providers. Healthline: A health information site that provides information on various health topics, as well as a symptom checker and a directory of healthcare providers. Centers for Disease Control and Prevention (CDC): A government agency that provides information on diseases, conditions, and public health topics, as well as guidelines for preventing and treating various health conditions. National Institutes of Health (NIH): A government agency that conducts and funds medical research and provides information on various health topics, as well as a directory of clinical trials. It is important to note that while medical information sites can be useful sources of information, it is always important to consult with a healthcare provider before making any medical decisions or changes to your health routine. ",1
555,"The philosophical and spiritual aspects of computing are complex and multifaceted, and they have been the subject of much debate and exploration in recent years.  Here are some key concepts that are relevant to this topic:Ethics of Artificial Intelligence: As artificial intelligence becomes more advanced, questions about the ethics of its development and use have arisen.  For example, some have raised concerns about the potential for AI to reinforce existing biases or to be used for harmful purposes. The Nature of Consciousness: Some philosophers have explored the question of whether computers can truly be conscious.  This raises questions about the nature of consciousness itself and whether it is a product of physical processes or something more elusive. The Impact of Technology on Society: Computing and technology more broadly have had a profound impact on society, changing the way we communicate, work, and interact with each other.  This has raised questions about whether these changes are ultimately positive or negative. The Intersection of Technology and Spirituality: Some have explored the potential for technology to aid in spiritual practices or to facilitate deeper connections with the divine.  Others have raised concerns about the potential for technology to distract us from spiritual practices or to create a sense of disconnection from nature. Overall, the philosophical and spiritual aspects of computing are complex and multifaceted, and they are likely to continue to be the subject of much exploration and debate in the years to come. ",1
556,"Phishing and spoofing are two common types of online scams used by cybercriminals to trick users into divulging sensitive information such as passwords, credit card numbers, and other personal data. Phishing is a type of social engineering attack where the attacker sends a fraudulent message, typically via email or instant messaging, pretending to be a trustworthy entity, such as a bank, government agency, or online service provider.  The message usually contains a link that takes the user to a fake website that looks like the legitimate one.  Once the user enters their login credentials or personal information, the attacker can use that information for various malicious purposes. Spoofing, on the other hand, involves manipulating the sender's address of an email or other communication to make it appear as if it's coming from a legitimate source, such as a known company or organization.  The goal of spoofing is to trick the recipient into thinking that the communication is legitimate and trustworthy.  This can be used to deliver phishing attacks or to spread malware. In summary, phishing is an attack that tricks users into providing sensitive information by impersonating a legitimate entity, while spoofing involves impersonating a trusted source in order to deceive the recipient into thinking that the communication is legitimate. ",1
557,"Phishing is a type of cybercrime where an attacker tries to trick individuals into revealing sensitive information like passwords, credit card numbers, or personal identification numbers.  Here are some ways to prevent phishing:Be cautious of emails and messages: Do not open any attachments or click on any links in emails or messages that appear suspicious.  Always verify the sender's email address and check for any spelling or grammatical errors. Keep your software updated: Always keep your software and operating system up to date with the latest security patches and updates.  This can help protect against known vulnerabilities that attackers can exploit. Use anti-phishing tools: Use anti-phishing tools provided by your web browser or email client, which can help identify and block phishing attempts. Use strong passwords: Use strong, unique passwords for each online account, and avoid using the same password across multiple accounts.  Consider using a password manager to generate and store complex passwords. Educate yourself: Stay informed about the latest phishing techniques and common scams.  Be wary of any messages or emails that ask for personal information. Enable two-factor authentication: Use two-factor authentication for all accounts that support it.  This adds an extra layer of security to your accounts by requiring a second factor, such as a fingerprint or code generated on your phone, to log in. Be skeptical of unsolicited offers: Be skeptical of any unsolicited offers or requests, especially those that seem too good to be true.  Avoid providing personal information unless you are sure that the request is legitimate. By following these steps, you can reduce the risk of falling victim to phishing attacks and protect your sensitive information. ",1
558,"PHP (Hypertext Preprocessor) is a popular open-source scripting language used for web development.  It was first released in 1995 and is primarily used for server-side web development.  PHP scripts are executed on a web server, generating dynamic web pages that can include HTML, CSS, and JavaScript. PHP is used in a variety of applications, including e-commerce websites, content management systems, and social media platforms.  It is well-known for its ease of use, flexibility, and scalability, making it a popular choice for building web applications. Some of the key features of PHP include:Cross-platform compatibility: PHP can run on a variety of platforms, including Windows, Linux, and macOS. Object-oriented programming support: PHP supports object-oriented programming, allowing developers to write modular, reusable code. Database integration: PHP has built-in support for many popular databases, including MySQL, PostgreSQL, and Oracle. Extensive library support: PHP has a vast collection of libraries and frameworks, making it easier for developers to build web applications quickly and efficiently. Easy integration with other technologies: PHP can be easily integrated with other web technologies, such as HTML, CSS, JavaScript, and XML. Overall, PHP is a powerful and versatile language that has played a significant role in the development of the modern web. ",1
559,"PL/I (Programming Language One) is a high-level programming language developed in the 1960s.  It was designed to combine the best features of FORTRAN and COBOL, and to provide a language suitable for both scientific and commercial applications. PL/I was initially developed for the IBM System/360 mainframe computer and was used extensively in business and government applications.  Some of the key features of PL/I include support for structured programming, advanced data types, and file handling. The impact of PL/I can be seen in several areas:Language features: PL/I introduced several innovative language features, including support for arrays, structures, and pointers.  It also included built-in support for string handling and regular expressions, which were not commonly found in other programming languages at the time. Improved productivity: PL/I was designed to be a high-level language that would improve programmer productivity.  Its advanced features and built-in functions allowed programmers to write complex programs with less effort. Widespread adoption: PL/I was widely adopted in business and government applications, particularly in the banking and finance sectors.  It was also used in scientific applications, such as in the development of the PLATO computer-assisted instruction system. Influence on other languages: PL/I influenced the development of several other programming languages, including Pascal and Ada.  Some of its language features, such as support for structured programming and advanced data types, are now standard in many modern programming languages. Overall, PL/I played an important role in the development of modern programming languages and contributed to the advancement of computer science and technology. ",1
560,"Plug and Play (PnP) is a technology standard that allows devices to be automatically configured and connected to a computer without requiring manual intervention from the user.  When a PnP device is connected to a computer, the operating system detects the device and automatically installs the necessary drivers and software to enable the device to function properly. The PnP standard was first introduced in the early 1990s as part of Microsoft's Windows operating system.  It has since been adopted by other operating systems, including macOS and Linux. Some of the key benefits of PnP include:Ease of use: PnP simplifies the process of connecting and configuring new devices, making it easier for users to install and use hardware peripherals. Time-saving: PnP eliminates the need for manual configuration and installation of device drivers, saving time and reducing the risk of errors. Compatibility: PnP ensures that devices are compatible with the operating system and other hardware components, reducing the risk of conflicts and compatibility issues. Scalability: PnP supports the automatic detection and configuration of multiple devices, making it easy to add and remove devices as needed. Overall, PnP has played an important role in making computers more user-friendly and accessible, and has helped to simplify the process of installing and using new hardware devices. ",1
561,"A plug-in, also known as an add-on, extension, or module, is a software component that adds specific functionality to an existing software application.  Plug-ins are typically designed to work with a specific application or platform, and can be installed or removed independently of the main application. Plug-ins can be used to extend the functionality of web browsers, multimedia players, image editors, and other software applications.  For example, a web browser might have a plug-in that enables it to display Adobe Flash content, while an image editor might have a plug-in that adds support for a new image file format. Plug-ins are often developed by third-party developers, and are usually distributed as separate downloads.  They can be installed manually by the user, or they may be installed automatically when the user installs or updates the main application. Some of the benefits of plug-ins include:Customizability: Plug-ins allow users to customize the functionality of their software applications to better suit their needs. Flexibility: Plug-ins can be added or removed without affecting the core functionality of the main application. Modularity: Plug-ins are designed to work independently of the main application, making it easier to develop and maintain software. Innovation: Plug-ins enable developers to add new features and capabilities to existing applications, promoting innovation and creativity. Overall, plug-ins are an important tool for extending the functionality of software applications, and have played an important role in the development of modern software platforms. ",1
562,"Podcasting is the production and distribution of digital audio content that is delivered through the internet.  It involves creating audio files, which are typically in the form of episodes, and making them available for download or streaming via a subscription-based service or website. Listeners can access podcasts through a variety of platforms, including dedicated podcasting apps like Apple Podcasts, Google Podcasts, and Spotify, as well as through individual podcast websites or RSS feeds. Podcasts cover a wide range of topics, from news and current events to comedy, entertainment, education, and more.  They offer a convenient way for people to stay informed and entertained on the go, and have become increasingly popular in recent years. ",1
563,"Podcasts have a wide range of applications, some of which include:Entertainment: Podcasts can be a great source of entertainment, offering listeners a wide range of options to choose from, such as comedy, storytelling, and music. Education: Podcasts can be used for educational purposes, offering a convenient way for people to learn about specific topics and gain new skills. News and Current Events: Podcasts can be a great way to stay informed about the latest news and events, with many news organizations offering podcasts that cover the latest headlines. Marketing: Podcasts can be used as a marketing tool for businesses to reach new audiences and promote their products or services. Personal Development: Podcasts can offer listeners insights and advice on personal development, such as self-help, motivation, and mindfulness. Storytelling: Podcasts can be used as a platform for storytelling, offering a unique way for people to share their stories with a global audience. Overall, podcasts offer a flexible and versatile platform that can be used for a wide range of purposes, making them an increasingly popular medium for content creators and consumers alike. ",1
564,"PostScript is a page description language and programming language used primarily in the printing and publishing industries.  It was developed by Adobe Systems and first released in 1984. PostScript works by describing the text and graphics that make up a document in terms of mathematical equations and geometric shapes.  This information is then sent to a PostScript-compatible printer, which uses the equations and shapes to create a high-quality printed document. PostScript also includes features for managing fonts, color, and other aspects of the printed output.  Additionally, it can be used for creating graphics and other visual elements, making it a powerful tool for designers and artists. PostScript has been widely adopted in the printing and publishing industries and is supported by many software applications and hardware devices.  It has also been a key technology in the development of desktop publishing and digital printing. ",1
565,"Presentation software, also known as presentation graphics software, is a computer program used to create digital presentations.  These presentations typically consist of a series of slides that contain text, images, audio, and video.  The purpose of presentation software is to help users create engaging and informative presentations that can be used for a variety of purposes, such as business presentations, educational lectures, or marketing pitches. Some popular examples of presentation software include Microsoft PowerPoint, Google Slides, Prezi, Apple Keynote, and LibreOffice Impress.  These programs offer a variety of features, such as the ability to add animations, transitions, and multimedia content to your slides.  They also allow you to customize the look and feel of your presentation, including the use of templates, colors, fonts, and other design elements. Overall, presentation software is a powerful tool that can help users communicate complex ideas and information in an engaging and visually appealing way. ",1
566,"Typewriters are mechanical or electromechanical devices used for writing or printing text on paper.  They were commonly used for typing documents, letters, and other written materials before the advent of personal computers and printers. Typewriters were first invented in the 19th century, and they quickly became popular due to their ability to produce legible, professional-looking documents.  Early typewriters were purely mechanical, with keys that pressed inked ribbons against paper to create the desired characters.  Later models added electric power to speed up the typing process and eliminate the need for manual carriage returns. Typewriters were once a common fixture in offices, schools, and other institutions, but they have largely been replaced by digital technology.  However, some people still use typewriters for their unique aesthetic and tactile experience.  In recent years, there has been a resurgence of interest in typewriters among writers, artists, and collectors",1
567,"Dot matrix printers are a type of impact printer that use a matrix of pins to print text and graphics on paper.  They were widely used in the 1980s and 1990s for printing documents, receipts, and other materials. Unlike modern inkjet and laser printers, which use a non-impact printing process, dot matrix printers work by striking a ribbon against paper to create dots that form characters and images.  The print quality of dot matrix printers is generally lower than other types of printers, but they have the advantage of being able to print on multi-part forms and other thick media. Dot matrix printers were popular in many industries because of their durability and reliability, as well as their ability to print carbon copies and other multipart forms.  They were commonly used in point-of-sale systems, inventory control systems, and other business applications. Today, dot matrix printers are less common, as they have largely been replaced by faster, more efficient printers with higher print quality.  However, they are still used in some specialized applications, such as printing receipts and invoices in certain industries. ",1
568,"here are several types of charts commonly used in project management to visualize and communicate project progress, status, and other important data.  Some of the most commonly used charts for project management include:Gantt chart: A horizontal bar chart that displays project tasks on a timeline, showing their start and end dates, dependencies, and progress. Network diagram: A graphical representation of project tasks and their dependencies, often used to show the critical path and identify potential delays or bottlenecks. Pert chart: A type of network diagram that uses statistical analysis to estimate task durations and identify the critical path. Milestone chart: A timeline that highlights key project milestones, such as major deliverables, deadlines, and events. Resource allocation chart: A chart that shows the allocation of project resources, such as people, equipment, and budget, over time. Burn-down chart: A chart that tracks the amount of work completed against the project timeline, allowing the project team to estimate how much work remains to be done. Scatter chart: A chart that plots two variables against each other to identify trends or patterns, often used to analyze project data such as task completion rates or resource utilization. These charts can be used individually or in combination to provide a comprehensive view of project progress and status, and to help stakeholders understand key project metrics and trends. ",1
569,"The psychology of computing is a field of study that focuses on the psychological processes involved in human-computer interaction.  It seeks to understand how people perceive, process, and interact with computer technology, and how these interactions affect their behavior, cognition, and emotions. One of the key areas of study in the psychology of computing is human-computer interaction (HCI).  HCI researchers seek to understand how people interact with computer technology, and how technology can be designed to better support human needs and capabilities.  This involves studying factors such as user interface design, usability, user experience, and cognitive load. Another important area of study in the psychology of computing is the impact of technology on human cognition and behavior.  Researchers in this field examine how computer technology affects attention, memory, learning, decision-making, and social behavior.  They also explore the impact of technology on mental health and well-being, and the potential risks and benefits of technology use. The psychology of computing is a multidisciplinary field that draws on theories and methods from psychology, computer science, engineering, design, and other related disciplines.  Its findings are applied in a variety of settings, including the design of computer systems, software applications, and digital media, as well as in areas such as healthcare, education, and entertainment. ",1
570,"The psychology of cyberspace is a field of study that focuses on the psychological processes involved in human interactions with computer-mediated environments, such as the internet, social media, virtual reality, and other digital spaces.  It seeks to understand how people perceive, process, and interact with technology-mediated environments, and how these interactions affect their behavior, cognition, and emotions. One of the key areas of study in the psychology of cyberspace is the impact of digital environments on human behavior and cognition.  Researchers in this field examine how technology-mediated environments influence attention, memory, learning, decision-making, and social behavior.  They also explore the effects of digital environments on mental health and well-being, including the potential risks and benefits of technology use. Another important area of study in the psychology of cyberspace is online identity and self-presentation.  Researchers in this area examine how people present themselves online, how they manage their online reputations, and how they form social connections in digital environments.  They also explore how anonymity, privacy, and surveillance impact online behavior and self-disclosure. The psychology of cyberspace is a multidisciplinary field that draws on theories and methods from psychology, communication studies, sociology, anthropology, and computer science.  Its findings are applied in a variety of settings, including the design of digital technologies and online environments, as well as in areas such as education, healthcare, and marketing. ",1
571,"Python is a high-level, interpreted, and general-purpose programming language.  It was created by Guido van Rossum in the late 1980s and released in 1991.  Python is designed to be easy to read and write, making it a popular choice for beginners and professionals alike. Python's syntax is simple and intuitive, which makes it easier to understand and learn.  It uses indentation to define blocks of code instead of curly braces like in other programming languages. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming.  It also has a vast standard library, which provides support for a wide range of tasks like working with databases, data processing, networking, and web development. Python is an interpreted language, meaning that code is executed line by line.  This makes it faster to develop and easier to debug.  It is also platform-independent, meaning that Python code can run on different operating systems without modification. Python is used in a variety of applications, including web development, scientific computing, data analysis, artificial intelligence, and machine learning.  It is a popular choice for web development frameworks such as Django and Flask, and also for data analysis libraries such as pandas and NumPy. Overall, Python is a versatile and powerful language that is easy to learn and use, making it an ideal language for both beginners and experienced developers. ",1
572,"RAID stands for ""Redundant Array of Independent Disks"".  It is a technology that involves combining multiple physical disk drives into a single logical unit to improve data performance, reliability, or both. There are several different RAID levels, each with its own characteristics and benefits.  The most common RAID levels are:RAID 0: This level stripes data across multiple drives for improved performance, but provides no redundancy.  If one drive fails, all data is lost. RAID 1: This level mirrors data across multiple drives for improved redundancy, but provides no performance benefits.  If one drive fails, data is still accessible from the remaining drives. RAID 5: This level stripes data across multiple drives and also includes parity information, allowing for one drive to fail without data loss. RAID 6: This level is similar to RAID 5, but includes two sets of parity information, allowing for two drives to fail without data loss. Other RAID levels, such as RAID 10, RAID 50, and RAID 60, combine aspects of these basic levels to provide different trade-offs between performance and redundancy. RAID is commonly used in enterprise and server environments to ensure data availability, but can also be used in personal computers for improved performance and data protection. ",1
573,"Reverse engineering is the process of analyzing a product, device, or system to understand how it was designed and how it works.  It involves taking apart the object and examining its components and behavior to determine its structure, function, and operation.  The goal of reverse engineering is to gain insights into how a product or system works in order to replicate, modify, or improve upon it. Reverse engineering is often used in the fields of engineering, computer science, and manufacturing.  In engineering, reverse engineering can be used to understand the design of a product or system that was created by another engineer or company.  This can be useful in the development of new products or in troubleshooting existing ones. In computer science, reverse engineering can be used to analyze software programs to determine their source code, structure, and behavior.  This can be useful for understanding how a program works or for detecting and fixing security vulnerabilities. In manufacturing, reverse engineering can be used to create a duplicate or modified version of a product or component.  This can be useful in cases where the original design or manufacturing process is no longer available or is too expensive to reproduce. However, it's important to note that reverse engineering can be a controversial practice, especially when it involves proprietary or confidential information.  It's important to follow ethical guidelines and legal regulations when performing reverse engineering. ",1
574,"RFID (Radio Frequency Identification) is a technology that uses radio waves to wirelessly transmit data from a tag or label attached to an object to a reader.  RFID technology can be classified into two types: passive RFID and active RFID. Passive RFID tags or labels do not have their own power source and rely on the power transmitted from the RFID reader to send their information.  When the RFID reader emits a signal, the passive RFID tag or label receives the signal and uses the power to transmit its information back to the reader.  Passive RFID tags are typically used for tracking inventory, managing supply chains, and tracking assets. Active RFID tags or labels, on the other hand, have their own power source and can transmit data independently of an RFID reader.  These tags use batteries or other power sources to broadcast a signal to the reader.  Active RFID tags are often used for tracking high-value assets, such as shipping containers or vehicles, or for tracking people in environments like hospitals or warehouses. The main differences between passive and active RFID are their power requirements and the range over which they can operate.  Passive RFID tags are cheaper, smaller, and have a shorter read range than active RFID tags.  Active RFID tags are more expensive, larger, and can have a longer read range.  The choice between passive and active RFID depends on the application and the requirements of the specific use case. ",1
575,"RFID (Radio Frequency Identification) tags and cards are used in a wide variety of applications across multiple industries.  Here are some examples of the current uses for RFID tags and cards of various types:Inventory Management: RFID tags are used to track inventory and monitor the movement of goods in warehouses, distribution centers, and retail stores.  RFID tags are attached to products or packaging and can be quickly scanned to track inventory levels and identify the location of specific items. Supply Chain Management: RFID tags are used to monitor the movement of goods throughout the supply chain, from manufacturers to distributors to retailers.  This helps to improve supply chain visibility and increase efficiency. Access Control: RFID cards are used to control access to buildings, rooms, and restricted areas.  Employees or authorized personnel carry RFID cards that are read by card readers at entry points, allowing them access to the authorized areas. Payment Systems: RFID cards can be used for contactless payments, such as in public transportation or at events.  The card is waved over a reader and the payment is automatically deducted from the card balance. Tracking Animals: RFID tags are used to track and monitor animals in agriculture and research.  The tags are attached to the animals and can be used to track their movements and behavior. Healthcare: RFID tags are used in healthcare to track medical equipment, supplies, and patients.  The tags are attached to equipment or supplies and can be quickly located when needed.  RFID wristbands are also used to identify patients and track their medical information. Vehicle Identification: RFID tags are used to identify vehicles and manage toll collection, parking access, and traffic management.  The tags are attached to the vehicle and are read by RFID readers at entry points. These are just a few examples of the many uses for RFID tags and cards in various industries.  As technology advances, it is likely that new applications for RFID will continue to emerge. ",1
576,"RFID (Radio Frequency Identification) technology uses radio waves to wirelessly identify and track objects.  While RFID has many benefits, such as improved supply chain management and inventory tracking, there are also privacy and security issues associated with its use.  Here are some of the main concerns:Data privacy: RFID tags can store a lot of data, including unique identification numbers, personal information, and tracking information.  If this data falls into the wrong hands, it can be used for identity theft, tracking individuals, or other nefarious purposes. Unauthorized access: RFID systems are vulnerable to hacking, which could allow an attacker to gain unauthorized access to sensitive information, such as credit card data or personal identification. Physical security: RFID tags are small and can be easily hidden, making them difficult to detect.  This can be a problem in high-security areas where individuals may try to smuggle in unauthorized RFID tags to gain access to restricted areas. Interception and eavesdropping: RFID signals are transmitted over the airwaves, which means they can be intercepted and eavesdropped on by third parties.  This can result in the unauthorized access of sensitive data. Tracking: RFID tags can be used to track the movements of individuals and objects, which can raise privacy concerns.  For example, if RFID tags are used in retail stores, they could be used to track customers' movements and buying habits. To mitigate these risks, it's important to implement appropriate security measures, such as encryption, access control, and regular monitoring and audits of RFID systems.  Additionally, individuals should be made aware of the potential privacy and security risks associated with RFID technology, and should be given the option to opt-out of RFID tracking if possible. ",1
577,"Computing has revolutionized the way we live and work, but it also comes with risks.  Here are some of the main risks associated with computing:Cybersecurity threats: With the rise of the internet, cyber threats have become a major concern for individuals and organizations alike.  Cyberattacks such as malware, phishing, and ransomware can compromise the security of computer systems and result in data theft, financial loss, and reputational damage. Data privacy concerns: The increasing amount of personal and sensitive data stored and transmitted via computing systems has raised concerns about data privacy.  Unauthorized access to this data can result in identity theft, financial fraud, and other malicious activities. Software vulnerabilities: Software vulnerabilities, such as bugs and coding errors, can be exploited by attackers to gain unauthorized access to computer systems or data.  These vulnerabilities can also result in system crashes and other issues. Dependence on technology: As we become more dependent on computing technology, we become more vulnerable to the consequences of its failure.  For example, a power outage or system failure can result in significant disruptions to business operations, critical services, and public safety. Technology addiction: Computing technology can be addictive, leading to issues such as decreased productivity, social isolation, and mental health problems. To mitigate these risks, it's important to implement appropriate security measures, such as strong passwords, firewalls, antivirus software, and regular software updates.  Individuals and organizations should also practice good cybersecurity hygiene, such as being cautious of suspicious emails, avoiding public Wi-Fi networks, and backing up important data.  Additionally, it's important to balance our reliance on computing technology with a healthy lifestyle and social interactions. ",1
578,"Industrial robotics refers to the use of robots in industrial manufacturing processes.  The development of industrial robotics can be traced back to the 1950s and 1960s, when the first programmable robots were created. The first industrial robot, the Unimate, was created in 1954 by George Devol and Joseph Engelberger.  It was used to automate the production line at a General Motors plant in New Jersey.  The Unimate was a large, hydraulic robot that could lift heavy objects and move them along a set path.  It was controlled by a computer program that could be programmed to repeat a set of motions. In the following years, the use of industrial robots became more widespread, particularly in the automotive industry.  Robots were used to perform tasks such as welding, painting, and assembly on production lines.  This led to increased efficiency and productivity in manufacturing processes, as well as improved worker safety. As technology advanced, industrial robots became more advanced and capable.  They became smaller and more precise, and were able to perform a wider range of tasks.  Today, industrial robots are used in a variety of industries, including electronics, pharmaceuticals, and food production.  They continue to play an important role in increasing efficiency, productivity, and safety in manufacturing processes. ",1
579,"Mobile robots are robots that are designed to move around and operate in different environments.  They can be either fully autonomous, or remotely controlled by a human operator. Mobile robots can come in many different forms, depending on their intended purpose and environment.  Some examples of mobile robots include:Drones - unmanned aerial vehicles that are used for tasks such as aerial photography, delivery, and inspection. Rovers - robots that are used to explore other planets, such as the Mars rovers Spirit, Opportunity, and Curiosity. Autonomous vehicles - self-driving cars and trucks that use sensors and machine learning algorithms to navigate and transport passengers or goods. Humanoid robots - robots that are designed to look and move like humans, such as ASIMO developed by Honda. Quadruped robots - robots that have four legs, such as Boston Dynamics' Spot and SpotMini robots, which are used for inspection, surveillance, and other tasks. Underwater robots - robots that are used for tasks such as oceanographic research, underwater exploration, and pipeline inspection. Mobile robots are becoming increasingly important in many industries, including manufacturing, transportation, logistics, and healthcare.  They offer numerous benefits, such as increased efficiency, improved safety, and greater precision, and are likely to play an increasingly important role in the future as technology continues to advance. ",1
580,"Service robots are robots that are designed to perform services for humans, either autonomously or under human control.  They are used in a wide range of applications, such as cleaning, cooking, caregiving, and entertainment. There are several different types of service robots, including:Domestic robots - robots that are designed for use in the home, such as robotic vacuum cleaners, lawn mowers, and window cleaners. Personal assistance robots - robots that are designed to assist people with disabilities or elderly people, such as robotic exoskeletons, mobility aids, and communication aids. Healthcare robots - robots that are used in healthcare settings, such as surgical robots, rehabilitation robots, and telepresence robots for remote consultations. Education and entertainment robots - robots that are designed to provide educational or entertainment services, such as language learning robots, social robots, and interactive robots for museums and exhibitions. Customer service robots - robots that are designed to interact with customers in commercial settings, such as reception robots, food service robots, and shopping assistants. Service robots are becoming increasingly common in many different settings, as they offer numerous benefits such as increased efficiency, reduced labor costs, and improved safety.  As technology continues to advance, service robots are likely to play an increasingly important role in our daily lives, offering a wide range of services to improve our quality of life. ",1
581,"RPG, or Report Program Generator, is a high-level programming language originally developed by IBM in the late 1950s for use on their IBM System/3x mid-range computers.  RPG was designed specifically for generating reports and was particularly well-suited for working with the IBM database management system (DB2). Over the years, RPG has evolved and expanded to become a more versatile language, capable of handling more complex programming tasks beyond report generation.  Modern versions of RPG support structured programming constructs, object-oriented programming, and integration with web services. RPG is still widely used today, particularly in IBM i (formerly known as AS/400 or iSeries) environments, which are often used in industries such as finance, manufacturing, and healthcare.  RPG has a loyal following among developers who appreciate its simplicity and ease of use, as well as its integration with IBM i system functions. ",1
582,"RSS stands for ""Really Simple Syndication"" or sometimes ""Rich Site Summary"".  It is a web feed format used to publish frequently updated content, such as blog posts, news articles, podcasts, and videos, in a standardized way that can be easily read by computers and other devices. RSS feeds typically contain a list of the latest items from a website or blog, along with metadata such as the publication date, author, and a brief summary or excerpt of the content.  Users can subscribe to RSS feeds using specialized feed reader software or built-in feed readers in web browsers or email clients.  When new content is added to a subscribed feed, the user is notified and can view the updated content without having to visit the website directly. RSS has been widely adopted as a way to distribute and consume content on the web, and many websites offer RSS feeds as an alternative to email newsletters or social media updates.  However, its usage has declined in recent years with the popularity of social media and other content distribution channels. ",1
583,"RSS, or Really Simple Syndication, was developed in the late 1990s as a way to easily distribute content from websites to other sites or individuals.  The first version of RSS was created by Netscape in 1999, but it was later refined and standardized by Dave Winer and others at UserLand Software. In 2002, RSS 2. 0 was released, which became the most widely adopted version of the format.  RSS 2. 0 introduced new features such as support for enclosures (which allowed podcasts and other media to be distributed through RSS), and improved support for metadata and internationalization. In the mid-2000s, RSS became increasingly popular as a way to follow blogs and news sites.  Many popular blogging platforms, such as WordPress and TypePad, included built-in support for RSS feeds, making it easy for users to subscribe to updates.  RSS readers and feed aggregators such as Feedly and Google Reader also gained popularity during this time. However, the popularity of RSS declined in the late 2000s and early 2010s as social media and other content distribution channels such as email newsletters became more popular.  Some major websites, such as Facebook and Twitter, stopped supporting RSS feeds entirely. Despite its decline in usage, RSS remains a useful tool for distributing and consuming content on the web, particularly for niche or specialized content that may not be easily discoverable through other channels. ",1
584,"RTF stands for Rich Text Format.  It is a file format used for storing formatted text and graphics that can be easily exchanged between different word processors and operating systems.  RTF files can be opened and edited by many different software applications, including Microsoft Word, Google Docs, LibreOffice, and others. RTF files contain a mixture of plain text and formatting codes that describe how the text should be displayed.  These codes include instructions for fonts, colors, text styles (such as bold or italic), paragraph formatting, and more.  RTF files may also include images, tables, and other types of multimedia. One of the main advantages of RTF is its compatibility across different platforms and software applications.  Since RTF is a standardized format, it can be opened and edited by many different word processors and text editors, without the need for additional software or converters. However, one potential limitation of RTF is that it may not support all the advanced features or formatting options of a particular word processor or software application.  In some cases, converting a document to RTF format may result in loss of formatting or other changes. ",1
585,"Ruby is a high-level, dynamic, object-oriented programming language that was created in the mid-1990s by Yukihiro ""Matz"" Matsumoto in Japan.  It was designed to be simple, easy to learn, and productive for developers. Ruby is known for its elegant syntax and its focus on programmer productivity.  It has a strong emphasis on object-oriented programming and provides many features for working with objects, including classes, inheritance, and polymorphism. Ruby is also a versatile language that can be used for a wide variety of programming tasks, including web development, system administration, data analysis, and more.  It has a large and active community of developers who contribute libraries, tools, and frameworks that make it easy to build complex applications. One of the most popular web frameworks built with Ruby is Ruby on Rails, also known simply as Rails.  Rails is a web application framework that provides a set of conventions and tools for building database-backed web applications.  Rails is known for its emphasis on convention over configuration, which makes it easy to get started with a new project and reduces the amount of boilerplate code that developers need to write",1
586,"Ruby on Rails, also known as simply Rails, is a web application framework written in the Ruby programming language.  It was created by David Heinemeier Hansson in 2004 and has since become one of the most popular web frameworks for building dynamic, database-driven web applications. Rails provides a set of conventions and tools for building web applications, including support for database migrations, model-view-controller (MVC) architecture, and a wide range of plugins and gems that extend its functionality. One of the key features of Rails is its emphasis on convention over configuration.  This means that Rails provides sensible defaults and conventions that help developers get up and running quickly without having to spend a lot of time configuring the framework.  For example, Rails provides a default directory structure for organizing files, a default database schema for storing data, and a set of naming conventions that make it easy to map between database tables and Ruby classes. Rails is also known for its focus on developer productivity, with a philosophy that emphasizes simplicity, elegance, and developer happiness.  This has made it a popular choice for startups and small businesses, as well as larger enterprises. Rails is used by many popular websites and companies, including GitHub, Shopify, Airbnb, and Basecamp. ",1
587,"SAP is a multinational technology company headquartered in Walldorf, Germany.  The company was founded in 1972 by five former IBM employees with the goal of creating software that could automate business processes.  Since then, SAP has become one of the world's largest software companies and a leading provider of enterprise software solutions. In addition to its software products, SAP is known for its commitment to sustainability and corporate social responsibility.  The company has set ambitious targets to reduce its greenhouse gas emissions and become carbon neutral by 2025.  SAP has also committed to promoting diversity and inclusion, both within the company and in the wider community. SAP has a strong presence in the global business community and has developed partnerships with a wide range of organizations, including other technology companies, universities, and non-profits.  The company has also established various programs to support entrepreneurship and innovation, including an accelerator program for startups and a venture capital fund. As a company, SAP places a strong emphasis on research and development, with a significant portion of its budget dedicated to innovation.  The company's research efforts focus on emerging technologies such as artificial intelligence, machine learning, and blockchain, as well as new applications for existing technologies. Overall, SAP is a leading global technology company with a strong commitment to sustainability, diversity, and innovation.  Its software products have become integral to the operations of many businesses around the world, and its influence is likely to continue growing in the years ahead. ",1
588,"SAP offers a wide range of enterprise software applications and products that are designed to help businesses manage their operations, processes, and data more efficiently.  Here are some of SAP's main application and product offerings:Enterprise Resource Planning (ERP): SAP's ERP software is its flagship product and helps businesses manage their core business processes, such as finance, sales, procurement, production, and supply chain management. Customer Relationship Management (CRM): SAP's CRM software helps businesses manage their customer interactions, analyze customer data, and improve customer engagement. Supply Chain Management (SCM): SAP's SCM software helps businesses manage their supply chain operations, including logistics, inventory management, and procurement. Human Capital Management (HCM): SAP's HCM software helps businesses manage their human resources processes, including recruitment, onboarding, training, and performance management. Financial Management: SAP offers a range of financial management applications, including financial accounting, management accounting, and treasury and risk management. Analytics and Business Intelligence: SAP's analytics and business intelligence products help businesses analyze their data and make informed decisions based on insights. Industry-Specific Applications: SAP also offers a range of industry-specific applications for businesses in sectors such as retail, manufacturing, healthcare, and public sector. Cloud Applications: SAP has been investing heavily in its cloud offerings, and now offers a range of cloud-based applications, including SAP S/4HANA Cloud, SAP SuccessFactors, and SAP Ariba. SAP's applications and products are used by businesses of all sizes, from small startups to large multinational corporations, and the company has a strong reputation for providing reliable and scalable solutions. ",1
589,"The challenges faced by SAP in its business growth include increased competition, evolving technology landscape, economic uncertainty, implementation challenges, and compliance and regulatory issues.  To overcome these challenges, SAP has responded with strategic investments, innovation, and a strong commitment to customer satisfaction. SAP's competitors include established players such as Oracle, Microsoft, and IBM, as well as newer entrants such as Salesforce and Workday.  These competitors offer products that are comparable to SAP's offerings, which has put pressure on the company to differentiate itself and continuously innovate.  Additionally, these competitors have been able to leverage new technologies and business models to gain market share. Overall, the software industry is highly competitive, and SAP must continue to adapt to changing market conditions, customer needs, and emerging technologies to maintain its position as a leading provider of enterprise software solutions",1
590,"A scanner is a device that captures images or text and converts them into digital data that can be stored, manipulated, and transmitted.  The first scanners were developed in the 1950s and 1960s for specialized applications such as document archiving and scientific research. In the 1980s, scanners became more widely available as personal computers became more popular.  The earliest scanners were flatbed scanners that used a moving light source and a photosensitive element to scan documents or images.  Over time, scanners became more sophisticated, with faster scanning speeds, higher resolutions, and more advanced features such as automatic document feeders, optical character recognition (OCR), and duplex scanning. Today, scanners are used in a variety of applications, from document archiving and digitization to graphic design, medical imaging, and security screening.  There are many different types of scanners available, including flatbed scanners, sheet-fed scanners, handheld scanners, and drum scanners, each with their own strengths and limitations. Overall, scanners have revolutionized the way we store and transmit information, making it easier and faster to share and access information in digital form. ",1
591,"There are several types of scanners available, each with its own unique features and uses.  Some of the most common types of scanners include:Flatbed Scanners: These are the most commonly used scanners and are similar in appearance to photocopiers.  They have a flat glass surface on which documents or photos are placed for scanning. Sheet-fed Scanners: These scanners are designed to scan multiple pages at once and are often used in offices for scanning large documents or contracts.  They work by feeding a stack of documents through the scanner, one page at a time. Handheld Scanners: These scanners are small and portable, and are often used to scan barcodes or other types of information in retail or warehouse environments. Drum Scanners: These scanners are used primarily in the graphic design industry and can produce very high-quality scans of photographs, slides, and negatives. Film Scanners: These scanners are specifically designed to scan film negatives, slides, or transparencies.  They typically offer high resolution and color accuracy for reproducing film images in digital form. 3D Scanners: These scanners are used to create digital models of physical objects in three dimensions.  They can be used in a variety of applications, from industrial design and engineering to virtual reality and gaming. Portable Scanners: These scanners are small and lightweight, and are designed to be easily transported.  They are often used by professionals who need to scan documents or images while on the go. Each type of scanner has its own strengths and weaknesses, and the choice of scanner depends on the specific needs and requirements of the user. ",1
592,"The quality of scanned images can be affected by several factors, including:Resolution: The resolution of a scanner determines the level of detail in the scanned image.  Higher resolution scanners produce more detailed images, but also require more time to scan and create larger file sizes. Color Depth: The color depth of a scanner determines the number of colors that can be represented in the scanned image.  Higher color depths produce more realistic and accurate images, but also result in larger file sizes. Dynamic Range: The dynamic range of a scanner determines the range of brightness levels that can be captured in the scanned image.  Higher dynamic ranges allow for more subtle variations in brightness and contrast, resulting in more realistic and accurate images. Scanning Speed: The scanning speed of a scanner can affect the quality of the image, especially if the scanner is moving too quickly or too slowly.  Slow scanning speeds can result in blurring or distortion of the image, while fast scanning speeds can result in reduced image quality. Scanner Quality: The quality of the scanner itself can affect the quality of the scanned image.  High-quality scanners are designed to produce more accurate and consistent images, while lower-quality scanners may produce images with more noise or distortion. Image Processing: The software used to process and edit the scanned image can also affect its quality.  Advanced image processing software can correct for common scanning issues such as color balance, sharpness, and noise reduction. Overall, the quality of scanned images is influenced by a variety of factors, and it's important to choose the right scanner and use the appropriate settings and software to achieve the best results. ",1
593,"Scheduling multiprocessor systems refer to computer systems that have multiple processors or cores that can perform parallel processing.  These systems are designed to handle complex and resource-intensive tasks that cannot be efficiently executed on a single processor system. In a scheduling multiprocessor system, the operating system is responsible for assigning tasks or processes to different processors or cores for parallel execution.  This is done through a scheduling algorithm that determines which process should be executed by which processor at a given time. The goal of scheduling in a multiprocessor system is to optimize the use of available resources and minimize the time required to complete a given set of tasks.  There are various scheduling algorithms used in multiprocessor systems, including Round Robin, First Come First Serve, and Priority Scheduling. Multiprocessor systems are commonly used in applications such as scientific computing, data processing, and real-time systems that require high performance and reliability.  They are also used in servers, supercomputers, and high-end workstations for demanding applications such as simulation, modeling, and analysis. ",1
594,"There are several trends in recent years that have changed the emphasis in scheduling algorithms.  Two of the most significant trends are:Multi-core and many-core processors: Modern computer systems often have multiple cores, which means that there are several processors on a single chip.  This trend has led to an increased emphasis on parallel processing and scheduling algorithms that can take advantage of the parallelism offered by multiple cores. Cloud computing: Cloud computing has become increasingly popular, and it allows users to access computing resources on demand.  This trend has led to an increased emphasis on scheduling algorithms that can handle dynamic workloads, where the number of users and the demand for computing resources can vary over time. As a result of these trends, scheduling algorithms have evolved to become more complex and sophisticated.  Many modern scheduling algorithms are designed to take advantage of parallel processing and dynamic workloads, while still ensuring that tasks are completed efficiently and effectively.  Some examples of these algorithms include gang scheduling, space-sharing scheduling, and adaptive scheduling. ",1
595,"Scientific computing involves the use of mathematical models, algorithms, and high-performance computing to solve complex scientific problems.  Here are some common applications of scientific computing:Computational modeling and simulation: Scientific computing is widely used to create and simulate models of physical, biological, and chemical systems.  These models help researchers to study phenomena that are difficult to observe or measure directly. Data analysis and visualization: Scientific computing techniques are used to analyze and visualize large datasets, such as those generated in fields like astronomy, genomics, and climate science. Numerical analysis: Scientific computing methods are used to develop and analyze numerical algorithms, which are essential for solving mathematical problems that cannot be solved analytically. Optimization: Scientific computing is used to optimize complex systems and processes, such as manufacturing and supply chain management, by identifying the best possible solutions within specified constraints. Machine learning and artificial intelligence: Scientific computing techniques are also used in the development of machine learning and artificial intelligence algorithms, which have revolutionized fields like computer vision, natural language processing, and robotics. ",1
596,"Simulation and visualization are important tools in scientific experiments because they allow scientists to better understand complex systems and phenomena.  Here are some of the key benefits of simulation and visualization in scientific experiments:Understanding complex systems: Many natural and engineered systems are too complex to be understood through direct observation or measurement alone.  By creating computer simulations of these systems, scientists can study how they behave under different conditions and gain insights into their underlying mechanisms. Prediction and hypothesis testing: Simulation and visualization allow scientists to make predictions about how systems will behave in the future and test hypotheses about their behavior.  This can help to guide experimental design and data collection, as well as to refine theories and models. Visualization of data: Visualization techniques are often used to display large and complex datasets in a more intuitive and accessible way.  By creating visual representations of data, scientists can identify patterns and relationships that might be difficult to see through other means. Communication: Simulation and visualization can also be powerful communication tools, allowing scientists to share their findings with a wider audience.  By presenting data and simulations in a visually compelling way, scientists can make their research more accessible and engaging to both experts and the general public. Overall, simulation and visualization are important tools in scientific experiments because they allow researchers to gain a deeper understanding of complex systems, test hypotheses, and communicate their findings in a more compelling way. ",1
597,"Search engines are software tools that allow users to search for information on the World Wide Web.  They have become an essential part of our daily lives, as they make it easy to find information on almost any topic.  Here is a brief background on the development of search engines and the approaches to exploring the web:Directory-based search engines: In the early days of the web, search engines were based on directories of websites.  These directories were organized by topic, and users could browse through them to find sites related to their interests.  Examples of early directory-based search engines include Yahoo! and DMOZ. Keyword-based search engines: As the number of websites on the web grew, it became impractical to rely on manual directory listings.  In the mid-1990s, keyword-based search engines were developed, which allowed users to search for specific words or phrases on the web.  Examples of early keyword-based search engines include AltaVista and Lycos. PageRank algorithm: In the late 1990s, Google introduced a new algorithm called PageRank, which revolutionized the way search engines ranked web pages.  PageRank is based on the idea that a web page is important if other important pages link to it.  This allowed Google to provide more relevant and accurate search results than its competitors. Natural language processing: In recent years, search engines have started to incorporate natural language processing techniques to better understand the intent behind user queries.  This has led to the development of more sophisticated search algorithms that can provide more personalized and accurate results. Approaches to exploring the web include:Browsing: This approach involves manually navigating through websites and following links to find information.  Browsing is useful for discovering new websites and exploring topics in depth. Keyword search: This approach involves entering one or more keywords into a search engine and viewing the search results.  Keyword search is useful for finding specific information quickly. Natural language search: This approach involves entering a question or phrase into a search engine and allowing the search engine to interpret the query and provide relevant results.  Natural language search is useful for complex or nuanced queries that may not be easily expressed as keywords. ",1
598,"The Semantic Web is an extension of the World Wide Web that aims to make web content more machine-readable and interconnected.  It is based on the idea of adding metadata, or data that describes other data, to web pages and other online resources. The Semantic Web is designed to enable machines to understand the meaning and context of web content, rather than just presenting it as a collection of text and images.  This can enable new types of applications and services that rely on machine-readable data, such as intelligent search engines, personalized recommendations, and automated decision-making systems. To achieve these goals, the Semantic Web uses a set of standardized technologies and protocols, including:Resource Description Framework (RDF): RDF is a language for describing resources on the web, including people, organizations, and concepts.  It allows metadata to be added to web pages in a structured and machine-readable way. Web Ontology Language (OWL): OWL is a language for creating ontologies, or formal representations of knowledge and concepts.  It allows web content to be classified and organized in a way that machines can understand. SPARQL Protocol and RDF Query Language (SPARQL): SPARQL is a language for querying and manipulating RDF data.  It allows machines to extract information from the Semantic Web and use it in applications and services. The Semantic Web has the potential to revolutionize the way we access and use information on the web.  By making web content more machine-readable and interconnected, it can enable new types of intelligent applications and services that can help us to find and use information more efficiently and effectively.  However, the full potential of the Semantic Web has yet to be realized, and there are still many technical and practical challenges that need to be addressed. ",1
599,"Semantic web is an extension of the World Wide Web that aims to make web data more meaningful and useful to humans and machines.  Some applications of semantic web include:Data integration: The semantic web provides a common framework for integrating data from multiple sources, which can be useful for tasks such as business intelligence, research, and data analysis. Knowledge management: The semantic web can help organizations manage knowledge by providing a standardized way to represent and organize data, making it easier to find and use. E-commerce: Semantic web technologies can be used to create more intelligent online shopping experiences by providing users with more relevant and personalized product recommendations. Search engines: Search engines can use semantic web technologies to better understand the meaning of web content and provide more accurate search results. Social networks: The semantic web can be used to create more intelligent social networks that can automatically identify and connect users with similar interests. Healthcare: The semantic web can be used in healthcare to help standardize medical terminology and enable more accurate diagnosis and treatment. Government: The semantic web can be used to create more intelligent government services, such as online tax filing or citizen information portals. Overall, the semantic web has the potential to revolutionize the way we access and use web data, making it more efficient and effective for a wide range of applications. ",1
600,"Senior citizens, also known as older adults, have become an increasingly important demographic in the use of computing technologies.  While some seniors may feel intimidated by new technologies or have physical limitations that make it difficult to use computers, many others have embraced computing as a way to stay connected with loved ones, access information, and engage with the world around them. Here are some key aspects of senior citizens and computing:Accessibility: Many seniors have physical limitations that make it difficult to use computers, such as poor eyesight, hearing loss, or mobility issues.  Computer manufacturers and software developers have responded by creating products and features that are more accessible to seniors, such as larger text, audio cues, and touch screens. Social connections: For many seniors, computing has become a way to stay connected with family and friends who live far away.  Social networking platforms such as Facebook, Twitter, and Instagram have made it easier to stay in touch with loved ones and share photos and updates. Health and wellness: Computing technologies can be used to help seniors monitor their health and wellness, such as through fitness trackers, medication reminders, and telemedicine platforms that allow them to consult with doctors remotely. Lifelong learning: The internet provides a wealth of information and educational resources that can be accessed from the comfort of one's home.  Seniors can use computers to continue learning and exploring new topics, whether through online courses, webinars, or research. Entertainment: Computers and mobile devices offer a wide range of entertainment options for seniors, such as streaming movies and TV shows, playing games, and listening to music. In summary, computing technologies have become an important part of many seniors' lives, providing them with new ways to stay connected, informed, and engaged.  By making computing more accessible and user-friendly for seniors, we can help ensure that everyone has the opportunity to benefit from these technologies. ",1
601,"A serial port is a type of communication interface on a computer or other device that enables data to be transmitted one bit at a time, sequentially over a single communication channel.  It is a physical connector on a computer that enables communication between the computer and external devices, such as printers, scanners, modems, and other peripherals. Serial ports were commonly used in older computer systems, but have largely been replaced by USB (Universal Serial Bus) and other more advanced interfaces.  Serial ports can still be found in some specialized applications where low-speed data communication is required, such as industrial automation, networking, and scientific instruments. Serial ports are typically labeled as COM ports in Windows systems, and have a standardized set of settings, such as baud rate, data bits, parity, and stop bits, that must be configured to ensure proper communication between devices. ",1
602,"The standard for serial transmission is a set of rules and guidelines that specify how data is transmitted over a serial communication channel.  These standards define the physical and electrical characteristics of the communication channel, as well as the format and timing of the data that is transmitted. The most common serial transmission standard is RS-232, which is a standard developed by the Electronic Industries Association (EIA) for serial communication between devices.  RS-232 defines the electrical signal levels, timing, and data format for serial communication, including the use of start and stop bits, parity bits, and other control characters. Other serial transmission standards include RS-422, RS-485, and Universal Serial Bus (USB), each with its own set of specifications for electrical characteristics, signaling, and data format.  These standards are used in different applications and industries, depending on the specific requirements for data transmission. In general, serial transmission involves transmitting data one bit at a time over a single communication channel, with each bit representing a binary value of 0 or 1.  The data is transmitted in a specific format, with each transmission including a start bit, data bits, optional parity bit, and stop bit, which enables the receiving device to synchronize and correctly interpret the data being transmitted.  The speed of transmission is measured in baud rate, which represents the number of signal changes per second. ",1
603,"Freeware is software that is available for free, without any cost or payment required.  Freeware first came into existence in the 1980s and 1990s, as personal computers became more popular and affordable, and software developers began to create software for these new platforms. One of the earliest examples of freeware was PC-Write, a word processing program created by Bob Wallace in 1983.  Wallace initially sold the program, but later decided to release it as freeware, allowing users to download and use it for free.  Other early examples of freeware included games, utilities, and other software programs that were distributed through bulletin board systems (BBS) and other early computer networks. The growth of the internet in the 1990s further fueled the development and distribution of freeware, as developers were able to reach a much larger audience through online distribution.  The open-source software movement also played a role in the growth of freeware, as developers began to release source code for their software under licenses that allowed others to modify and distribute the software for free. Today, freeware continues to be an important part of the software landscape, with many popular programs and tools available for free download and use.  While some freeware is supported by advertising or other revenue streams, many developers release their software as a labor of love, to contribute to the community and to share their work with others. ",1
604,"In computing, a shell is a program that provides a user interface for accessing an operating system's services and resources.  The shell acts as an intermediary between the user and the operating system, allowing the user to interact with the system by issuing commands and receiving feedback and results. A shell can be thought of as a command interpreter or command-line interface that allows users to enter commands and execute them on the operating system.  The shell also provides access to utilities and programs installed on the system, allowing users to perform tasks such as file management, process management, and system configuration. There are several different types of shells, including the Bourne shell (sh), the C shell (csh), the Korn shell (ksh), and the Bash shell (bash), which is the default shell for most Linux and Unix-based operating systems.  Each shell has its own set of commands and syntax, but they all provide similar functionality and are designed to make it easier for users to interact with the operating system. In addition to providing a command-line interface, some shells also support graphical user interfaces (GUIs) and other advanced features such as scripting and automation.  Shells are an important part of the computing ecosystem, and are used by system administrators, developers, and power users to perform a wide variety of tasks and operations. ",1
605,"Simulation is the process of creating a model or representation of a system or process, in order to study its behavior under different conditions or scenarios.  Simulations can be performed using a variety of techniques, including mathematical models, computer programs, physical models, or a combination of these methods. Simulations are used in many fields, including engineering, physics, biology, economics, and social sciences, to name a few.  They are often used to test hypotheses, explore new ideas, and make predictions about the behavior of complex systems. In a simulation, the system being studied is typically represented by a set of variables and equations, which describe the behavior of the system over time.  The simulation is then run using various input values or scenarios, and the output is analyzed to understand how the system behaves under different conditions. Simulations can be used for many purposes, including predicting the performance of a new product or design, optimizing the operation of a manufacturing process, testing the safety of a new drug, or studying the behavior of a complex ecosystem.  They can also be used to train individuals in a safe, controlled environment, such as flight simulators for pilots or simulators for medical procedures. Overall, simulation is a powerful tool for understanding and predicting the behavior of complex systems, and is widely used in a variety of applications and fields. ",1
606,"Simulation has a wide range of applications across many fields, including engineering, medicine, economics, social sciences, and many others.  Here are some specific examples of how simulation is used:Engineering: Simulation is used extensively in engineering to design, test, and optimize products and systems.  For example, simulations are used to model the performance of aircraft, automobiles, and other transportation systems, as well as to optimize manufacturing processes and improve energy efficiency. Medicine: Medical simulation is used to train healthcare professionals, simulate medical procedures, and develop new medical technologies.  Examples include surgical simulations, virtual patient simulations, and simulations of disease progression. Economics: Simulation is used in economics to model and predict the behavior of financial systems, including stock markets, currency exchange rates, and other economic indicators.  It can also be used to simulate the impact of policy changes on the economy. Social sciences: Simulation is used in social sciences to model and study complex social systems, such as human behavior, urban development, and environmental management.  It can also be used to study the spread of disease and the impact of social policies. Gaming: Simulation is used extensively in gaming to create realistic virtual environments and experiences.  Examples include flight simulators, racing simulators, and sports simulations. Overall, simulation is a powerful tool that can be used to model and study complex systems in a wide range of fields.  It allows researchers and professionals to test hypotheses, optimize designs, and make predictions about the behavior of complex systems. ",1
607,"A smart building is a building that utilizes advanced technology and automated systems to enhance its functionality, efficiency, and security while reducing its environmental impact.  Some of the features of a smart building include:Energy Efficiency: Smart buildings are designed to optimize energy use, reduce wastage, and lower energy bills.  They often have automated systems that control lighting, heating, and cooling, and can adjust settings based on occupancy, weather, and time of day. Automation: Smart buildings have automated systems that can control various functions such as lighting, heating, ventilation, and air conditioning (HVAC), security, and access control.  These systems can be controlled remotely or automatically, and can adjust settings based on occupancy, temperature, and other parameters. Internet of Things (IoT) Integration: Smart buildings often integrate IoT sensors and devices to monitor various aspects of the building, such as occupancy, temperature, humidity, air quality, and energy use.  These devices can communicate with each other and with central systems to optimize building performance and identify issues early on. Sustainability: Smart buildings often incorporate sustainable features, such as renewable energy sources, green roofs, rainwater harvesting systems, and energy-efficient building materials, to reduce their environmental impact and improve their carbon footprint. Smart Lighting: Smart buildings have advanced lighting systems that can automatically adjust brightness and color temperature based on natural light, occupancy, and time of day.  They can also be controlled remotely, and can turn off lights when a room is unoccupied to reduce energy wastage. Security: Smart buildings have advanced security systems that can monitor access control, detect intrusions, and alert security personnel in case of emergencies.  These systems can also integrate with other building systems to provide a seamless security experience. Data Analytics: Smart buildings can collect and analyze data from various sensors and devices to identify patterns, optimize building performance, and detect anomalies.  This data can also be used to improve building design, energy efficiency, and occupant comfort. ",1
608,"A smart card, also known as an integrated circuit card (ICC) or a chip card, is a small plastic card with a built-in microprocessor chip that stores and processes data.  Smart cards are used for a variety of applications, such as payment, identification, access control, and transportation. Smart cards have several advantages over traditional magnetic stripe cards, such as:Enhanced Security: Smart cards provide a higher level of security than magnetic stripe cards as the microprocessor chip is capable of storing and processing encrypted data.  This makes it more difficult for fraudsters to copy or tamper with the data stored on the card. Multiple Applications: Smart cards can store and process multiple types of data, such as personal information, financial data, and access credentials.  This makes it possible to use a single card for different applications, such as payment, identification, and access control. Offline Authentication: Smart cards can perform offline authentication, which means that the card can verify the identity of the user without the need for an online connection.  This makes smart cards ideal for applications such as access control and transportation. Convenient and Portable: Smart cards are small and lightweight, making them easy to carry around.  They can also be used with a variety of devices, such as card readers, point-of-sale terminals, and mobile phones. Some examples of smart card applications include credit and debit cards, national ID cards, health cards, transit cards, and access control cards. ",1
609,"Smart cards require specialized hardware to function properly.  The hardware required for smart cards includes:Card Reader: Smart cards need a reader to interact with the card.  The reader communicates with the chip on the smart card and performs the necessary operations. Interface: The card reader needs to have an interface that supports the type of smart card being used.  The interface could be USB, serial, or some other standard. Smart Card Chip: The chip on the smart card contains the necessary memory and processing power to store and execute programs.  The chip could be a microcontroller or a memory chip, depending on the type of smart card. Security Features: Smart cards often have additional security features, such as encryption and authentication, to ensure that data is protected.  The card reader must be capable of supporting these features. Overall, the hardware required for smart cards is specialized and requires careful consideration when implementing a smart card system. ",1
610,"There are many smartphone manufacturing companies and operating systems available in the market, but some of the major ones include:Apple: Apple is a US-based company that manufactures iPhones and uses the iOS operating system. Samsung: Samsung is a South Korean company that manufactures smartphones and tablets, and uses the Android operating system. Huawei: Huawei is a Chinese company that manufactures smartphones and tablets, and uses the Android operating system. Xiaomi: Xiaomi is a Chinese company that manufactures smartphones, tablets, and other electronic devices, and uses the Android operating system. Oppo: Oppo is a Chinese company that manufactures smartphones and uses the Android operating system. Vivo: Vivo is a Chinese company that manufactures smartphones and uses the Android operating system. Google: Google is a US-based company that manufactures the Pixel smartphones and uses the Android operating system. OnePlus: OnePlus is a Chinese company that manufactures smartphones and uses the Android operating system. LG: LG is a South Korean company that manufactures smartphones and uses the Android operating system. Sony: Sony is a Japanese company that manufactures smartphones and uses the Android operating system. These are just a few examples of the many smartphone manufacturing companies and operating systems available in the market.  There are many other companies and operating systems that are also widely used. ",1
611,"SOAP stands for Simple Object Access Protocol.  It is a messaging protocol used for exchanging structured information between networked applications.  SOAP is a widely used protocol for web services and is often used in enterprise applications. SOAP messages are typically sent using XML (Extensible Markup Language) and can be transmitted over a variety of transport protocols, such as HTTP, SMTP, or JMS.  A SOAP message consists of a header and a body.  The header contains information about the message, such as the encoding used and any security information, while the body contains the actual data being exchanged. One of the key benefits of SOAP is that it allows applications running on different platforms and written in different programming languages to communicate with each other.  This is because SOAP is based on open standards and is independent of any particular platform or programming language. SOAP is often used in enterprise applications, such as those used for financial transactions or supply chain management, where security and reliability are critical.  However, it can be more complex and slower than other web services protocols like REST, which is more commonly used for modern web applications. ",1
612,"The origins of social networking can be traced back to the early days of the internet, when people began using online forums and bulletin board systems (BBS) to connect with others who shared their interests.  These systems allowed users to post messages and share information with others who were interested in similar topics. In the late 1990s, the first social networking sites began to emerge.  These sites were designed to help people connect with each other in new ways, using advanced features like user profiles, messaging systems, and friend lists.  Some of the earliest social networking sites included:Six Degrees: Launched in 1997, Six Degrees was one of the first social networking sites.  It allowed users to create profiles and connect with other users, although it was not very successful and shut down in 2001. Friendster: Launched in 2002, Friendster was one of the first social networking sites to gain widespread popularity.  It allowed users to create profiles, connect with friends, and share photos and other content. MySpace: Launched in 2003, MySpace quickly became one of the most popular social networking sites in the world.  It allowed users to create customized profiles, connect with friends, and share music and other content. Facebook: Launched in 2004, Facebook was initially only available to college students, but quickly grew to become the largest social networking site in the world.  It offered features like user profiles, news feeds, and messaging systems, and quickly became a global phenomenon. Since the early days of social networking, the concept has continued to evolve and expand, with new sites and services emerging to meet the needs of users around the world.  Today, social networking is an integral part of many people's daily lives, with billions of users around the world connecting with each other through a wide range of platforms and services. ",1
613,"Social scientists use a variety of software tools to facilitate the research process.  Some of the commonly used software in social science research include:Statistical Analysis Software: Social scientists use statistical analysis software like SPSS, Stata, R, and SAS to analyze large datasets.  These software tools help researchers to run descriptive statistics, regression analyses, and other statistical tests to test hypotheses and identify relationships between variables. Qualitative Data Analysis Software: Social scientists use qualitative data analysis software like NVivo, Atlas. ti, and MAXQDA to analyze textual data, such as interview transcripts, focus group discussions, and open-ended survey responses.  These software tools help researchers to identify themes, patterns, and trends in the data. Survey and Data Collection Software: Social scientists use survey and data collection software like Qualtrics, SurveyMonkey, and Google Forms to collect data from study participants.  These software tools help researchers to create and administer online surveys and collect data in a structured and organized way. Geographic Information Systems (GIS) Software: Social scientists use GIS software like ArcGIS and QGIS to analyze spatial data.  These software tools help researchers to create maps, analyze spatial patterns, and identify spatial relationships between variables. Network Analysis Software: Social scientists use network analysis software like Gephi and NodeXL to analyze social network data.  These software tools help researchers to identify patterns of interaction between individuals, groups, or organizations. Overall, the software tools used in social science research vary depending on the research question, the type of data being collected or analyzed, and the research methods being used.  Social scientists often use a combination of software tools to complete their research projects. ",1
614,"Software development generally follows a standard process, which can be summarized in the following steps:Requirements Gathering: This is the initial stage of software development, where the requirements for the software are collected from stakeholders, users, and business analysts. Analysis and Design: The gathered requirements are analyzed and the system architecture is designed.  The design is documented in the form of diagrams, flowcharts, or other models. Implementation: The software is developed based on the design using a programming language and development tools. Testing: Once the software is developed, it is tested to ensure that it meets the requirements and works as intended.  Testing may include unit testing, integration testing, and system testing. Deployment: After testing, the software is deployed to the production environment or to the end-users. Maintenance: After the software is deployed, it may require maintenance to fix any bugs or issues that are discovered, as well as to add new features or enhancements. These steps may be iterative, with the development team going back and forth between steps as necessary to refine the software and ensure that it meets the needs of the users and stakeholders. ",1
615,"Sony Corporation, commonly referred to as Sony, is a multinational conglomerate corporation based in Tokyo, Japan.  Founded in 1946, the company is involved in a wide range of businesses, including electronics, gaming, entertainment, financial services, and more. Sony is well-known for its consumer electronics products, including TVs, audio equipment, cameras, and smartphones.  The company is also a major player in the gaming industry, with its PlayStation console series and related games and accessories. In addition to its electronics and gaming businesses, Sony has a significant presence in the entertainment industry.  The company owns multiple film and television studios, as well as music labels and streaming services.  Sony Pictures Entertainment is responsible for producing and distributing a wide range of movies and TV shows, while Sony Music Entertainment is one of the world's largest music companies. Overall, Sony is a large and diverse corporation with a wide range of products and services.  The company has a reputation for innovation and quality, and its products are used by millions of people around the world. ",1
616,"Sony has produced many successful consumer products over the years, including:Sony Walkman - The Walkman revolutionized portable music and was one of Sony's most successful products. PlayStation - Sony's gaming console series has been incredibly successful, with the PlayStation 2 being one of the best-selling consoles of all time. Sony Bravia TV - Sony's range of high-quality televisions has been popular with consumers for many years. Sony Cyber-shot Camera - The Cyber-shot camera line has been popular with consumers looking for high-quality digital cameras. Sony Xperia Smartphone - Sony's smartphone range has been popular with consumers, offering features such as high-quality cameras and water resistance. Sony MDR Headphones - Sony's range of headphones, including the MDR line, has been popular with consumers looking for high-quality audio. Sony Alpha Camera - Sony's range of mirrorless cameras, including the Alpha line, has been popular with professional and amateur photographers alike. Overall, Sony has a long history of producing successful consumer products across a range of industries, with a focus on innovation and quality. ",1
617,"Selection sort is a simple sorting algorithm that works by repeatedly finding the smallest element from the unsorted part of an array and moving it to the beginning of the array.  The algorithm maintains two subarrays: the sorted subarray, which starts as empty, and the unsorted subarray, which starts as the entire array. The algorithm proceeds by finding the smallest element in the unsorted subarray and swapping it with the first element in the unsorted subarray.  This effectively moves the smallest element to the beginning of the array and expands the sorted subarray by one element.  The algorithm then repeats this process for the remainder of the unsorted subarray until the entire array is sorted. In each step of the algorithm, the smallest element in the unsorted subarray is identified and swapped with the first element in the unsorted subarray.  After four steps, the array is sorted in ascending order. Selection sort has a time complexity of O(n^2), which makes it inefficient for large arrays.  However, it is simple to implement and is useful for small arrays or as a building block for more complex sorting algorithms. ",1
618,"Bubble sort is a simple sorting algorithm that repeatedly iterates through an array of elements and compares adjacent elements, swapping them if they are in the wrong order.  The algorithm continues to iterate through the array until no more swaps are necessary, indicating that the array is sorted. In each step of the algorithm, the current element is compared with the next element and swapped if they are in the wrong order.  The algorithm continues to iterate through the array until no more swaps are necessary, indicating that the array is sorted. Bubble sort has a time complexity of O(n^2), which makes it inefficient for large arrays.  However, it is simple to implement and is useful for small arrays or as a building block for more complex sorting algorithms. ",1
619,"Quick sort is a widely-used sorting algorithm that uses a divide-and-conquer strategy to sort an array of elements.  The algorithm works by selecting a ""pivot"" element from the array and partitioning the other elements into two subarrays, according to whether they are less than or greater than the pivot.  The pivot element is then placed between the two subarrays. The partitioning process is repeated recursively on each of the subarrays until the entire array is sorted.  The two subarrays are sorted independently using quick sort. Quick sort has a time complexity of O(n log n) in the average case and O(n^2) in the worst case.  However, it is generally faster than other sorting algorithms for large arrays due to its efficient use of memory and cache.  It is widely used in practice and is often the sorting algorithm of choice for many programming languages and applications. ",1
620,"Insertion sort is a simple sorting algorithm that builds the final sorted array one element at a time.  The algorithm iterates through the array and at each iteration, it takes the current element and inserts it into its correct position in the sorted subarray to the left of it. The algorithm maintains two subarrays: the sorted subarray, which starts as the first element of the array, and the unsorted subarray, which starts as the remaining elements of the array. In each step of the algorithm, the current element in the unsorted subarray is compared with the elements in the sorted subarray until its correct position is found.  The elements in the sorted subarray that are greater than the current element are shifted one position to the right to make room for the current element.  The current element is then inserted into its correct position in the sorted subarray. Insertion sort has a time complexity of O(n^2), which makes it inefficient for large arrays.  However, it is simple to implement and is useful for small arrays or for sorting partially sorted arrays.  It is also an efficient algorithm for sorting small datasets, or for use as a sub-routine in more complex algorithms. ",1
621,"In hash-based sorting algorithms, a hash table is used to store the elements to be sorted.  The keys of the hash table are computed from the values of the elements using a hash function.  The values stored in the hash table can be either the elements themselves, or pointers to the elements. The elements are then sorted by iterating through the hash table and collecting the values in order.  Since hash tables do not maintain any order on their own, the order of the elements collected from the hash table must be determined based on the order of the hash keys.  In some cases, a secondary sorting algorithm may be used to sort the values with the same hash key. One advantage of hash-based sorting algorithms is that they can achieve linear time complexity in the average case, since hash table operations such as insertion and lookup can be performed in constant time on average.  However, the worst-case time complexity is typically higher than other sorting algorithms, since collisions in the hash table can lead to slower performance. ",1
622,"There are several different sound file formats, each with its own advantages and disadvantages depending on the intended use case.  Here are some of the most common sound file formats:WAV (Waveform Audio File Format): This is a standard uncompressed audio format that is commonly used for storing high-quality audio.  It has good sound quality but large file sizes. MP3 (MPEG Audio Layer III): This is a compressed audio format that has become popular for music downloads and streaming.  It is able to reduce file sizes significantly while still maintaining good sound quality. AAC (Advanced Audio Coding): This is another compressed audio format that is commonly used for music downloads and streaming.  It is similar to MP3 but provides better sound quality at lower bit rates. FLAC (Free Lossless Audio Codec): This is a compressed audio format that is designed to provide lossless compression, meaning that the sound quality is not degraded in the compression process.  It is commonly used for storing high-quality audio files. OGG (Ogg Vorbis): This is an open-source compressed audio format that is similar to MP3 but provides better sound quality at lower bit rates.  It is commonly used for music downloads and streaming. AIFF (Audio Interchange File Format): This is an uncompressed audio format that is commonly used on Apple computers.  It provides high-quality sound but has larger file sizes than some other formats. These are just a few examples of the many sound file formats that are available.  The choice of format will depend on the intended use case, the desired sound quality, and the available storage space. ",1
623,"Spam is unsolicited or unwanted electronic messages, such as emails, text messages, or comments on social media, that are sent in bulk to a large number of recipients.  The content of spam messages can vary widely, but they often include advertisements for products or services, phishing scams that attempt to steal personal or financial information, or fraudulent messages that promise a reward or prize. Spam can be sent through a variety of methods, including email, social media platforms, instant messaging services, and text messaging.  It can be a nuisance for individuals and can also cause harm by spreading malware or other malicious software. Many email services and social media platforms have built-in spam filters that attempt to detect and block spam messages.  Users can also take steps to protect themselves from spam by being cautious about providing personal information, avoiding suspicious links or attachments, and reporting spam messages to the appropriate authorities. ",1
624,"Here are some ways to stop the spread of spam:Use spam filters: Many email services and social media platforms have built-in spam filters that can help detect and block spam messages.  Check your settings and ensure that your spam filter is turned on and properly configured. Be cautious with personal information: Do not share personal information, such as your email address or phone number, with unknown or suspicious sources. Do not click on suspicious links or attachments: Spam messages often contain links or attachments that can lead to malware or phishing scams.  Do not click on any links or download any attachments from suspicious sources. Report spam messages: If you receive a spam message, report it to the appropriate authorities or to the platform where the message was received.  Reporting spam can help prevent it from being sent to other users. Use strong passwords: Use strong and unique passwords for all of your online accounts.  This can help prevent hackers from accessing your accounts and using them to send spam messages. Be cautious with social media: Be careful when sharing personal information on social media, as this can be used to send spam messages or other unwanted communications. Keep your software up-to-date: Keep your computer and mobile device software up-to-date with the latest security patches and updates.  This can help protect your device from malware and other security threats that can be used to send spam messages. ",1
625,"Spyware and adware are types of malicious software designed to collect information about a user's online activities without their knowledge or consent. Spyware is a type of software that secretly monitors a user's computer activity, such as web browsing, keystrokes, and email conversations.  The information collected by spyware is often sent to third-party advertisers or other malicious actors for the purpose of targeted advertising or identity theft. Adware, on the other hand, is software that displays unwanted ads on a user's computer.  These ads can appear in pop-up windows, banners, or even as fake system messages.  Adware is often bundled with free software or downloads, and the ads can be difficult to remove without specialized software. Both spyware and adware are considered harmful to computer systems and user privacy, and it's important to take steps to prevent and remove them.  This includes using anti-virus and anti-malware software, avoiding suspicious downloads, and keeping software up-to-date with the latest security patches. ",1
626,"There are several steps you can take to prevent spyware from infecting your computer or to stop it if it's already installed:Install reputable anti-spyware software: Use anti-spyware software from reputable vendors and keep it updated regularly.  This software can detect and remove spyware from your computer. Keep your software updated: Keep your operating system, web browser, and other software updated with the latest security patches.  This can help prevent spyware from exploiting vulnerabilities in your software. Be cautious of downloading software: Only download software from reputable sources and avoid downloading software from unknown websites or peer-to-peer networks. Use a firewall: Use a firewall to prevent unauthorized access to your computer and block suspicious incoming connections. Be cautious of email attachments: Be cautious of email attachments, especially if they're from unknown senders or contain suspicious file extensions. Practice safe browsing: Be cautious of clicking on pop-up ads or suspicious links, especially if they're offering free software or services. Regularly scan your computer: Regularly scan your computer with anti-spyware software to detect and remove any spyware that may be present. By following these steps, you can help protect your computer from spyware and keep your personal information safe. ",1
627,"In computer science, a stack is an abstract data type that represents a collection of elements, where the elements are stored and retrieved based on a last-in, first-out (LIFO) principle. This means that the most recently added element is the first one to be removed.  Think of a stack of plates; you can only remove the top plate, and to add another plate, it must be placed on top. A stack has two main operations: push and pop.  The push operation adds an element to the top of the stack, while the pop operation removes the most recently added element from the top of the stack.  Other operations on a stack can include peek, which retrieves the most recently added element without removing it, and is_empty, which checks if the stack contains any elements. Stacks are commonly used in programming, especially for implementing recursive algorithms, maintaining a history of function calls, and evaluating expressions.  They are also used in many other applications, such as compilers, operating systems, and web browsers. ",1
628,"Streaming refers to the continuous delivery of audio or video content over the internet to a user's device, allowing the user to access and enjoy the content in real-time without having to download it first. With streaming, the content is sent in small chunks over the internet, allowing the user to start playing the content immediately while the remaining parts are being downloaded in the background.  This is in contrast to downloading a file, where the user must wait for the entire file to be downloaded before they can start playing it. Streaming is made possible by a combination of server-side technology and client-side software.  The server-side technology (often called a streaming server) is responsible for sending the content to the client in small chunks.  The client-side software (such as a web browser, media player, or mobile app) is responsible for receiving and playing the content as it arrives. Streaming is widely used for a variety of applications, including music and video services, live events, webinars, online gaming, and more.  Streaming allows for high-quality content to be delivered to a global audience in real-time, without the need for users to wait for the entire file to download. ",1
629,"Sun Microsystems was a technology company that was founded in 1982 and specialized in developing computer hardware, software, and network solutions.  The company's products included workstations, servers, storage systems, and software, and it was particularly known for its Unix-based operating system, Solaris. In addition to its hardware and software products, Sun Microsystems was also a pioneer in network computing and was a major contributor to the development of the Java programming language.  Java is an object-oriented language that is designed to be platform-independent, meaning that it can run on any system that has a Java Virtual Machine (JVM) installed. Sun Microsystems was acquired by Oracle Corporation in 2010, and many of its products and technologies have since been integrated into Oracle's offerings.  However, Sun Microsystems' legacy lives on, particularly in the continued use of Java and Solaris in various industries, including finance, telecommunications, and healthcare. ",1
630,"Supercomputers are powerful computing systems that are capable of processing massive amounts of data and performing complex computations at a very high speed.  Due to their advanced computing capabilities, supercomputers are used in a wide range of applications, including:Scientific research: Supercomputers are widely used in scientific research to simulate complex phenomena, such as climate change, nuclear reactions, and the behavior of materials under extreme conditions. Aerospace and defense: Supercomputers are used to simulate aerodynamic and hydrodynamic flows, as well as to design and test new aircraft and spacecraft. Energy: Supercomputers are used to model and simulate complex systems related to energy production and distribution, such as nuclear reactors, wind farms, and smart grids. Medical research: Supercomputers are used in medical research to model complex biological systems, such as the human brain, and to develop new drugs and treatments. Financial modeling: Supercomputers are used in financial modeling to simulate complex financial systems and to develop risk management strategies. Data analytics: Supercomputers are used to process and analyze massive amounts of data, such as weather data, social media data, and financial data. Artificial intelligence and machine learning: Supercomputers are used to train complex machine learning models and to develop advanced artificial intelligence algorithms. Overall, supercomputers play a crucial role in advancing scientific research, technological innovation, and economic growth. ",1
631,"Supply chain management (SCM) refers to the coordination and management of all the activities involved in the production, procurement, and delivery of products or services.  Effective supply chain management helps organizations to streamline their operations, reduce costs, and improve customer satisfaction.  The following are the key activities involved in supply chain management:Planning: This involves forecasting demand, developing production plans, and determining inventory levels. Sourcing: This involves identifying suppliers, negotiating contracts, and managing supplier relationships. Procurement: This involves purchasing raw materials, components, and other inputs needed for production. Production: This involves transforming raw materials and other inputs into finished products or services. Inventory management: This involves managing inventory levels to ensure that there is enough stock to meet demand without incurring excess inventory costs. Logistics and transportation: This involves managing the transportation of goods and services from suppliers to the organization and from the organization to customers. Distribution: This involves managing the distribution of finished products or services to customers. Customer service: This involves ensuring that customers are satisfied with the products or services they receive and addressing any issues that arise. Performance measurement: This involves tracking key performance indicators, such as inventory turnover, order fulfillment rates, and on-time delivery rates, to evaluate the effectiveness of the supply chain management system and identify areas for improvement. Overall, effective supply chain management involves close coordination and collaboration among all stakeholders, including suppliers, manufacturers, logistics providers, and customers, to ensure that products or services are delivered efficiently, cost-effectively, and with high quality. ",1
632,"A system administrator, also known as a sysadmin, is a professional responsible for the maintenance, configuration, and operation of computer systems and networks.  Their primary role is to ensure that the organization's computer systems are running smoothly and efficiently, and to troubleshoot any issues that arise. The responsibilities of a system administrator may include:Installing and configuring computer hardware and software. Managing servers and computer networks. Ensuring the security and privacy of computer systems and data. Managing user accounts and access permissions. Creating and maintaining system backups. Monitoring system performance and identifying and resolving performance issues. Managing and responding to system alerts and error messages. Providing technical support and assistance to users. Performing system upgrades and patches. Conducting regular system maintenance activities, such as disk defragmentation and virus scans. System administrators play a crucial role in ensuring that an organization's computer systems are secure, reliable, and up-to-date.  They work closely with other IT professionals, such as network engineers and database administrators, to ensure that all aspects of the organization's IT infrastructure are working together seamlessly. ",1
633,"A systems analyst is a professional who analyzes an organization's business and information systems to design solutions that meet the organization's needs.  They work with stakeholders to identify business problems and opportunities and develop information technology (IT) solutions that improve business processes and achieve organizational goals. The responsibilities of a systems analyst may include:Conducting research and gathering information on business processes and requirements. Developing functional and technical specifications for IT solutions. Identifying and evaluating alternative solutions and recommending the most effective course of action. Designing and developing IT systems and applications. Testing and validating IT solutions to ensure they meet requirements. Documenting system designs, procedures, and user manuals. Providing support and training to end-users of IT systems and applications. Managing IT projects and ensuring that they are delivered on time and within budget. Systems analysts are essential for ensuring that an organization's IT systems and applications are aligned with the organization's strategic goals and objectives.  They work closely with business stakeholders to understand their needs and translate those needs into effective IT solutions.  Systems analysts must have a solid understanding of both business processes and IT systems, as well as strong analytical and problem-solving skills. ",1
634,"System programming refers to the development of software that controls and manages computer hardware and system resources, such as memory, CPU, and input/output (I/O) devices.  System programmers create software that interacts with the underlying system and hardware to perform low-level tasks, such as device drivers, operating system kernels, and system utilities. System programming involves developing software that can interact with computer hardware directly and manage system resources efficiently.  Some of the key areas of system programming include:Device drivers: System programmers develop device drivers that allow the operating system to interact with various hardware devices, such as printers, scanners, and graphics cards. Operating system kernels: System programmers develop operating system kernels, which are the core components of an operating system that manage system resources, such as memory, CPU, and input/output (I/O) devices. System utilities: System programmers develop system utilities that allow users to manage and monitor system resources, such as disk space, memory usage, and CPU usage. Embedded systems: System programmers develop software for embedded systems, such as smart appliances, industrial control systems, and automotive systems. System programming requires strong knowledge of computer architecture, operating systems, and programming languages such as C and assembly language.  It is a complex and challenging field that requires attention to detail and the ability to work with low-level hardware and system resources. ",1
635,"A tablet PC is a type of mobile computer that is designed to be used with a touch screen interface.  Tablet PCs are typically smaller and more portable than traditional laptops, and they are often used for tasks such as browsing the internet, checking email, watching videos, and playing games. Tablet PCs are distinguished from traditional laptops by their form factor and input methods.  Most tablet PCs are designed to be used with a touch screen, which allows users to interact with the device by tapping, swiping, and pinching the screen.  Many tablets also include additional input methods such as stylus pens or keyboards. Tablet PCs are powered by a range of operating systems, including iOS, Android, and Windows.  They typically include built-in Wi-Fi and/or cellular connectivity, as well as cameras and other sensors.  Some tablet PCs are designed to be used with a detachable keyboard, allowing users to switch between a tablet and a laptop form factor as needed. Overall, tablet PCs are a popular choice for users who need a portable and versatile computing device that can be used for a range of tasks.  They are particularly popular for tasks such as reading, browsing the internet, and consuming media, and they are often used as a complement to traditional desktop and laptop computers. ",1
636,"An internet tablet is a type of mobile device that is designed primarily for internet and multimedia use.  It typically features a large touchscreen display, Wi-Fi connectivity, and various multimedia capabilities, such as music and video playback, as well as web browsing, email, and social media apps. Internet tablets are often smaller and more portable than traditional laptops or desktop computers, making them ideal for on-the-go use.  They typically run on mobile operating systems such as Android, iOS, or Windows, and may offer access to app stores where users can download and install various applications to expand the device's functionality. Internet tablets may also feature additional hardware components, such as cameras, speakers, and microphones, which allow for video calls and other interactive multimedia experiences. ",1
637,"Tape drives are a type of data storage technology that use magnetic tape to store and retrieve digital information.  The first tape drives were developed in the early 1950s as a means of storing large amounts of data in a cost-effective and compact manner. The early tape drives were large, cumbersome devices that were primarily used by businesses and government organizations for data backup and archival purposes.  The tapes themselves were made of a thin strip of plastic coated with a magnetic material that could be read and written to by the tape drive's read/write head. Over time, tape drives became smaller and more reliable, with increased storage capacities and faster read/write speeds.  They remained popular in the business and government sectors, where data backup and archival requirements were often high, and the cost per gigabyte of storage was lower than alternative technologies like hard disk drives. In the late 1990s and early 2000s, tape drives faced competition from newer storage technologies like hard disk drives and solid-state drives.  However, tape drives have continued to evolve, with the introduction of technologies like Linear Tape-Open (LTO) and Advanced Intelligent Tape (AIT), which have increased storage capacities and improved performance. Today, tape drives remain a popular choice for businesses and organizations that require large-scale data backup and archival capabilities, particularly in industries like healthcare, finance, and government, where data retention and compliance requirements are strict. ",1
638,"Tape drives have long been a popular backup device due to their reliability, cost-effectiveness, and large storage capacities.  Here are some key points to keep in mind about using tape drives as a backup device:Large storage capacity: Tape drives can store large amounts of data, often up to multiple terabytes per tape.  This makes them an ideal choice for backing up large data sets, such as databases, file servers, or multimedia content. Cost-effectiveness: Compared to other backup devices like hard disk drives or solid-state drives, tape drives are generally more cost-effective, especially at scale.  Tapes themselves are relatively inexpensive, and the cost per gigabyte of storage is typically lower than other media types. Portability: Tapes are easy to transport and store offsite, making them a good option for disaster recovery scenarios or situations where data needs to be restored in a different location. Durability: Tapes are designed to be durable and can withstand extreme temperatures and humidity levels.  This makes them a good option for long-term data storage and archiving. Compatibility: Tape drives are compatible with a wide range of backup software and operating systems, making them a flexible option for different backup environments. However, there are some drawbacks to using tape drives as a backup device.  One of the main concerns is the slow read/write speeds of tape drives compared to other backup technologies.  Additionally, restoring data from tape can be a time-consuming process, especially for large data sets.  Finally, tapes can also be susceptible to damage or degradation over time, which can lead to data loss or corruption if not properly maintained. ",1
639,"TCL, which stands for Tool Command Language, is a scripting language that is used primarily for rapid prototyping, automation, and embedding into other applications.  Here are some of the extensions and applications of TCL:Tk - Tcl/Tk is a popular combination of scripting language and graphical user interface (GUI) toolkit that is used for creating cross-platform applications.  Tk provides a set of widgets, such as buttons, menus, and text boxes, that can be easily integrated into a Tcl script to create a GUI. Expect - Expect is an extension to Tcl that provides a way to automate interactive applications, such as Telnet, SSH, and FTP.  With Expect, scripts can be written to interact with these applications, entering commands and responding to prompts automatically. SQLite - SQLite is a popular relational database management system (RDBMS) that can be embedded into other applications.  Tcl provides an interface to SQLite, allowing it to be used as a database engine in Tcl scripts. Apache HTTP Server - Tcl can be used to create server-side scripts that can be executed by the Apache HTTP Server.  This allows Tcl to be used to create dynamic web pages and web applications. NS-2 - NS-2 is a popular network simulation tool that is used to simulate and analyze network protocols and architectures.  Tcl is used as the scripting language for NS-2, providing a flexible and powerful way to configure and run simulations. Rapid prototyping - Tcl's ease of use and flexibility make it a popular choice for rapid prototyping and scripting tasks.  It is often used to create quick and dirty scripts for testing and experimentation. Automation - Tcl's scripting capabilities make it a good choice for automating repetitive tasks, such as system administration and testing. Overall, Tcl's flexibility, ease of use, and powerful scripting capabilities make it a popular choice for a wide range of applications, from rapid prototyping and automation to web development and network simulation. ",1
640,"The header of a TCP/IP packet contains information that is used to ensure that data is transmitted reliably and efficiently between network devices.  Here are the main fields that are included in the header of a TCP/IP packet:Source port - This field identifies the port number on the sender's device that is being used to transmit the data. Destination port - This field identifies the port number on the recipient's device that the data is being sent to. Sequence number - This field contains a unique sequence number that is assigned to each segment of data that is transmitted.  The sequence number is used to ensure that data is transmitted in the correct order and to detect any missing or duplicate packets. Acknowledgment number - This field is used to acknowledge receipt of the data.  The acknowledgment number contains the next expected sequence number that the recipient is waiting to receive. Data offset - This field specifies the length of the TCP header in 32-bit words.  This field is important because the length of the header can vary depending on the options that are included. Control flags - These are a set of 6 bits that are used to control the flow of data between sender and recipient.  The most important control flags are the SYN (synchronize), ACK (acknowledge), FIN (finish), and RST (reset) flags. Window size - This field specifies the amount of data that the recipient is willing to receive before sending an acknowledgment. Checksum - This field contains a checksum value that is calculated over the entire TCP segment, including the header and data.  The checksum is used to detect any errors that may have occurred during transmission. Urgent pointer - This field is used to indicate that the data in the TCP segment is urgent and should be processed immediately. Overall, the information in the TCP/IP header is critical to ensuring that data is transmitted reliably and efficiently between network devices. ",1
641,"The support section of most software companies offers various services to assist users in resolving issues with their products.  Here are some of the common services that are offered:Technical support - This includes services to assist users with technical issues they are experiencing with the software.  This may include installation and setup issues, troubleshooting, and bug reports. Customer service - This involves services to assist customers with general questions, billing and payment issues, and other non-technical inquiries. Knowledge base - A knowledge base is a collection of articles, guides, and tutorials that provide users with self-help resources to resolve common issues and questions. Community forums - Community forums are online platforms where users can interact with each other to share tips, solutions, and advice.  Software companies often have official forums where users can ask questions and get help from the company's staff or other users. Training and education - Some software companies offer training and education resources to help users learn how to use the software effectively.  This may include online tutorials, videos, or live training sessions. Software updates and upgrades - Companies may offer software updates and upgrades to fix bugs and improve the functionality of the software. Remote assistance - Some companies offer remote assistance services, where support staff can remotely access the user's device to troubleshoot and resolve issues. Overall, the support section of software companies offers a variety of services to assist users in resolving issues and improving their experience with the software. ",1
642,"Trade books are books that are intended for the general public and are sold through bookstores, online retailers, and other outlets.  They are not typically used as textbooks or for academic purposes.  Instead, they are written for a broad audience and cover a wide range of topics, including fiction, non-fiction, memoirs, and biographies. Trade books are often published by trade publishers, which are companies that specialize in publishing books for a general audience.  They are distinct from academic or scholarly publishers, which focus on publishing books for a specialized audience such as academics, researchers, or professionals. Trade books are typically marketed and sold through various channels, such as bookstores, online retailers, and direct-to-consumer sales.  They may be sold in hardcover or paperback formats, and may be available in e-book or audiobook formats as well. Some examples of popular trade books include novels such as Harry Potter and The Hunger Games, non-fiction books like The Immortal Life of Henrietta Lacks and The 7 Habits of Highly Effective People, and memoirs such as Becoming by Michelle Obama and Educated by Tara Westover. Overall, trade books are an important part of the publishing industry and are a popular form of entertainment and education for the general public. ",1
643,"In-house documentation refers to the process of creating, maintaining, and updating internal documents within an organization.  These documents can include policies, procedures, guidelines, manuals, and other resources that are used by employees to perform their jobs effectively. In-house documentation is essential for any organization because it provides a consistent framework for how work is done and helps to ensure that everyone is on the same page.  It also helps to ensure that best practices are followed, and that employees have the information and resources they need to do their jobs well. Examples of in-house documentation include employee handbooks, training manuals, operational procedures, and quality control guidelines.  These documents are typically created by internal staff, such as human resources, training departments, or subject matter experts, and are designed to meet the specific needs of the organization. In-house documentation may be stored in a variety of formats, such as paper documents, electronic files, or cloud-based systems.  It is important to ensure that these documents are up-to-date and accessible to all employees who need them. Overall, in-house documentation is a critical component of organizational success, providing a consistent framework for how work is done and ensuring that employees have the resources they need to perform their jobs effectively. ",1
644,"Privacy and data protection: As technology becomes more prevalent in our lives, concerns about the collection and use of personal data have become more significant.  Issues related to privacy and data protection include data breaches, surveillance, data ownership, and the use of personal data for advertising purposes. Cybersecurity: As more businesses and individuals store sensitive information online, cybersecurity has become a major policy issue.  Cybersecurity concerns include hacking, ransomware attacks, and the protection of critical infrastructure from cyber threats. Net neutrality: This policy issue relates to the principle that all internet traffic should be treated equally, without discrimination or favoritism towards certain websites or services.  The debate around net neutrality has centered on whether internet service providers should be allowed to charge for faster access to certain websites or services. Intellectual property: With the growth of digital content, including music, movies, and books, the protection of intellectual property has become a significant policy issue.  This includes debates around copyright law, fair use, and piracy. Access to technology: There is a growing concern about the digital divide and access to technology.  Issues related to access to technology include the availability of broadband internet, access to devices, and the cost of technology. Artificial intelligence and automation: As artificial intelligence and automation technologies become more prevalent, there are concerns about the impact on employment and the potential for bias in decision-making processes. E-commerce and consumer protection: With the growth of e-commerce, there are concerns about consumer protection, including online fraud, identity theft, and scams. Globalization and digital trade: With the growth of digital trade, there are policy issues related to trade agreements, intellectual property protection, and cross-border data flows. Overall, these are some of the major policy issues involving information technology industries that are currently being debated and addressed by governments and other stakeholders around the world. ",1
645,"Deregulation can have positive effects such as increased competition and innovation, and negative effects such as reduced consumer protection, job losses, and increased environmental impact.  Its impact depends on the industry and context. ",1
646,"Telecommuting, also known as teleworking or remote work, refers to the practice of working from a location other than a traditional office or workplace, such as from home or a remote location, using technology to communicate with colleagues and complete job tasks. Telecommuting has become increasingly popular in recent years due to advances in technology that have made remote work more feasible and efficient.  Some of the benefits of telecommuting include:Increased flexibility: Telecommuting allows workers to work from anywhere with an internet connection, providing greater flexibility and control over their work schedules. Cost savings: Telecommuting can save workers money on transportation, parking, and other costs associated with commuting to an office or workplace. Increased productivity: Telecommuting has been shown to increase productivity in some cases, as workers are often less distracted and can focus more on their work tasks without the distractions of a traditional office environment. Better work-life balance: Telecommuting can help workers achieve a better work-life balance, as it allows them to spend more time with family and pursue personal interests while still meeting work obligations. Reduced environmental impact: Telecommuting can help reduce the environmental impact of commuting, as it reduces the number of cars on the road and the associated emissions. While there are many benefits to telecommuting, it is important to note that it may not be suitable for all types of jobs or workers.  It can also pose challenges in terms of communication, collaboration, and maintaining a sense of community among remote workers. ",1
647,"While telecommuting or remote work has many advantages, there are also some potential disadvantages that may affect both employees and organizations.  Some of the disadvantages of telecommuting include:Lack of social interaction: Telecommuting can lead to a lack of social interaction, which can be challenging for some employees who may feel isolated and disconnected from their colleagues. Reduced productivity: Telecommuting may lead to reduced productivity, particularly if employees are not disciplined enough to stay on task or if they lack access to the necessary resources and tools. Difficulty with team collaboration: Collaborating with team members can be challenging when working remotely, particularly if there are time zone differences, and communication and coordination can be slower. Difficulty separating work and personal life: Telecommuting can make it difficult to separate work and personal life, which can lead to burnout and stress. Limited career growth: Telecommuting may limit career growth opportunities for employees, particularly if they are not able to network and build relationships with colleagues and managers. Dependence on technology: Telecommuting requires access to and dependence on technology, which can be challenging for some employees who may not be as comfortable with technology or who may not have access to high-speed internet or a reliable computer. Overall, telecommuting has its advantages and disadvantages, and it is up to individuals and organizations to determine if it is a good fit for them. ",1
648,"Telerobotics is a technology that enables the control of a robot from a remote location using telecommunication networks such as the internet or satellite links.  The word ""tele"" in telerobotics refers to the use of remote communication technology to control the robot. Telerobotics is used in a variety of fields, including space exploration, medicine, and manufacturing.  For example, telerobotic surgery allows a surgeon to operate on a patient in a different location using a robot, while telerobotic manufacturing allows a worker to control machinery from a remote location. The use of telerobotics has several advantages, including increased safety, reduced costs, and the ability to operate in hazardous or inaccessible environments.  However, it also has some disadvantages, such as potential delays in communication and limited tactile feedback for the operator. ",1
649,"A template is a pre-designed format or layout that serves as a starting point for creating a new document, project, or website.  It is essentially a pre-built framework that contains placeholders for text, images, and other elements that can be customized to create a finished product. Templates can be found in many different contexts, including word processing software, graphic design tools, and website builders.  For example, a word processing template might include pre-set margins, fonts, and styles for a specific type of document, such as a resume or a letter.  A graphic design template might include a pre-made layout for a business card or a flyer.  A website template might include pre-built pages, menus, and widgets that can be customized to create a complete website. Using templates can save time and effort, as the basic structure and formatting are already in place.  They can also help ensure consistency and professionalism across multiple documents or projects. ",1
650,"Text editors are software applications used for creating and editing plain text files.  Here are some common uses of text editors:Writing and editing code: Text editors are often used by software developers to write and edit code for programming languages such as HTML, CSS, JavaScript, Python, and many others. Creating and editing configuration files: Many software applications use configuration files to store settings and preferences.  Text editors are often used to create and modify these files. Writing and editing documentation: Technical writers often use text editors to create and edit documentation, such as user manuals, technical guides, and other types of instructional materials. Taking notes: Text editors can be used to take notes and store them in plain text format, which can be easily searched and organized. Writing and editing scripts: System administrators often use text editors to create and modify shell scripts, which are used to automate various tasks on Linux and other Unix-like operating systems. Collaborative writing: Text editors can be used for collaborative writing, where multiple authors can work on the same document simultaneously, making it easier to collaborate and share ideas. Overall, text editors are versatile tools that can be used for a wide range of tasks related to writing and editing plain text files. ",1
651,"Instant messaging and texting are both forms of communication that allow individuals to send and receive messages in real-time.  They share some similarities, including:Real-time communication: Both instant messaging and texting allow people to communicate in real-time.  This means that messages are sent and received instantly, enabling a faster exchange of information. Informal communication: Both instant messaging and texting are generally considered to be informal forms of communication.  They are often used for quick, casual conversations that do not require a more formal communication channel. Multimedia support: Many instant messaging and texting apps support multimedia, such as images, videos, and audio files.  This allows users to share a wider range of content beyond just text. Mobile compatibility: Both instant messaging and texting are designed to work on mobile devices, such as smartphones and tablets, making them easily accessible to a wide range of users. Group chats: Instant messaging and texting both support group chats, enabling users to communicate with multiple people at once. However, there are also some differences between instant messaging and texting.  Instant messaging apps are typically more feature-rich than texting, with more advanced functionalities like video calling, voice calling, and screen sharing.  Instant messaging apps also often require an internet connection to work, whereas texting can often work using only a cellular connection. Furthermore, instant messaging apps usually require users to create an account and sign in, while texting does not require any additional setup beyond having a mobile phone number.  Additionally, instant messaging apps are typically used to communicate with people across different platforms and geographical locations, while texting is usually limited to communicating with people within the same country. ",1
652,"Texting and instant messaging have had a significant cultural impact since their inception.  Here are some of the ways in which they have influenced our culture:Changed communication norms: Texting and instant messaging have become a preferred mode of communication for many people, especially younger generations.  This has led to changes in communication norms, with people becoming more comfortable with using abbreviations, emojis, and other forms of shorthand. Increased efficiency: Texting and instant messaging have made it easier and faster to communicate with others, especially in situations where a phone call or in-person conversation is not possible or convenient. Created new social etiquette: Texting and instant messaging have created new social etiquette norms around things like response time, the appropriateness of certain types of messages, and the use of emojis and other symbols. Impacted language: The shorthand and informal language used in texting and instant messaging have had an impact on the way people speak and write, with some of these terms and phrases becoming more widely accepted in everyday language. Changed dating and relationships: Texting and instant messaging have become a significant part of dating and romantic relationships, with many couples using these methods to communicate and stay connected. Increased accessibility: Texting and instant messaging have made it easier for people with disabilities, such as those who are deaf or hard of hearing, to communicate with others. Overall, texting and instant messaging have had a significant impact on our culture, influencing the way we communicate, interact, and express ourselves. ",1
653,"A touchscreen is an input device that allows a user to interact with a computer or other electronic device by touching the screen with their finger or a stylus.  Touchscreens can be found in a wide range of devices, including smartphones, tablets, laptops, interactive kiosks, and even some car dashboards. There are several types of touchscreens, including:Resistive touchscreens: These touchscreens work by applying pressure to the surface of the screen, which triggers a response.  They are typically made up of multiple layers, including a flexible top layer and a rigid bottom layer, with a gap between them that contains conductive material.  When pressure is applied to the top layer, it comes into contact with the conductive material, which triggers a response. Capacitive touchscreens: These touchscreens work by detecting changes in electrical charges on the screen.  They are typically made up of a single layer of conductive material, such as indium tin oxide, which is coated onto a glass surface.  When a user touches the screen, their finger or stylus changes the electrical charge at that point, which is detected by the screen's sensors and triggers a response. Infrared touchscreens: These touchscreens use infrared sensors to detect the position of a user's finger or stylus on the screen.  The screen emits a grid of infrared beams across the surface, and when a user touches the screen, their finger or stylus interrupts the beams, which is detected by the sensors and triggers a response. Surface acoustic wave touchscreens: These touchscreens use ultrasonic waves to detect the position of a user's finger or stylus on the screen.  The screen emits ultrasonic waves across the surface, and when a user touches the screen, their finger or stylus absorbs some of the waves, which is detected by the screen's sensors and triggers a response. Touchscreens are becoming increasingly popular due to their ease of use and intuitive interface, and are expected to continue to be a key input device for a wide range of devices in the future. ",1
654,"Transaction processing refers to the process of managing, executing and recording transactions in a computer system or database.  A transaction is a series of operations that must be completed as a single, indivisible unit.  In other words, a transaction is a set of actions that must be performed as a whole, or not at all. Transaction processing systems (TPS) are commonly used in business and finance to manage the flow of data and information.  In these systems, a transaction may involve adding, updating, or deleting records in a database, or executing financial transactions such as processing payments, issuing invoices, and recording receipts. The basic steps of transaction processing include:Begin Transaction: The system initiates a transaction and begins to execute a series of operations. Execute Operations: The system executes the operations or steps required for the transaction. Commit Transaction: If all operations are successful, the transaction is committed, and the system records the changes to the database. Rollback Transaction: If an error occurs during any of the operations, the transaction is rolled back, and the system undoes all of the changes made during the transaction. The use of transaction processing systems ensures data consistency and accuracy, as well as maintaining the integrity of the system.  Additionally, transaction processing systems enable businesses to manage high-volume, complex transactions efficiently and effectively, which helps to minimize errors, reduce processing time, and improve customer satisfaction. ",1
655,"In computer science, a tree is a widely-used data structure that represents a hierarchical structure or an abstract model of a set of elements.  It is called a ""tree"" because it resembles a tree that has a root at the top and branches that spread out in different directions. A tree consists of nodes, which are connected by edges or links.  The topmost node in a tree is called the root, and each node in a tree has zero or more child nodes.  Nodes with no child nodes are called leaves. In a binary tree, each node has at most two child nodes, which are called left and right children.  In contrast, a general tree can have any number of child nodes. Trees are used in a wide range of applications, including computer science, data structures, algorithms, and programming languages.  Some common applications of trees include representing file systems, representing hierarchical data, organizing data for searching and sorting, and implementing decision-making algorithms such as decision trees and game trees. ",1
656,"A binary tree is a type of tree in which each node has at most two child nodes, which are called the left child and right child.  Binary trees are widely used in computer science and data structures to represent hierarchical relationships between data elements. In a binary tree, each node contains a value or key, as well as pointers or references to its left and right child nodes.  The left child node is typically smaller than the parent node, while the right child node is typically larger than the parent node.  This ordering property of binary trees allows for efficient searching and sorting of data. In terms of memory allocation, each node in a binary tree is typically implemented as a struct or a class in programming languages such as C++ or Java.  The struct or class contains fields or members for storing the node's key value, as well as pointers or references to its left and right child nodes. Memory allocation for binary trees is typically done dynamically using heap memory.  When a new node is created, memory is allocated for the node using a memory allocation function such as malloc() in C or new() in C++.  The allocated memory is then used to store the node's key value and pointers or references to its child nodes. When a node is no longer needed, its memory is freed using a memory deallocation function such as free() in C or delete() in C++.  This helps to avoid memory leaks and ensure efficient use of memory. ",1
657,"Typography was computerized through the development of digital typefaces and the creation of software that allowed users to manipulate and design text on a computer. The process of digitizing typography began in the 1950s with the development of the first computer fonts.  These were initially created as bitmap images, where each letter was represented by a grid of pixels.  As technology advanced, more sophisticated font formats were developed, such as PostScript and TrueType, which allowed for scalable and high-quality typography. In the 1980s and 1990s, software such as Adobe Illustrator, Adobe Photoshop, and QuarkXPress made it possible for designers to create and manipulate typography on a computer.  These programs allowed users to select fonts, adjust kerning and tracking, and create layouts with precision. Today, typography continues to be an integral part of digital design, with a wide range of software and tools available for designers and typographers.  The development of variable fonts has also revolutionized typography by allowing for more customizable and responsive typefaces. ",1
658,"Ubiquitous computing, also known as pervasive computing, refers to the concept of integrating computing and communication technologies into everyday objects and environments to create a seamless and interconnected computing experience.  Here are some examples of ubiquitous computing:Smart homes: Home automation systems that control lighting, heating, and security systems, as well as appliances and entertainment devices, through a single interface. Wearable technology: Devices such as smartwatches, fitness trackers, and augmented reality glasses that provide users with real-time information and interact with the environment. Intelligent transportation systems: Technologies such as GPS navigation, traffic management systems, and driver assistance features that enhance safety and efficiency on the roads. Health monitoring systems: Wearable devices that track vital signs and fitness data, as well as remote health monitoring systems that allow healthcare providers to monitor patients' health status in real-time. Smart cities: Urban environments that leverage data and technology to improve public services and enhance the quality of life for residents, such as traffic management, waste management, and public safety systems. Industrial internet of things (IIoT): Networks of interconnected devices and sensors in manufacturing plants and other industrial settings that enable real-time monitoring and control of operations. Ambient computing: Smart environments that use sensors and AI to provide personalized and context-aware experiences, such as smart offices and retail spaces. These are just a few examples of ubiquitous computing, and the possibilities for integrating computing technologies into our everyday lives are virtually limitless. ",1
659,"UNIX is a multi-user, multi-tasking operating system that was developed in the late 1960s and early 1970s at Bell Labs.  The architecture of UNIX can be divided into several layers, each providing different services to the user and to the system. Kernel: The kernel is the core of the operating system, responsible for managing system resources such as memory, CPU time, and input/output devices.  It also provides an interface between applications and the hardware. Shell: The shell is a command-line interface that allows users to interact with the operating system.  It interprets user commands and passes them to the kernel for execution.  There are several types of shells available in UNIX, including the Bourne shell (sh), C shell (csh), Korn shell (ksh), and Bash shell (bash). File system: The file system is responsible for managing files and directories on the storage devices.  UNIX file systems are organized as a hierarchical tree structure, with the root directory (/) at the top and subdirectories branching out from there. Utilities: UNIX provides a set of command-line utilities for managing the operating system and performing various tasks, such as copying files, searching for text in files, and manipulating text files.  These utilities include ls, cp, grep, and sed, among many others. Libraries: UNIX provides a set of standard libraries that can be used by application developers to simplify their coding efforts.  These libraries include the Standard C Library (libc) and the POSIX (Portable Operating System Interface) library. Applications: UNIX supports a wide range of applications, including text editors, compilers, web servers, and database management systems.  Some of the most popular UNIX applications include the vi editor, the gcc compiler, the Apache web server, and the MySQL database management system. Overall, the architecture of UNIX is designed to provide a flexible, modular, and extensible operating system that can be customized to meet the needs of individual users and organizations. ",1
660,"User-created content (UCC) refers to any form of content, such as text, images, audio, and video, that is created by users of a particular platform, website, or app, rather than by the platform or website itself.  UCC is also known as user-generated content (UGC). UCC can take many forms, including blog posts, comments, product reviews, social media posts, forum discussions, videos, podcasts, and more.  Users may create this content for various reasons, such as sharing their opinions, experiences, or expertise with others, expressing themselves creatively, or simply for entertainment purposes. The rise of social media and other online platforms has greatly increased the amount of UCC on the internet.  This type of content can be a powerful marketing tool for businesses, as it can help them engage with their audience and build brand loyalty.  However, UCC also presents challenges, such as the need to monitor content for inappropriate or harmful material, as well as the potential for copyright infringement and legal issues related to ownership and liability",1
661,"There are several alternative options for user interfaces (UI) that can be used in place of the traditional graphical user interface (GUI).  Some of these options include:Command-line interface (CLI): This is a text-based interface where the user types in commands to interact with the system.  It is commonly used in operating systems and programming environments. Natural language interface (NLI): This is an interface that allows users to interact with the system using natural language, such as spoken language or written text.  NLI is often used in voice assistants and chatbots. Gesture-based interface: This is an interface that uses gestures, such as swiping or pinching, to control the system.  It is commonly used in touchscreens and virtual reality systems. Haptic interface: This is an interface that uses touch to provide feedback to the user.  Haptic interfaces can be used in combination with other UIs, such as touchscreens or gesture-based interfaces, to enhance the user experience. Brain-computer interface (BCI): This is an interface that allows users to interact with the system using their brainwaves.  BCI is still in the early stages of development but has the potential to revolutionize how we interact with technology. Each of these alternative UIs has its own strengths and weaknesses, and the choice of interface will depend on the specific use case and the needs of the user. ",1
662,"VBScript, or Visual Basic Scripting Edition, is a scripting language developed by Microsoft that is based on the Visual Basic programming language.  It is designed to be easy to learn and use, and it is often used to automate tasks in Windows environments. VBScript is a lightweight language that can be embedded in HTML pages, Windows applications, and other software products.  It can also be used to write Windows Script Host scripts, which can automate tasks on a Windows machine. VBScript supports a variety of programming constructs, including variables, data types, control structures, functions, and subroutines.  It also has built-in support for a wide range of Windows system functions, such as file and folder manipulation, registry access, and network communication. While VBScript was once a popular scripting language, it has largely been replaced by newer languages such as PowerShell and JavaScript.  However, it is still supported on many Windows systems, and there are many legacy scripts and applications that use VBScript. ",1
663,"Video tapes have undergone significant evolution since their inception in the 1950s.  The three primary types of video tapes are VHS (Video Home System), Betamax, and VHS-C (Compact VHS).  Here is a brief history of the evolution of video tapes:Reel-to-Reel Tapes (1950s - 1960s): The first video tape format was the reel-to-reel tape, which was developed in the 1950s.  These tapes were large, heavy, and expensive, and were primarily used for professional and industrial purposes. Cartridge-Based Systems (1960s - 1970s): In the 1960s, video tapes started to be produced in cartridge-based systems, which were easier to handle and store.  The first popular cartridge-based video tape system was the Sony Betamax, which was introduced in 1975.  Betamax was smaller and higher quality than VHS, but lost out in the market due to a lack of licensing agreements. VHS (1970s - 2000s): VHS tapes were introduced in the 1970s, and quickly became the dominant video tape format for home use.  VHS tapes were larger and lower quality than Betamax, but were cheaper and had more available content.  VHS tapes remained popular throughout the 1980s and 1990s, but started to be replaced by DVDs and digital formats in the early 2000s. VHS-C (1980s - 1990s): In the 1980s, VHS-C tapes were introduced as a compact version of VHS.  VHS-C tapes were smaller and easier to handle than VHS tapes, but required an adapter to play in a standard VHS player. Digital Video Tapes (1990s - Present): In the 1990s, digital video tape formats such as DV and MiniDV were introduced, which offered higher quality and smaller size than VHS tapes.  These formats were primarily used by professionals and enthusiasts, and have largely been replaced by digital formats such as DVD and Blu-ray. Overall, video tapes played a significant role in the evolution of home entertainment, but have largely been replaced by digital formats in the 21st century. ",1
664,"The WELL (Whole Earth 'Lectronic Link) is one of the oldest and most well-known virtual communities on the internet, which was founded in 1985.  It is a text-based online community where people can communicate with each other on various topics, using forums and message boards. The WELL was created with the intention of bringing people together to discuss important issues and ideas.  It was popular among counterculture and tech-savvy individuals, who used it to discuss everything from politics and social issues to computer technology and programming. Unlike many modern social media platforms, the WELL has a strong focus on privacy and security.  Members are encouraged to use pseudonyms rather than their real names, and the community is moderated to ensure that discussions remain civil and respectful. The WELL has a unique culture and history, and it continues to thrive as a vibrant online community today, with members from around the world.  It has influenced many other online communities and platforms that have come after it, and it remains a fascinating example of how the internet can be used to connect people and foster meaningful conversations. ",1
665,"Virtualization is the creation of a virtual version of something, such as an operating system, a server, a storage device, or a network.  Virtualization technology enables multiple operating systems and applications to run on a single physical machine or server, which can offer many benefits.  Here are some applications of virtualization:Server consolidation: Virtualization technology enables multiple virtual servers to run on a single physical server, which can reduce hardware and maintenance costs. Testing and development: Virtualization provides an environment for software developers and testers to test their applications in a controlled and isolated environment without affecting production systems. Disaster recovery: Virtualization enables backup and recovery of entire virtual machines, which can help in disaster recovery scenarios. Cloud computing: Virtualization technology is the foundation for cloud computing, which provides on-demand access to a pool of computing resources. Security: Virtualization can improve security by creating isolated virtual environments that are separated from the host operating system. Education and training: Virtualization technology can be used in educational settings to provide students with access to virtual environments and simulations. Desktop virtualization: Virtualization technology enables desktops to be delivered as a service, which can provide users with secure and consistent access to their desktops from any location. High availability: Virtualization enables the creation of high availability clusters, which can provide continuous availability of critical services and applications. Energy efficiency: Virtualization can reduce energy consumption by consolidating multiple servers onto a single physical machine. These are just a few of the many applications of virtualization technology.  As the technology continues to evolve, we can expect to see new and innovative uses of virtualization in a wide range of industries and applications. ",1
666,"Social virtualization, also known as social VR (virtual reality), is a technology that creates a shared virtual space where users can interact with each other and digital objects in a simulated environment.  This technology creates a sense of presence, allowing users to feel as if they are in the same physical space even if they are geographically located in different parts of the world. Social virtualization is often used for entertainment and socialization, allowing users to meet and interact with each other in virtual environments.  Users can create avatars to represent themselves, which can be customized to their liking.  They can then explore virtual worlds, attend virtual events, and participate in activities such as games, concerts, and virtual tourism. Social virtualization is also used for collaborative work and education, where users can work together on projects, attend meetings, and participate in virtual training sessions.  Social VR platforms can also enable remote teams to communicate and collaborate more effectively, particularly in industries that require 3D visualization and simulation, such as architecture, engineering, and product design. Overall, social virtualization has the potential to provide immersive and engaging experiences for users, allowing them to connect with others and interact with digital content in new and innovative ways.  As the technology continues to develop and improve, we can expect to see new applications and use cases emerge. ",1
667,"Virtual reality (VR) is a technology that creates a simulated environment that can be experienced through a head-mounted display or other immersive devices.  VR has many applications in various industries.  Here are some of the applications of virtual reality:Gaming: Virtual reality gaming provides an immersive experience that allows players to feel as if they are actually in the game.  It can enhance the gaming experience by creating a sense of presence and interaction with the virtual world. Education and Training: VR can provide a safe and controlled environment for training purposes, especially in high-risk professions such as surgery, firefighting, and military training.  It can also provide immersive experiences for educational purposes such as historical and scientific simulations. Healthcare: VR can be used in healthcare to provide immersive therapies and treatments such as exposure therapy for phobias, pain management, and physical therapy. Architecture and Construction: VR can be used in architecture and construction to provide immersive 3D visualizations of buildings and structures, allowing architects and engineers to visualize and simulate designs in real-time. Tourism: Virtual reality can provide an immersive experience of a destination, allowing people to experience a place before visiting it physically.  It can also be used to promote tourism and attract visitors to a destination. Advertising and Marketing: VR can be used in advertising and marketing to create immersive experiences and showcase products in a unique way. Sports and Entertainment: VR can provide an immersive experience of sporting events and live performances, allowing viewers to feel as if they are actually in the event. Military and Defense: VR can be used in military and defense for training purposes, simulations, and mission planning. Overall, virtual reality has the potential to provide immersive and engaging experiences in various industries and applications.  As the technology continues to improve, we can expect to see new and innovative uses of virtual reality emerge. ",1
668,"VoIP stands for Voice over Internet Protocol, which is a technology that allows users to make voice calls over the internet rather than traditional phone lines.  Instead of using a dedicated circuit-switched network, VoIP converts analog audio signals into digital data packets that can be transmitted over the internet. VoIP technology has several advantages over traditional telephone systems.  First, it is typically much cheaper than traditional phone systems, especially for long-distance and international calls.  Second, it allows users to make calls from anywhere with an internet connection, which is especially useful for remote workers and businesses with multiple locations.  Third, it offers a range of features and services such as call forwarding, voicemail, and conference calling, which are often included at no extra cost. There are several ways to use VoIP technology.  One way is to use a software-based service such as Skype, which allows users to make free voice and video calls over the internet.  Another way is to use a dedicated VoIP phone, which looks and works like a traditional phone but uses the internet instead of phone lines to make calls.  Finally, many businesses use VoIP phone systems, which are designed specifically for businesses and can support a large number of users and advanced features such as call routing, auto-attendant, and integration with other business applications. Overall, VoIP technology has revolutionized the way we make phone calls, providing a cheaper, more flexible, and feature-rich alternative to traditional phone systems. ",1
669,"While VoIP (Voice over Internet Protocol) technology has many benefits, there are also some disadvantages that users should be aware of.  Here are some of the main disadvantages of VoIP:Dependence on the Internet: VoIP calls require a reliable and high-speed internet connection to work effectively.  If the internet connection is slow or unreliable, the call quality may suffer, or the call may drop altogether. Power Outages: Unlike traditional phone systems that can continue to work during a power outage, VoIP phones require electricity to function.  If the power goes out, VoIP phones may not work, unless they are equipped with a backup power supply. Emergency Calling: VoIP services may not be as reliable as traditional phone systems when it comes to emergency calling.  In some cases, emergency services may not be able to identify the caller's location accurately, which could delay response times. Compatibility Issues: Some VoIP services may not be compatible with certain devices or operating systems.  Users may need to install additional software or hardware to use VoIP services effectively. Security: VoIP calls are susceptible to security breaches, such as eavesdropping, hacking, and identity theft.  Users should take appropriate security measures, such as using strong passwords and encryption, to protect their calls and data. Quality of Service: VoIP call quality can be affected by various factors, such as network congestion, bandwidth limitations, and packet loss.  The quality of service may vary depending on the internet connection and other factors, which could affect the user experience. Overall, while VoIP technology offers many benefits, it is important to consider the potential disadvantages and limitations when deciding whether to use it as a primary communication method. ",1
670,"Wearable computers are small electronic devices that can be worn on the body or integrated into clothing or accessories.  They are designed to be mobile and can be used to access and process information, communicate, and monitor various health and fitness parameters. Some examples of wearable computers include smartwatches, fitness trackers, augmented reality glasses, and virtual reality headsets.  These devices typically have sensors and connectivity capabilities, allowing them to collect data and communicate with other devices or networks. Wearable computers can be used for a variety of purposes, including personal health and fitness tracking, navigation, communication, entertainment, and work-related tasks.  They offer a convenient and hands-free way to access and interact with digital information, and can also provide real-time feedback and insights into various aspects of the wearer's life. ",1
671,"Several technologies have played a crucial role in enabling the development of Web 2. 0, including:AJAX (Asynchronous JavaScript and XML): AJAX allows for asynchronous communication between a web browser and a web server, allowing for faster and more dynamic web applications. RSS (Really Simple Syndication): RSS is a technology that allows users to subscribe to and receive updates from websites, blogs, and other online content sources. Web Services: Web services provide a standardized way for different applications to communicate and share data over the internet. Rich Internet Applications (RIAs): RIAs are web applications that have the look and feel of desktop applications, with rich media and advanced user interfaces. Social networking and user-generated content tools: These tools enable users to create, share, and interact with content, and to connect with each other in new ways. APIs (Application Programming Interfaces): APIs allow developers to create new applications and services that can interact with existing web platforms and services. Together, these technologies have enabled the development of a more dynamic, interactive, and socially-engaging web experience, and have paved the way for the development of new web-based businesses and services. ",1
672,"Web browsers are software applications that allow users to access and navigate the World Wide Web (WWW) by displaying web pages and other online content.  The following are the main functionalities of web browsers:Rendering: A web browser reads the HTML, CSS, and JavaScript code in a web page and uses it to display the content of the page on the user's device.  This process is called rendering. Navigation: Web browsers allow users to navigate through the web by following hyperlinks and entering URLs.  They also provide a back button, which allows users to return to previously visited pages. Security: Web browsers have security features that protect users from online threats such as phishing scams, malware, and malicious websites.  For example, web browsers may display warnings when a user visits a suspicious website or block downloads from untrusted sources. Extensions: Many web browsers allow users to install extensions, which are small software programs that add extra functionality to the browser.  Examples of extensions include ad blockers, password managers, and productivity tools. Bookmarks: Web browsers allow users to save bookmarks, which are links to frequently visited web pages.  Bookmarks make it easy for users to quickly access their favorite websites. History: Web browsers keep a record of the websites that users have visited, which is known as browsing history.  Users can use their browsing history to revisit pages that they have previously visited. Tabs: Web browsers allow users to open multiple tabs, which are separate instances of the browser window.  Tabs make it easy for users to switch between different web pages without having to open multiple windows. Customization: Web browsers allow users to customize the appearance and functionality of the browser by changing settings such as the homepage, search engine, and theme. ",1
673,"Browsers are used as a platform in several ways, including:Running web applications: Browsers provide an environment in which web applications can run, allowing users to access web-based software like email clients, productivity tools, and content management systems. Rendering multimedia content: Browsers are capable of rendering multimedia content like images, audio, and video, allowing users to consume and interact with this content without the need for separate media players. Supporting web standards: Browsers are built to support web standards like HTML, CSS, and JavaScript, allowing developers to create web applications that can run on any browser without requiring platform-specific code. Enabling cross-device compatibility: Browsers are used to provide a consistent user experience across multiple devices and operating systems, enabling developers to create web applications that can run on desktop and mobile devices alike. Providing access to web services: Browsers allow users to access web services like search engines, online marketplaces, and social media platforms, making them a convenient platform for accessing a wide range of online resources. Overall, browsers serve as a powerful platform for delivering and consuming web-based content and services, making them a crucial component of the modern digital landscape. ",1
674,"A webcam is a type of digital camera designed to capture video and transmit it over the Internet or computer network in real-time.  It is typically built into a laptop or a computer monitor or attached to it via USB cable.  Webcams are commonly used for video conferencing, video calls, online education, online interviews, live streaming, and other similar purposes. Webcams usually come with built-in microphones for capturing audio and can capture video in different resolutions ranging from standard definition (SD) to high definition (HD) and beyond.  They can also have features such as autofocus, low-light correction, and zoom capabilities.  With the increase in remote work and remote learning, webcams have become a popular tool for communicating and collaborating with others over the internet. ",1
675,"Web filters are software programs or services designed to restrict or control access to certain websites or web content on the internet.  These filters can be implemented by individuals, organizations, or governments to enforce their policies, protect their networks or prevent users from accessing harmful or inappropriate content. Web filters can be categorized based on the type of content they block or restrict access to.  Some common types of web filters include:URL filters: These filters block access to specific websites or web pages by matching the URL of the website or page against a predefined list of blocked URLs. Content filters: These filters block access to specific types of content such as pornography, violence, hate speech, gambling, or social media platforms. Keyword filters: These filters block access to websites or pages containing specific keywords or phrases. Time-based filters: These filters restrict access to certain websites or web content during specific times of the day or week. Web filters can be implemented at different levels such as on a personal computer, router, or internet service provider (ISP).  They can also be implemented in schools, libraries, and workplaces to prevent users from accessing certain content during work or study hours. ",1
676,"A webmaster is a person responsible for managing and maintaining a website or group of websites.  This role involves various tasks, including designing and updating website content, ensuring website functionality and performance, monitoring website traffic and user behavior, managing website security and backups, and implementing search engine optimization (SEO) strategies to improve website visibility and ranking on search engine results pages.  The webmaster may also be responsible for managing online advertising campaigns, social media accounts, and other digital marketing efforts related to the website.  The specific responsibilities of a webmaster may vary depending on the size and complexity of the website or websites they manage. ",1
677,"The skill set for a webmaster can vary depending on the specific needs of the organization or website they work for, but some common skills and knowledge areas that are important for a webmaster include:Web design and development: A webmaster should have knowledge of HTML, CSS, and other web development technologies to design and build websites. Content creation and management: A webmaster should be able to create and manage website content such as text, images, videos, and audio. Website maintenance: A webmaster should have knowledge of website maintenance tasks such as backups, updates, and troubleshooting. Search engine optimization (SEO): A webmaster should understand SEO principles and techniques to improve website ranking and visibility in search engine results pages. Analytics and monitoring: A webmaster should have knowledge of website analytics tools and be able to monitor website traffic, user behavior, and website performance. Security: A webmaster should have knowledge of website security best practices to ensure the website is secure and protected against cyber threats. Communication and collaboration: A webmaster should be able to communicate effectively with team members and stakeholders, collaborate on projects, and provide support to users. Marketing and advertising: A webmaster should have knowledge of digital marketing and advertising techniques to promote the website and increase traffic and engagement. Continuous learning: A webmaster should have a passion for continuous learning and staying up-to-date with the latest web technologies, trends, and best practices. ",1
678,"A web server is a software program that runs on a computer and is responsible for serving web content to clients, typically web browsers.  The working of a web server can be explained in the following steps:A client (a web browser) sends a request to the web server for a particular web page or resource, using the HTTP (HyperText Transfer Protocol) protocol.  The request typically contains information such as the type of resource requested, the desired encoding, and any other relevant data. The web server receives the request and analyzes it to determine which resource is being requested, and if it is available on the server.  If the resource is not found, the web server will send a ""404 Not Found"" response to the client. If the resource is found, the web server retrieves it and prepares to send it to the client.  This may involve reading the resource from disk, generating it dynamically using server-side scripting languages like PHP, or retrieving it from a database. The web server sends the resource to the client in the form of an HTTP response.  The response typically includes headers that provide metadata about the resource, such as its content type, length, and encoding. The client receives the response and renders the resource to the user, typically by displaying it in a web browser. Throughout this process, the web server may interact with other servers, databases, or services to fulfill the client's request.  For example, the web server might use a load balancer to distribute requests across multiple servers, or it might authenticate the client's identity using a separate authentication server. Overall, the web server acts as a middleman between the client and the resources it is requesting, handling the low-level details of communication and data transfer to ensure that the client receives the correct content in a timely and efficient manner. ",1
679,"Web services can be accessed using a variety of protocols and programming languages.  The most common way to access web services is through HTTP (HyperText Transfer Protocol), using a variety of HTTP methods such as GET, POST, PUT, and DELETE. To access a web service, you typically need to know its API (Application Programming Interface), which is a set of rules that defines how clients can interact with the service.  The API typically includes information about the URL structure, available HTTP methods, input and output formats, and any authentication or authorization requirements. Here are a few common ways to access web services:Using a web browser: Some web services provide a web interface that you can access using a web browser like Chrome, Firefox, or Safari.  This allows you to interact with the service using a graphical user interface (GUI) rather than writing code. Using a programming language: Many programming languages provide libraries or frameworks for accessing web services, such as Python's requests library or Java's Spring framework.  These libraries typically abstract away the details of HTTP communication, allowing you to focus on writing code to interact with the service. Using a tool like Postman: Postman is a popular tool for testing and interacting with web services.  It allows you to send HTTP requests to a web service, inspect the responses, and save the requests for later use. Using command-line tools: Tools like curl or wget can be used to send HTTP requests from the command line.  These tools are often used in scripts or automation workflows. Overall, accessing web services requires an understanding of the service's API and the tools and libraries available in your programming language or environment.  Once you know how to access a web service, you can use it to retrieve data, perform operations, or integrate it into your own applications. ",1
680,"A wiki is a collaborative website that allows users to create, edit, and link web pages easily.  The structure of a wiki typically consists of the following elements:Pages: A wiki is made up of individual pages, each of which can contain text, images, links, and other media.  Pages can be created, edited, and deleted by users with appropriate permissions. Links: Pages in a wiki are connected through links, which allow users to navigate from one page to another.  Links can be created within a page or between pages, and they often form a web of interconnected information. Navigation: A wiki typically has a navigation system that allows users to browse through pages and find the information they need.  Navigation can take the form of a table of contents, a search bar, or a hierarchical menu. History: One of the defining features of a wiki is its revision history.  Every change made to a page is recorded, allowing users to see who made what changes and when.  This makes it easy to track the evolution of a page over time and to undo mistakes or vandalism. Categories: Categories are a way of organizing pages in a wiki.  Pages can be assigned to one or more categories, which helps users find related content and facilitates maintenance and cleanup of the wiki. Overall, the structure of a wiki is designed to promote collaboration and sharing of knowledge among a community of users.  By allowing anyone to contribute and edit pages, wikis can quickly accumulate a wealth of information and evolve over time to reflect the collective knowledge of their users. ",1
681,"Wireless LAN, or WLAN, is a type of wireless network that uses radio waves to provide wireless high-speed internet and network connections between devices within a limited area such as a home, office, or campus. The working of wireless LAN involves the following steps:Wireless Access Point (WAP) - A WAP is a device that connects wireless devices to a wired network using radio signals.  It serves as a central hub for wireless communication between devices within the network. Wireless Network Interface Cards (NICs) - Wireless NICs are installed in devices such as laptops, smartphones, and tablets to allow them to connect to the wireless network. Radio Waves - Radio waves are used to transmit data between the WAP and the wireless NICs.  The WAP broadcasts radio signals, which are received by the wireless NICs within its range. Network Protocols - WLANs use various network protocols such as the 802. 11 standards to ensure that data is transmitted securely and efficiently between devices. Authentication and Encryption - WLANs use authentication and encryption techniques to protect the network from unauthorized access and data theft.  These techniques include WPA and WPA2 encryption, MAC filtering, and SSID hiding. Range Extenders - In case the range of the WAP is limited, range extenders can be used to increase the range of the wireless network.  Range extenders work by amplifying the radio signals and rebroadcasting them to increase the coverage area. Overall, wireless LANs provide users with the convenience and flexibility of wireless communication while maintaining the security and reliability of wired networks. ",1
682,"The journey of the word processor can be traced back to the mid-1960s, when IBM developed the Magnetic Tape Selectric Typewriter (MT/ST).  This was the first commercially available word processing machine, and it used magnetic tape to store and edit text. In the 1970s, the first true word processors were developed.  These machines, such as the Wang 1200 and the Xerox Alto, used digital technology to store and edit text.  The Wang 1200 was one of the first word processors to feature a CRT display, which allowed users to see what they were typing as they typed it. By the 1980s, personal computers had become more powerful and affordable, and word processing software began to be developed for these machines.  Some of the early popular word processing software included WordStar, WordPerfect, and Microsoft Word. In the 1990s and early 2000s, word processing software became more advanced, with features such as spell-checking, grammar-checking, and desktop publishing tools.  The rise of the internet and email also led to the development of collaborative word processing software such as Google Docs and Microsoft Office 365. Today, word processors continue to evolve and adapt to the changing needs of users.  The software has become more user-friendly, with intuitive interfaces and features that make it easier to create and edit documents.  Word processors are now available as both standalone software applications and cloud-based services, allowing users to access their documents from anywhere and collaborate in real-time with others. ",1
683,"The features of a word processor may vary depending on the software, but some of the common features are:Text formatting: Allows users to format text in various styles, sizes, and colors.  Features include font styles, alignment, spacing, bullet points, numbering, and indentation. Spell and grammar checking: Checks for spelling and grammar errors in the document, and suggests corrections. Search and Replace: Allows users to search for specific words or phrases in the document and replace them with another word or phrase. Headers and Footers: Allows users to add headers and footers to the document, which can contain page numbers, date and time, and other information. Tables and Graphs: Allows users to create tables and graphs to display data and information. Page Layout: Allows users to adjust the page layout of the document, such as margins, orientation, and page size. Collaboration: Allows users to collaborate on a document with others in real-time, with features such as track changes and commenting. Insert media: Allows users to insert images, videos, and other media into the document. Mail Merge: Allows users to create form letters or labels by merging a list of names and addresses with a document. Footnotes and Endnotes: Allows users to add footnotes or endnotes to the document to provide additional information or references. AutoCorrect: Automatically corrects common typos or misspellings as the user types. AutoSave: Automatically saves the document at regular intervals to prevent data loss in case of a power failure or software crash. Overall, the features of a word processor aim to make the creation and editing of documents easier, faster, and more efficient. ",1
684,"Konrad Zuse (1910-1995) was a German engineer and computer pioneer who is credited with building the world's first programmable digital computer.  Zuse's computer, called the Z3, was completed in 1941 and used binary arithmetic and Boolean logic to perform calculations.  It was also the first computer to use punched tape as a means of input and output. Zuse continued to build and develop computers throughout his career, including the Z1, Z2, and Z4 machines.  He also invented the first high-level programming language, called Plankalkül, although it was not widely used during his lifetime. Zuse's work on computers and programming languages was groundbreaking and laid the foundation for the development of modern computing.  He received many awards and honors for his contributions to the field, including the Werner von Siemens Ring in 1964, which is considered one of the most prestigious engineering awards in Germany. ",1
685,"YouTube is a video-sharing website and platform owned by Google.  It allows users to upload and share videos, view and comment on videos posted by others, and search for videos on a wide variety of topics.  YouTube has become one of the most popular websites in the world, with over 2 billion monthly active users and over 1 billion hours of videos watched daily. YouTube was created in 2005 by three former PayPal employees and was acquired by Google in 2006.  The site quickly became a cultural phenomenon, with users uploading videos ranging from home movies and amateur content to professional productions and music videos. In addition to its main video-sharing functionality, YouTube also offers various tools and features for creators, including the ability to monetize their content through advertising revenue, live streaming capabilities, and analytics to track audience engagement and video performance. YouTube has had a significant impact on popular culture, entertainment, and social media, and has become a major platform for news, education, and activism. ",1
686,"YouTube, like all major social media platforms, can be influenced by political actors in a variety of ways.  Here are a few examples of how politics can affect YouTube:Content moderation policies: YouTube's policies on what content is allowed on the platform and how that content is moderated can be influenced by political pressure.  For example, governments may pressure YouTube to remove certain videos or channels that they view as being harmful or threatening to national security. Advertising policies: Political campaigns can use YouTube's advertising platform to target voters with their messages.  YouTube has faced criticism in the past for allowing misleading or deceptive political ads to run on the platform. Algorithmic recommendations: YouTube's recommendation algorithm can influence what videos users see on the platform, which in turn can affect their political views.  Political groups may try to manipulate the algorithm to promote their own videos or suppress those of their opponents. Censorship: In some countries, YouTube is blocked or heavily censored by the government.  This can limit access to information and perspectives that are critical of the ruling party or government. ",1
687,"Yahoo Inc.  is an American multinational technology company that was founded in 1994 by Jerry Yang and David Filo.  Initially, Yahoo started as a web directory but later expanded into a wide range of internet services, including email, search, news, finance, sports, and advertising.  In the late 1990s and early 2000s, Yahoo was one of the most popular websites on the internet and played a significant role in shaping the early internet era. However, Yahoo struggled to keep up with the fast-changing landscape of the internet and faced stiff competition from newer players like Google and Facebook.  In 2017, Yahoo was acquired by Verizon Communications and merged with AOL to form a new company called Oath Inc.  Later, Oath was rebranded as Verizon Media. Today, Yahoo is still one of the most visited websites globally, with over 800 million monthly active users.  Its services include Yahoo Mail, Yahoo News, Yahoo Finance, Yahoo Sports, and Yahoo Search, among others.  However, Yahoo's market share has declined significantly in recent years, and it no longer dominates the internet space like it did in the early days of the web. ",1
688,"Young people and computing have a close relationship, as the digital age has made computing an integral part of everyday life for many young people.  Here are a few ways in which young people engage with computing:Education: Computing is now a core subject in many schools worldwide, and young people are learning how to code, develop apps, and use various software tools.  Many universities and colleges offer degrees in computer science, software engineering, and related fields. Social Media: Young people use social media platforms like Instagram, Facebook, and TikTok to connect with friends and share content.  These platforms also use algorithms to personalize content, and young people learn how to navigate and use these systems to their advantage. Gaming: Gaming is a popular activity among young people, and video games often require technical skills and problem-solving abilities.  Online gaming communities and forums also provide opportunities for young people to learn and collaborate with others. Entrepreneurship: Young people are increasingly starting their own businesses, and computing skills are essential for many startups.  With the rise of e-commerce and digital marketing, young people can create their own websites, sell products online, and market their services to a global audience. Overall, computing skills have become essential for young people in many areas of life, from education to work and leisure activities.  As technology continues to advance, it is likely that computing skills will become even more critical in the future. ",1
689,"Neurality refers to the concept of neural networks, which are a type of artificial intelligence modeled after the structure and function of the human brain. Neural networks are composed of interconnected nodes, called artificial neurons, that process and transmit information.  Each neuron receives input from multiple other neurons and computes a weighted sum of these inputs.  The neuron then passes this sum through a nonlinear activation function to produce an output, which is then transmitted to other neurons.  This process of computing weighted sums and passing the results through activation functions is repeated multiple times, allowing the neural network to build increasingly complex representations of the input data. Neural networks can be trained to perform a wide range of tasks, such as image recognition, language translation, and even playing games like chess.  The training process involves adjusting the weights on the connections between neurons so as to minimize a chosen loss function.  This process is repeated for multiple passes over the training data, until the network has learned to accurately predict the desired output for a given input. ",1
690,"NLP, or Natural Language Processing, is a field of study and technology concerned with the interaction between computers and humans using natural language.  It involves using algorithms and statistical models to analyze and understand human language and to generate text or speech in response. For example, consider a simple NLP application: sentiment analysis.  Sentiment analysis is the task of determining the emotional tone behind a piece of text, such as a movie review.  A sentiment analysis model might take in a movie review written in English and output a label indicating whether the review is positive, negative, or neutral.  The model might make this determination by analyzing the words used in the review and their relationships to each other, as well as other information such as the context in which the words are used. ",1
691,"Multitasking refers to the ability of an individual or a system to perform multiple tasks simultaneously.  In a computing context, multitasking refers to the ability of an operating system to run multiple programs or processes at the same time, allowing the user to switch between them or have multiple programs running in the background. For humans, multitasking can refer to the ability to perform multiple tasks or activities at the same time, such as typing an email while listening to music, or cooking dinner while talking on the phone.  However, research has shown that human multitasking is often not as effective as performing each task separately and in sequence, as multitasking can lead to decreased productivity and increased stress. In both human and computer systems, multitasking can also refer to the ability to quickly switch between tasks and effectively allocate resources to different tasks as needed.  This requires effective management of resources, such as memory and processing power, as well as efficient algorithms for scheduling and switching between tasks. ",1
692,". NET is a software framework developed by Microsoft that provides a platform for building and running a wide range of applications, including desktop applications, web applications, and mobile applications.  The framework is designed to make it easier for developers to build high-quality, secure, and scalable applications for the Windows operating system and other platforms. . NET provides a large set of libraries and tools that developers can use to build their applications.  These libraries cover a wide range of functionality, from basic input/output operations to more complex tasks such as network programming, data access, and multimedia support.  Additionally, the . NET framework provides a common execution environment for all . NET applications, allowing them to share resources and take advantage of optimizations provided by the framework. . NET also includes a runtime environment, called the Common Language Runtime (CLR), which manages the execution of . NET code and provides services such as memory management, security, and exception handling.  The CLR also enables . NET applications to run on multiple platforms and devices, including Windows, Linux, and macOS, as well as iOS and Android for mobile devices. In summary, . NET is a comprehensive software framework that provides developers with a platform for building a wide range of applications, as well as a set of libraries, tools, and a runtime environment to make development easier and more efficient. . NET is a software framework developed by Microsoft that provides a platform for building and running a wide range of applications, including desktop applications, web applications, and mobile applications.  The framework is designed to make it easier for developers to build high-quality, secure, and scalable applications for the Windows operating system and other platforms. . NET provides a large set of libraries and tools that developers can use to build their applications.  These libraries cover a wide range of functionality, from basic input/output operations to more complex tasks such as network programming, data access, and multimedia support.  Additionally, the . NET framework provides a common execution environment for all . NET applications, allowing them to share resources and take advantage of optimizations provided by the framework. . NET also includes a runtime environment, called the Common Language Runtime (CLR), which manages the execution of . NET code and provides services such as memory management, security, and exception handling.  The CLR also enables . NET applications to run on multiple platforms and devices, including Windows, Linux, and macOS, as well as iOS and Android for mobile devices. In summary, . NET is a comprehensive software framework that provides developers with a platform for building a wide range of applications, as well as a set of libraries, tools, and a runtime environment to make development easier and more efficient. ",1
693,"Memory management is a critical component of any computer system, and its role is to manage the use of memory to ensure that programs have the memory they need to function properly, while avoiding issues such as memory leaks and out-of-memory errors. Memory management is responsible for allocating memory to programs and processes as needed, as well as deallocating memory that is no longer being used.  This involves managing a pool of available memory and determining which portions of the memory should be assigned to each program. The memory manager also ensures that each program only accesses the memory that has been allocated to it, and that no program can access memory that belongs to another program.  This is important for maintaining the stability and security of the system. Memory management also plays a role in optimizing the use of memory in a system.  This can involve techniques such as swapping memory contents to and from disk when memory is running low, or compressing memory to make better use of the available space. In short, the role of memory management is to ensure that the system has enough memory to run all its programs and processes, and that memory is used efficiently and securely.  Memory management is a complex and critical component of any computer system, and its proper operation is essential for the overall stability and performance of the system. ",1
694,"A loop is a control flow construct that allows you to repeat a block of code multiple times.  The idea behind a loop is to automate the repetitive tasks within a program, making it more concise and efficient.  There are several types of loops in computer programming, including:For loop: used to repeat a block of code for a specified number of times. While loop: used to repeat a block of code as long as a specified condition is met. Do-While loop: used to repeat a block of code at least once, and then continue to repeat the block as long as a specified condition is met. Foreach loop: used to iterate over elements in a collection, such as an array or list. Loops are fundamental concepts in computer programming and can be found in most programming languages, including Python, Java, C++, and many others. ",1
695,"LAN stands for Local Area Network, which is a computer network that connects devices within a limited area, such as a home, office, or school.  It allows for the sharing of resources, such as files and printers, and enables communication between connected devices.  LANs are typically faster and more secure than wide area networks (WANs), which connect devices over a larger geographic area, such as the internet. ",1
696,"LAN software refers to any software that is specifically designed to operate on a Local Area Network (LAN).  This includes any software that facilitates communication between devices on the LAN, such as network operating systems, network protocols, and network management software.  LAN software can also include applications that are used within the LAN, such as file sharing software, database software, and collaborative software.  Examples of LAN software include Microsoft Windows Server, Linux, TCP/IP protocol suite, and Wireshark. ",1
697,"List processing refers to a method of data processing that involves working with lists of data elements.  A list is a collection of items that are stored in a specific order, and list processing involves manipulating and analyzing these lists to extract meaningful information or perform specific tasks. List processing can be done using various programming languages and tools, such as Python, R, and MATLAB.  Common operations performed on lists include sorting, filtering, searching, merging, and transforming data.  These operations can be used to extract information, perform calculations, or generate reports. List processing is commonly used in data analysis, scientific research, and information management.  It is also an important part of algorithm design, as many computational problems can be framed in terms of manipulating lists of data elements. ",1
698,"List processing has a wide range of applications in various fields, including:Data analysis: List processing is often used to process and analyze large datasets in fields such as finance, marketing, and scientific research. Information management: Lists are commonly used to manage and organize data, such as customer information, inventory data, and financial records. Algorithm design: Many computational problems can be framed in terms of manipulating lists of data elements, and list processing is a fundamental technique in algorithm design. Natural language processing: List processing is used in natural language processing (NLP) applications to parse and analyze text data. Image processing: List processing can be used to manipulate and analyze image data, such as pixel values and color information. Artificial intelligence and machine learning: List processing is an important component of many machine learning and artificial intelligence algorithms, such as neural networks and decision trees. Web development: Lists are commonly used in web development to store and manipulate data, such as user profiles and shopping carts. Overall, list processing is a versatile and powerful technique that has many practical applications in a wide range of fields. ",1
699,"Linux is a free and open-source operating system based on the Unix operating system.  It was created in 1991 by Linus Torvalds, and has since become one of the most widely used operating systems in the world. Linux is known for its stability, security, and flexibility, and is used for a wide range of applications, from servers to desktop computers and mobile devices.  It is highly customizable, and users can choose from a wide range of software applications and user interfaces to suit their needs. One of the key features of Linux is its open-source nature, which means that the source code is freely available for anyone to access, modify, and distribute.  This has led to the development of a large and active community of developers, who contribute to the development and improvement of the Linux operating system. Linux is used by many organizations and individuals around the world, including businesses, governments, universities, and home users.  It is a powerful and versatile operating system that can be used for a wide range of applications, and is an important part of the modern computing landscape. ",1
700,"The kernel is a core component of an operating system that manages system resources and provides a layer of abstraction between hardware and software.  It is responsible for handling system calls, managing memory and storage, scheduling tasks, and controlling input and output operations. The kernel is loaded into memory when the operating system starts, and it remains in memory throughout the operation of the system.  It provides a set of services and interfaces that allow applications and other software components to interact with the system and access system resources. The kernel is an essential part of any operating system, and its design and implementation have a significant impact on the performance, stability, and security of the system.  Different operating systems use different kernel designs, and there are many different types of kernels, such as monolithic kernels, microkernels, and hybrid kernels. ",1
701,"JavaScript is a programming language that is used to create interactive web pages and web applications.  It is a high-level language that is primarily used to add dynamic and interactive elements to websites, such as forms, menus, and multimedia content. JavaScript was created in 1995 by Brendan Eich while he was working at Netscape Communications, and has since become one of the most widely used programming languages in the world.  It is supported by all major web browsers, including Google Chrome, Mozilla Firefox, and Microsoft Edge. JavaScript code is typically embedded within HTML documents and is executed by the web browser to perform various tasks.  This includes manipulating the content of web pages, responding to user actions, and communicating with web servers to retrieve and send data. JavaScript is a versatile language that can be used for a wide range of applications, including web development, game development, and mobile app development.  It is also commonly used with popular web development frameworks and libraries, such as React and Node. js. Overall, JavaScript is a powerful and flexible programming language that is essential to modern web development and is used by millions of developers around the world. ",1
702,"Java is a popular programming language that offers a number of advantages, including:Platform independence: Java code can be run on any platform that has a Java Virtual Machine (JVM), which means that Java programs can be developed and run on a wide range of operating systems and hardware. Object-oriented programming: Java is an object-oriented programming language, which means that it is based on the concept of objects that can interact with one another to perform tasks.  This makes it easy to write and maintain complex programs, and promotes code reuse and modularity. Large standard library: Java comes with a large standard library that includes many pre-built components and tools for common programming tasks, such as file I/O, network communication, and database access. Memory management: Java has built-in memory management capabilities that help to prevent memory leaks and other memory-related problems that can occur in other programming languages. Security: Java has a strong security model that includes features such as automatic memory allocation and a sandboxing environment that helps to prevent malicious code from damaging the system. Community support: Java has a large and active community of developers and users who contribute to the development and improvement of the language and related tools, such as IDEs, frameworks, and libraries. Overall, Java is a versatile and powerful programming language that is used for a wide range of applications, from enterprise software to mobile app development.  Its platform independence, object-oriented programming model, and strong security features make it an ideal choice for many developers and organizations. ",1
703,"Java is a high-level, object-oriented programming language that is used to create a wide range of applications, from desktop software to mobile apps and web applications.  It was first introduced in 1995 by Sun Microsystems, and is now owned and developed by Oracle Corporation. One of the key features of Java is its platform independence, which means that Java code can be run on any platform that has a Java Virtual Machine (JVM) installed, including Windows, Linux, and macOS.  This makes Java programs highly portable and enables developers to write code that can run on a wide range of devices without modification. Java is also known for its ease of use and maintainability, with a syntax that is designed to be straightforward and easy to learn, even for beginners.  It is based on an object-oriented programming model, which allows developers to write code that is organized around objects and their interactions. In addition to its core language features, Java also has a large and active community of developers who have created a wide range of libraries, frameworks, and tools that can be used to simplify common programming tasks, such as database access, network communication, and user interface design. Overall, Java is a versatile and powerful programming language that is used by millions of developers around the world for a wide range of applications, and it is considered to be one of the most popular and widely-used programming languages in the world. ",1
704,"An interpreter is a computer program that translates and executes code written in a high-level programming language.  Unlike a compiler, which translates the entire program into machine code before executing it, an interpreter executes the code line by line, as it is encountered in the program. As the interpreter executes each line of code, it converts the code into machine-readable instructions that can be executed by the computer's CPU.  This process is known as interpretation, and it allows developers to test and debug their code quickly and easily, without the need to compile the entire program each time a change is made. Interpreters are commonly used in scripting languages, such as Python and Ruby, which are designed to be easy to learn and use.  They are also used in languages such as JavaScript and PHP, which are commonly used in web development, as they can be executed directly in a web browser or on a web server. One disadvantage of using an interpreter is that it can be slower than using a compiler, as the interpreter must execute each line of code as it is encountered.  However, interpreters offer many advantages in terms of ease of use and rapid development, making them a popular choice for many developers. ",1
705,"ISP stands for Internet Service Provider.  An ISP is a company that provides internet access to individuals, businesses, and other organizations.  ISPs typically offer a range of services, including broadband internet access, email services, web hosting, and virtual private network (VPN) services. ISPs provide the physical infrastructure and technology necessary to connect users to the internet, such as fiber optic cables, DSL modems, and wireless routers.  They also typically provide customers with a range of service plans that offer different levels of bandwidth, data usage, and other features. In addition to providing internet access, many ISPs also offer a range of value-added services, such as email hosting, web hosting, and cloud storage.  Some ISPs also provide security services, such as antivirus software and firewalls, to help protect their customers from online threats. Overall, ISPs play a critical role in enabling individuals and businesses to connect to the internet and access the vast array of information, services, and applications that are available online. ",1
706,"Internet radio, also known as web radio or streaming radio, is a digital audio broadcasting service that allows users to listen to live or pre-recorded radio broadcasts over the internet.  Unlike traditional radio, which broadcasts over the airwaves and requires a radio receiver, internet radio is accessible through any device that is connected to the internet, including computers, smartphones, and smart speakers. Internet radio stations can be accessed through a variety of websites and apps that offer live streaming or on-demand playback of radio content.  Some internet radio stations are run by traditional broadcasters, such as commercial or public radio stations, while others are operated by independent producers or hobbyists. One of the key advantages of internet radio is its global reach.  Unlike traditional radio, which is limited by geography and broadcast range, internet radio can be accessed from anywhere in the world, as long as the listener has an internet connection. Internet radio also offers a greater variety of content than traditional radio, as there are thousands of internet radio stations that cater to a wide range of interests and musical genres.  Additionally, internet radio often includes interactive features, such as the ability to rate songs, share playlists, and connect with other listeners through social media. Overall, internet radio has become a popular alternative to traditional radio, as it offers greater flexibility, variety, and accessibility for listeners. ",1
707,"The growth of the internet can be attributed to advances in computer networking technology and the widespread adoption of the World Wide Web.  The internet has enabled the development of a wide range of applications, including email, online communication, e-commerce, social media, streaming media, and cloud computing.  These applications have revolutionized the way people communicate, work, learn, and access information, and have transformed many industries, such as entertainment, retail, and finance.  The growth of the internet and its applications is expected to continue as technology advances and more people around the world gain access to the internet. ",1
708,"Netnews, also known as Usenet, is a global network of discussion forums, where people can post and read messages on various topics.  It was developed in the late 1970s as a way for users of different computer systems to share information and communicate with each other. Netnews uses a distributed architecture, where messages are stored on servers called Usenet hosts, and are replicated across multiple hosts to ensure redundancy and availability.  Users can access netnews using specialized newsreader software, which allows them to browse and participate in discussions on various newsgroups. While netnews has declined in popularity since the rise of the World Wide Web and other social media platforms, it still remains an important part of the internet's history and is used by a dedicated community of users today. ",1
709,"FTP stands for File Transfer Protocol, which is a standard network protocol used for transferring files between computers on the internet. FTP allows users to upload and download files from a remote server, such as a web server or a file storage service.  To use FTP, users need an FTP client program, which can connect to an FTP server using a username and password, and then transfer files using commands such as upload, download, delete, or rename. FTP is commonly used for website maintenance, software distribution, and large file transfers, and it is supported by most operating systems and web browsers.  However, FTP is not a secure protocol, and data transferred over FTP is not encrypted, which can make it vulnerable to interception or modification by unauthorized parties.  For this reason, more secure protocols such as SFTP (Secure File Transfer Protocol) or FTPS (FTP over SSL) are often used instead of plain FTP. ",1
710,"Telnet is a network protocol used to establish a remote login session on a computer or other device over a network. Telnet allows a user to log in to a remote computer or device and perform tasks as if they were sitting in front of it.  This is done by establishing a Telnet session between the user's computer and the remote device, and then sending commands and receiving responses through the Telnet protocol. While Telnet was widely used in the early days of computer networking, it is now considered an insecure protocol, as it sends data and credentials in plain text, which can be intercepted and read by attackers.  For this reason, more secure protocols such as SSH (Secure Shell) are often used instead of Telnet for remote login sessions. ",1
711,"WAIS (Wide Area Information Server) was an early internet-based information retrieval system, which allowed users to search for and retrieve text-based information from a large collection of servers around the world. WAIS worked by indexing the contents of documents on participating servers and creating a searchable database of the indexed content.  Users could then search for specific keywords or phrases and retrieve a list of documents that matched their query. WAIS was widely used in the early days of the internet, before the World Wide Web became popular, and it was known for its powerful search capabilities and ability to handle large volumes of text-based data.  However, with the rise of the web and more user-friendly search interfaces, WAIS became less popular and was eventually largely supplanted by web search engines such as Google. ",1
712,"The internet is a global network of interconnected computer networks, which enables the exchange of data and communication between devices and users around the world. The internet allows users to access and share a wide range of information and resources, including websites, email, instant messaging, social media, online gaming, and file sharing.  It also enables e-commerce, online banking, and other online services that have transformed many aspects of daily life and work. The internet was developed over several decades, beginning in the 1960s, and it has continued to evolve and expand with advances in computer networking technology.  Today, the internet is an essential part of modern society and the global economy, and it has enabled new forms of collaboration, innovation, and social interaction that were not previously possible. ",1
713,"Internationalization and localization are two related concepts that are important for adapting software and other digital products to different languages, cultures, and regions. Internationalization (often abbreviated as i18n) is the process of designing and developing software so that it can be easily adapted to different languages and regions.  This involves separating the user interface text and other content from the software code, so that it can be translated and localized without modifying the underlying code.  Internationalization also involves addressing other cultural and technical differences, such as date and time formats, numeric conventions, and character sets. Localization (often abbreviated as l10n) is the process of adapting software to a specific language or region, taking into account the cultural and linguistic differences of the target audience.  This involves translating the user interface text, as well as adapting other content and features to meet the needs and expectations of the local audience.  Localization can also involve testing and validating the localized product to ensure that it is fully functional and meets the quality standards of the target market. Together, internationalization and localization enable software and other digital products to be used by people in different countries and regions, and they are essential for reaching a global audience and expanding business opportunities. ",1
714,"New media, which refers to digital technologies and platforms such as social media, online video, mobile apps, and other forms of digital content, has brought about many benefits, such as greater access to information, enhanced communication, and new forms of creativity and expression.  However, it has also brought about a number of challenges, including:Information overload: With so much information available at our fingertips, it can be difficult to filter out noise and find reliable sources of information.  The sheer volume of content can also be overwhelming and can lead to feelings of stress and anxiety. Privacy and security: New media platforms collect vast amounts of personal data, which can be vulnerable to hacking and other security breaches.  This has raised concerns about privacy and the protection of personal information. Misinformation and fake news: New media has made it easier for misinformation and fake news to spread quickly and widely.  This has led to increased polarization, decreased trust in traditional news sources, and the erosion of public discourse. Addiction and mental health: New media has been linked to addiction and negative impacts on mental health, such as depression, anxiety, and sleep disturbances.  This is due in part to the constant connectivity and stimulation that new media provides, as well as the pressures to maintain a social media presence and engage with online communities. Economic disruption: New media has disrupted traditional business models in industries such as journalism, entertainment, and retail, leading to job losses and economic uncertainty.  This has also created new opportunities for entrepreneurs and digital creators, but has also widened the gap between winners and losers in the digital economy. These challenges are complex and multifaceted, and they require careful attention and strategic responses from individuals, organizations, and governments. ",1
715,"Intellectual property (IP) refers to intangible creations of the human mind that can be legally protected, including inventions, literary and artistic works, symbols, names, images, and designs. IP includes a wide range of categories, such as patents, trademarks, copyrights, trade secrets, and industrial designs.  Each category has its own legal framework and requirements for protection. IP protection is important because it allows creators and innovators to control and benefit from their ideas and works, and to prevent others from using them without permission or compensation.  This helps to encourage innovation and creativity by providing incentives for individuals and companies to invest time, money, and resources into developing new products and ideas. IP protection is also a complex and evolving area of law, with different rules and practices in different countries and regions.  As digital technologies and the global economy continue to evolve, IP issues such as copyright infringement, trademark disputes, and patent trolls have become more common and more challenging to address. ",1
716,"Information theory is a mathematical and computational theory that studies the quantification, storage, transmission, and processing of information.  It was developed by Claude Shannon in the 1940s and 1950s, and it has since become a fundamental tool in fields such as communication, computer science, physics, and cryptography. At its core, information theory seeks to understand the fundamental limits of communication and data processing, and to identify efficient ways of encoding, transmitting, and decoding information.  It provides mathematical tools and models for measuring the amount of information in a message or signal, and for evaluating the reliability and efficiency of different coding and compression techniques. Some key concepts in information theory include entropy, which measures the amount of uncertainty or randomness in a signal; channel capacity, which represents the maximum rate of information that can be transmitted through a communication channel; and error-correcting codes, which are techniques for detecting and correcting errors that can occur during transmission or storage of digital data. ",1
717,"Information retrieval (IR) is the process of retrieving relevant information from large collections of digital data, such as text, images, videos, or audio recordings.  It involves using various techniques to analyze and understand the content and context of a given information need or query, and then retrieving the most relevant documents or records from a collection of information sources. IR systems are used in a wide range of applications, including web search engines, digital libraries, e-commerce platforms, and enterprise search systems.  IR techniques can include methods such as natural language processing, machine learning, and information extraction, and may involve the use of specialized algorithms and indexing techniques to efficiently store and retrieve information. ",1
718,"Image processing is a field of study that involves the use of computer algorithms and techniques to manipulate, analyze, and enhance digital images.  It involves a wide range of tasks, such as image filtering, segmentation, feature extraction, pattern recognition, and image compression. Digital images can come from a variety of sources, such as digital cameras, satellite imagery, medical imaging devices, or scientific instruments.  Image processing techniques can be used to improve the quality and clarity of these images, to extract meaningful information from them, or to enable new applications and insights. Some common applications of image processing include:Medical imagingRobotics and automationVideo surveillance and securityArt and entertainmentImage processing is a complex and multidisciplinary field, drawing on knowledge from computer science, mathematics, physics, and engineering.  As digital imagery continues to play an increasingly important role in our lives, the demand for image processing techniques and applications is expected to grow. ",1
719,"Hacking refers to the act of accessing computer systems or networks without authorization, and the term ""hacker"" originally referred to someone who was skilled in computer programming and used their skills to solve problems in creative and innovative ways. The first known instance of hacking occurred in 1960, when a group of students at the Massachusetts Institute of Technology (MIT) explored the limitations of the institution's computer system.  In the following decades, hacking grew in popularity and became associated with both positive and negative activities. In the 1980s, hackers gained notoriety for their involvement in high-profile computer security breaches, such as the Morris Worm, which affected thousands of computers.  This led to the passage of the Computer Fraud and Abuse Act in the United States in 1986, which made computer hacking a federal crime. Since then, the field of cybersecurity has evolved to protect against hacking and other types of cyberattacks.  However, hacking and hackers continue to pose a threat to computer systems and networks, and both ethical and unethical hacking are still practiced today",1
720,"The hexadecimal system is a number system with a base of 16.  It is commonly used in computing and digital electronics, as it provides an efficient way to represent binary data.  In the hexadecimal system, each digit can have 16 different values, ranging from 0 to 9 and A to F. The first 10 digits in the hexadecimal system represent the numbers 0 to 9, while the letters A to F represent the numbers 10 to 15.  For example, the number 10 in hexadecimal is represented by the letter A, and the number 15 is represented by the letter F. To convert a hexadecimal number to a decimal number, each digit is multiplied by its corresponding power of 16 and then added together.  For example, the hexadecimal number 3B can be converted to decimal as follows:3B = (3 x 16^1) + (11 x 16^0)= (3 x 16) + (11 x 1)= 48 + 11= 59Similarly, to convert a decimal number to a hexadecimal number, the number is divided by 16 and the remainder is converted to a hexadecimal digit.  The process is repeated until the quotient becomes zero.  The resulting hexadecimal digits are written in reverse order to obtain the final hexadecimal number. For example, to convert the decimal number 210 to hexadecimal:210 ÷ 16 = 13 with a remainder of 213 ÷ 16 = 0 with a remainder of 13 (which is represented by the letter D)Therefore, 210 in hexadecimal is D2. ",1
721,"In computer science, a heap is a specialized tree-based data structure that is used to efficiently maintain a collection of elements where the priority of each element is determined by its value or some other key. Heaps can be of two types:Max heap: In a max heap, the parent nodes are always greater than or equal to their child nodes.  The maximum element in the heap is always the root node. Min heap: In a min heap, the parent nodes are always smaller than or equal to their child nodes.  The minimum element in the heap is always the root node. A heap is often used in algorithms such as sorting, graph algorithms, and priority queue implementations. The most common operation performed on a heap is insertion, where a new element is added to the heap in a way that maintains the heap property.  The next most common operation is deletion, where the root node is removed from the heap and replaced with another element in a way that also maintains the heap property.  Other common operations on a heap include searching for an element, merging two heaps, and extracting the maximum or minimum element. Heaps have an average time complexity of O(log n) for both insertion and deletion, making them a popular choice for a wide range of applications where efficient and fast access to elements is required. ",1
722,"Help systems are computer software programs designed to provide assistance and guidance to users of software applications, websites, or other digital products. Help systems can take different forms, such as:Contextual help: This type of help provides assistance that is specific to the context in which the user is working.  For example, a tooltip that appears when the user hovers over a button, providing a brief explanation of its function. User manuals: These are documents that provide comprehensive information about how to use a software application, including step-by-step instructions, screenshots, and diagrams. Tutorials: These are interactive guides that take the user through a software application or process, providing guidance and feedback as they progress. Knowledge bases: These are searchable databases that contain information about a software application or product, including frequently asked questions (FAQs), troubleshooting guides, and other resources. Help systems are important for ensuring that users are able to use software applications effectively and efficiently.  They can also help reduce support costs and improve customer satisfaction by providing users with the information they need to solve problems on their own.  Additionally, help systems can provide valuable insights into how users interact with software applications, which can be used to improve usability and design. ",1
723,"In computer science, a hash is a fixed-length string of characters generated by applying a mathematical function or algorithm to a variable-length input data.  The resulting hash value is usually a unique representation of the input data, which is why it is commonly used for data integrity, authentication, and encryption. Hashing is a widely used technique for data storage, indexing, and retrieval.  Hash functions are designed to map variable-length data to a fixed-size hash value in a way that is fast, efficient, and avoids collisions. ",1
724,"Hashing has many applications in computer science, but two major ones are:Data Storage and Retrieval: Hashing is used for efficient data storage and retrieval in many applications.  Hash tables are widely used data structures that store data in a way that allows for fast lookups, insertions, and deletions.  In a hash table, data elements are stored based on their keys, and a hash function is used to map the keys to an index in an array.  This makes it easy to quickly access a data element given its key, and the time complexity of basic operations like insertions and lookups is typically O(1) on average. Data Authentication and Verification: Hashing is also used for data authentication and verification.  When a hash function is applied to a message or data file, it generates a fixed-length hash value that is unique to the input data.  This makes it possible to verify that the data has not been tampered with or corrupted in any way, as any changes to the input data will result in a different hash value.  Hashing is widely used for password storage, where passwords are hashed and stored in a database, and for digital signatures, where a hash of a document or message is signed to ensure its authenticity and integrity. ",1
725,"A hard drive is a non-volatile storage device used to store digital data, such as computer files and software applications.  It consists of one or more platters, which are circular disks coated with a magnetic material that can store data, and a set of read/write heads that can read and write data to the platters. When data is written to a hard drive, the read/write heads magnetically encode the data onto the platters.  To read the data, the read/write heads move over the platters and sense the magnetic fields to retrieve the encoded data.  Hard drives typically spin at high speeds, often between 5400 and 7200 revolutions per minute (RPM), to enable faster data access. Hard drives come in different form factors, such as 2. 5-inch and 3. 5-inch drives, and in different capacities, ranging from a few hundred gigabytes to several terabytes.  They can be connected to a computer via various interfaces, such as SATA, SCSI, or IDE, and can be internal or external. In addition to traditional hard drives, there are also solid-state drives (SSDs) that use flash memory to store data.  Unlike hard drives, SSDs have no moving parts, which makes them faster and more durable than hard drives.  However, they are generally more expensive and have lower storage capacities than hard drives. Hard drives are a common type of storage device used in personal computers, servers, and other computing devices.  They are used to store operating systems, applications, and user data, and are often the primary storage device in a computer system. ",1
726,"Haptic interfaces, also known as haptic devices or force feedback devices, are computer input/output devices that provide tactile feedback to the user.  They allow the user to interact with digital content by providing physical sensations, such as vibrations, forces, or movements, that simulate real-world experiences. Haptic interfaces are designed to enhance the user's sense of touch and provide a more immersive and realistic experience.  They are often used in virtual reality (VR) and augmented reality (AR) applications, where they can be used to simulate the feel of objects or environments in the virtual world. ",1
727,"There are many types of haptic technologies, some of which include:Haptic feedback controllers: These are input devices that provide tactile feedback in response to user actions, such as pressing buttons or moving a joystick.  Examples include the Xbox One controller and the PlayStation DualShock controller. Haptic gloves: These are gloves that have sensors and actuators embedded in them that can provide tactile feedback to the user's fingers and hands.  Examples include the HaptX Gloves and the Teslasuit Glove. Haptic suits: These are full-body suits that have sensors and actuators embedded in them that can provide tactile feedback to different parts of the body.  Examples include the HaptX Suit and the Teslasuit. ",1
728,"The history of hacking dates back to the 1960s when the first computer networks were being developed.  However, at that time, hacking was not seen as a criminal activity, and many of the earliest hackers were simply curious students and researchers who wanted to explore the capabilities of the new technology. As computer networks became more widespread in the 1970s and 1980s, hackers began to gain more notoriety, and their activities became more sophisticated.  Some hackers began to engage in illegal activities such as stealing data and disrupting computer systems.  These early hackers were often motivated by a desire for fame, challenge, or financial gain. As the internet became more widespread in the 1990s, the number of hackers and their activities increased dramatically.  Hackers began to form groups and communities, sharing knowledge and tools to help them carry out their activities.  At the same time, law enforcement agencies began to take notice of hacking and to develop strategies to combat it. ",1
729,"The history of handwriting recognition systems dates back to the 1960s, when researchers began to develop algorithms to recognize handwritten characters and symbols.  One of the earliest systems was developed at IBM, which used pattern recognition techniques to recognize handwritten digits. In the following decades, researchers continued to refine handwriting recognition algorithms, and by the 1980s, commercial handwriting recognition systems began to appear.  These early systems were limited in their accuracy and were mostly used in specialized applications such as check processing. The introduction of personal digital assistants (PDAs) in the 1990s and the increasing use of tablet computers and smartphones in the 2000s led to renewed interest in handwriting recognition technology.  By the early 2000s, handwriting recognition software had become widely available on mobile devices, allowing users to input text using a stylus or their finger. ",1
730,"Handwriting recognition technology has numerous applications in various fields.  Here are some of the most common applications:Digitizing handwritten documents: Handwriting recognition technology can convert handwritten documents into digital form, making them easier to store, search, and share. Automatic form processing: Handwriting recognition can be used to automatically process forms that are filled out by hand, such as medical forms, surveys, and tax forms. Text recognition in images: Handwriting recognition can be used to recognize text in images, such as street signs, license plates, and product labels. Personalized note-taking: Handwriting recognition can be used in note-taking applications to recognize handwritten notes and convert them into typed text. Digital signatures: Handwriting recognition can be used to authenticate digital signatures by verifying the authenticity of a person's handwriting. Accessibility: Handwriting recognition technology can be used to help people with disabilities, such as those with physical disabilities or dyslexia, to communicate more easily by using handwriting as an input method. Education: Handwriting recognition can be used in education to provide feedback on handwriting skills, to grade written assignments, and to automatically generate quizzes or tests based on handwritten responses. Overall, handwriting recognition has many applications that can help streamline tasks, improve accessibility, and enhance productivity in a variety of fields. ",1
731,"Game consoles are specialized computers designed primarily for playing video games.  They are typically connected to a television or monitor and controlled using a game controller.  Game consoles offer a dedicated platform for playing games, often providing exclusive titles and unique gameplay experiences not available on other platforms",1
732,Genetic algorithms are a type of optimization algorithm inspired by the principles of natural selection and evolution.  They are used to find optimal solutions to complex problems by mimicking the process of natural selection. ,1
733,"Genetic algorithms have a wide range of applications in various fields.  Here are some of the most common applications:Optimization problems: Genetic algorithms can be used to find optimal solutions to complex optimization problems in engineering, finance, logistics, and other areas.  For example, they can be used to optimize the design of aircraft or to optimize investment portfolios. Machine learning: Genetic algorithms can be used in machine learning to optimize the hyperparameters of a model, such as the learning rate, the number of layers, and the number of neurons in a neural network. Robotics: Genetic algorithms can be used to optimize the behavior of robots, such as the movement and coordination of robotic arms or the navigation of autonomous vehicles. Game development: Genetic algorithms can be used to create non-player characters (NPCs) with unique behaviors and strategies in video games. Art and design: Genetic algorithms can be used to generate art and design, such as creating unique visual patterns or architectural designs. Pattern recognition: Genetic algorithms can be used to recognize patterns in data, such as identifying genes associated with diseases or analyzing patterns in financial data. Bioinformatics: Genetic algorithms can be used in bioinformatics to analyze DNA sequences and identify genetic markers associated with diseases or traits. ",1
734,"A genetic program consists of a population of computer programs, represented as strings of code, which are evolved and refined through a process of natural selection and mutation inspired by the principles of evolution. The genetic program typically includes the following components:Initialization: A set of initial computer programs is generated randomly to create the initial population. Fitness evaluation: Each program in the population is evaluated based on its fitness, which is a measure of its ability to solve a specific problem or perform a specific task. Selection: Programs with higher fitness are more likely to be selected to reproduce and pass on their genetic information to the next generation. Reproduction: The selected programs are combined through crossover and mutation to generate new programs for the next generation. Termination: The evolution process is terminated when a satisfactory solution is found, or when a predetermined number of generations have been reached. ",1
735,"Bill Gates is an American entrepreneur, software developer, investor, and philanthropist.  He is best known as the co-founder of Microsoft Corporation, one of the world's largest and most successful software companies.  Gates was born on October 28, 1955, in Seattle, Washington. Gates co-founded Microsoft with Paul Allen in 1975, starting out with a simple vision to put a computer on every desk and in every home.  Under Gates' leadership, Microsoft developed and sold a range of software products, including the popular Windows operating system, which became the dominant platform for personal computers. Gates served as the CEO of Microsoft until 2000, and during his tenure, he was responsible for many of the company's key innovations and business strategies.  He is widely recognized as one of the most influential figures in the history of the technology industry. In recent years, Gates has become increasingly involved in philanthropy, through the Bill and Melinda Gates Foundation, which he founded with his wife in 2000.  The foundation focuses on global health and development, education, and climate change, and has become one of the largest private charitable organizations in the world. ",1
736,"Geographic data is typically stored in a geographic information system (GIS) database or file format.  A GIS is a software system designed to store, manipulate, and analyze spatial data, which includes both geographic and cartographic data. There are several common data storage formats used in GIS:Shapefile: A shapefile is a common format for storing vector data, such as points, lines, and polygons.  It consists of several files that store the spatial and attribute data. GeoJSON: GeoJSON is a format for encoding geographic data structures in JSON (JavaScript Object Notation).  It is a lightweight format that is commonly used for web mapping applications. Geodatabase: A geodatabase is a proprietary format used by Esri's ArcGIS software.  It can store both vector and raster data, and provides advanced features for data management, editing, and analysis. KML: Keyhole Markup Language (KML) is a file format used for displaying geographic data in Google Earth and other virtual globes.  It can store points, lines, polygons, and other features. GeoTIFF: GeoTIFF is a format for storing raster data with geographic information.  It includes georeferencing information, such as the location, scale, and orientation of the raster image. ",1
737,"Geographic Information Systems (GIS) are powerful tools for capturing, storing, manipulating, analyzing, and visualizing geographic data.  Here are some of the key strengths and benefits of GIS:Spatial analysis: GIS allows users to analyze spatial data and relationships between geographic features, such as proximity, adjacency, and connectivity.  This can be used to identify patterns, trends, and relationships that are not visible in tabular data. Decision-making: GIS can help decision-makers to understand and visualize complex data in a geographic context, allowing them to make more informed and effective decisions.  For example, GIS can be used to identify suitable locations for new facilities, such as schools, hospitals, or stores. Resource management: GIS can be used to manage and optimize natural resources, such as water, land, and forests.  For example, GIS can be used to track and analyze the distribution of natural resources, monitor changes in land use and land cover, and assess the impact of human activities on the environment. ",1
738,"Globalization is the process of increased interconnectedness and integration of the world's economies, societies, and cultures.  It refers to the growing interdependence of nations, as they become more connected through trade, investment, communication, and migration.  Globalization has been driven by a combination of technological, economic, political, and social factors, and has had profound effects on virtually every aspect of life. The phenomenon of globalization has been accelerating since the latter half of the 20th century, as advances in technology, transportation, and communication have made it easier and cheaper to connect people and businesses across borders.  As a result, goods, services, capital, and ideas can move more freely and easily than ever before, leading to increased economic growth and opportunity, as well as new challenges and risks. ",1
739,"Global trends can have a significant impact on computer technology, as they drive demand for new applications, devices, and services, and shape the direction of innovation and development.  Here are some examples of how global trends are influencing computer technology:Digital transformation: The trend towards digitalization is transforming virtually every industry, as businesses and governments seek to digitize their operations and services.  This is driving demand for new technologies such as cloud computing, artificial intelligence, and the Internet of Things (IoT), as well as new security and privacy measures to protect digital assets. Remote work: The COVID-19 pandemic has accelerated the trend towards remote work, as companies and organizations have had to quickly adapt to new ways of working.  This has led to increased demand for collaboration and communication tools, such as video conferencing, messaging apps, and project management software, as well as new security measures to protect remote workers and their data. Sustainability: The growing awareness of environmental issues is driving demand for more sustainable and energy-efficient technologies, such as renewable energy sources, green computing, and circular economy models.  This is leading to new developments in areas such as battery technology, power management, and energy-efficient computing hardware and software. Globalization: The trend towards globalization is driving demand for technologies that can support global communication and collaboration, such as translation tools, internationalization and localization software, and cross-border payment systems.  It is also leading to increased demand for cybersecurity measures to protect against global threats, such as cyber attacks and data breaches. ",1
740,"Computer-related businesses are heavily impacted by the effects of globalization, as they operate in a global marketplace that is increasingly interconnected and competitive.  Here are some ways in which computer-related businesses deal with the effects of globalization:Internationalization: Many computer-related businesses seek to expand their operations globally, by entering new markets and developing international partnerships.  This involves adapting their products and services to meet the needs of different cultures and languages, as well as complying with local regulations and laws. Outsourcing: Globalization has led to the outsourcing of many computer-related jobs to countries where labor is cheaper, such as India and China.  This allows businesses to reduce costs and increase efficiency, but it also raises ethical and social concerns about the impact on local workers and economies. Competition: Globalization has led to increased competition in the computer industry, as businesses from different countries compete for customers and market share.  This has led to rapid innovation and development, but it also presents challenges for businesses that must keep up with changing technologies and market trends. Supply chain management: Many computer-related businesses rely on complex global supply chains to manufacture and distribute their products.  This involves managing relationships with suppliers and partners in different countries, as well as ensuring compliance with international trade and environmental regulations. ",1
741,"Google is a multinational technology company that specializes in internet-related services and products.  It was founded in 1998 by Larry Page and Sergey Brin while they were Ph. D.  students at Stanford University.  Today, Google is one of the largest and most successful companies in the world, with a market capitalization of over a trillion dollars. Google is best known for its search engine, which is the most widely used search engine in the world.  It uses a complex algorithm to provide users with relevant and useful search results based on their queries.  Google also offers a wide range of other products and services, including:Google Ads: a platform for creating and displaying online advertisements. Google Cloud: a cloud computing platform that offers a wide range of services, including storage, data analytics, and machine learning. Google Workspace: a suite of productivity tools, including Gmail, Google Drive, Google Docs, and Google Sheets. Google Maps: a mapping and navigation service that provides detailed maps and driving directions. Google's success has been driven by its focus on innovation, user experience, and data-driven decision making.  The company has a strong culture of experimentation and encourages its employees to take risks and think creatively.  Google's business model is primarily based on advertising revenue, which is generated by displaying ads alongside search results and other online content. ",1
742,"The history of graphics cards can be traced back to the 1970s, when computer graphics were first being developed.  In the early days, graphics were generated by the computer's central processing unit (CPU), which limited the quality and speed of graphics rendering. The first dedicated graphics card was developed in the late 1970s by IBM, which introduced the Monochrome Display Adapter (MDA) in 1981.  The MDA was a text-only display adapter that could display up to 80 columns of text on a monochrome monitor. In the early 1980s, the Color Graphics Adapter (CGA) was introduced, which was the first graphics card to support color graphics.  The CGA could display 16 colors at a resolution of 320x200 pixels. In the mid-1980s, the Enhanced Graphics Adapter (EGA) was introduced, which could display up to 16 colors at a higher resolution of 640x350 pixels. In the late 1980s and early 1990s, a number of new graphics standards were introduced, including Video Graphics Array (VGA), Super VGA (SVGA), and XGA.  These standards offered higher resolutions, more colors, and faster refresh rates, and helped to drive the development of dedicated graphics cards. In the mid-1990s, 3D graphics technology began to emerge, with the introduction of the first 3D graphics accelerators.  These cards, such as the 3dfx Voodoo and the Nvidia RIVA, were designed to offload 3D rendering tasks from the CPU, resulting in smoother and more realistic graphics. ",1
743,"Graphic formats are file formats used to store and display digital images.  There are many different types of graphic formats, each with its own strengths and weaknesses, and each designed for different purposes. ",1
744,"Bitmap format, also known as BMP format, is a type of graphic format used to store digital images.  It was originally developed by Microsoft for use on Windows platforms. Bitmap images are made up of pixels, which are small dots that combine to form the overall image.  Each pixel in a bitmap image is assigned a specific color, which is represented by a series of binary numbers that describe the color intensity and brightness. ",1
745,"One advantage of the bitmap format is that it can store images with very high color depths, meaning that it can capture a wide range of colors and shades.  However, this comes at the cost of file size, as bitmap images tend to be larger in size than other image formats due to the amount of data required to store each pixel. ",1
746,"Encapsulated PostScript (EPS) is a file format used to store and exchange high-resolution graphics and images.  It was developed by Adobe Systems and is widely used in the printing and publishing industries. EPS files are essentially a special type of PostScript file that has been designed to be embedded into other documents, such as word processing documents or page layout documents.  EPS files can contain vector graphics, raster images, or a combination of both. ",1
747,"GIF (Graphics Interchange Format) is a file format used to store and display simple animated images or graphics on web pages or in other digital media.  It was developed by CompuServe in the late 1980s. A GIF file consists of a sequence of images or frames that are displayed in rapid succession, creating the illusion of motion.  GIFs are limited to 256 colors, which makes them relatively small in size and easy to share online. ",1
748,"JPEG (Joint Photographic Experts Group) is a file format used to store and display digital images, especially photographs.  It was developed by the Joint Photographic Experts Group and has become one of the most widely used image formats in the world. JPEG files use a lossy compression method, which means that some of the image data is discarded in order to reduce the file size.  This can result in some loss of image quality, especially when the compression level is high. ",1
749,PCX is a file format used to store and exchange digital images.  It was one of the first widely used graphic file formats for IBM PC computers and was originally developed by ZSoft Corporation in the 1980s. ,1
750,"TIFF (Tagged Image File Format) is a file format used to store and exchange digital images.  It was developed by Aldus Corporation in the 1980s and is widely used in the printing, publishing, and graphic design industries. TIFF files can store both grayscale and color images, as well as images with transparency.  They use a lossless compression method, which means that the image quality is not compromised during compression. ",1
751,"A graphics tablet, also known as a digitizing tablet or pen tablet, is an input device used to create digital artwork or to annotate documents.  It consists of a flat surface and a special stylus or pen that is used to draw or write on the surface. When the pen or stylus is used on the surface of the graphics tablet, it sends a signal to the computer that corresponds to the position of the pen on the tablet.  This allows the user to draw, write, or manipulate images on the computer in a natural and intuitive way. ",1
752,"A Green PC is a computer that has been designed to be energy-efficient and environmentally friendly.  Green PCs are designed to use less energy than traditional computers, which can help reduce their impact on the environment and lower energy costs. ",1
753,"Resource consumption in computers refers to the amount of system resources, such as CPU, memory, and disk space, that a computer uses to perform its tasks.  Different applications and processes on a computer can consume varying amounts of system resources, and understanding and managing resource consumption is important to ensure that the computer performs optimally and efficiently. ",1
754,"E-waste in computers refers to the disposal or recycling of electronic components and devices such as computers, laptops, monitors, and other peripherals that are no longer useful or functional.  E-waste disposal is an important environmental concern, as electronic devices can contain hazardous materials and chemicals that can pollute the environment if not disposed of properly. Recycling is the most common method of disposing of e-waste from computers.  Recycling involves separating the different components of the computer, such as metals, plastics, and glass, and processing them to make new products.  Many local and national organizations offer e-waste recycling programs, which can help ensure that e-waste is disposed of safely and responsibly. ",1
755,"Computers are involved in pollution and greenhouse gas emissions throughout their entire lifecycle, from manufacturing to disposal.  Here are some ways that computers contribute to pollution and greenhouse gas emissions:Manufacturing: The production of computers and electronic devices involves the extraction of raw materials, such as metals and minerals, which can contribute to deforestation, soil degradation, and water pollution.  The manufacturing process also requires energy and generates greenhouse gas emissions, particularly through the use of fossil fuels. Energy consumption: The operation of computers requires electricity, which is often generated from fossil fuels such as coal, oil, and natural gas.  The use of energy-intensive data centers, servers, and cooling systems also contributes to greenhouse gas emissions. E-waste: The disposal of electronic waste, including computers, can contribute to pollution and greenhouse gas emissions.  Electronic devices can contain hazardous materials, such as lead, mercury, and cadmium, which can pose health and environmental risks if not disposed of properly.  The disposal of e-waste in landfills or incineration can also release greenhouse gases such as methane and carbon dioxide. Upgrades and replacements: The frequent upgrading and replacement of computers and electronic devices can contribute to pollution and greenhouse gas emissions.  The production and disposal of new devices require energy and resources, while the disposal of old devices generates e-waste. ",1
756,Grid computing is a type of distributed computing that involves the coordination of resources across a network of computers to perform a common task.  Grid computing allows multiple computers to work together to solve large-scale problems or perform complex calculations that would be difficult or impossible for a single computer to handle. ,1
757,"The architecture of grid computing typically consists of several layers that work together to manage and coordinate resources across a network of computers.  The following are the main layers of a typical grid computing architecture:Resource Layer: This layer includes the individual computers or nodes that make up the grid network.  Each node contributes resources such as processing power, storage, and memory to the grid.  Nodes may vary in their capabilities and configurations, and may be connected to the grid intermittently or permanently. Fabric Layer: This layer provides the infrastructure to manage and coordinate the resources of the grid.  It includes software and hardware components such as network switches, routers, and firewalls that enable communication and data exchange between nodes. Middleware Layer: This layer provides the software framework that enables applications to access and utilize the resources of the grid.  It includes middleware components such as resource managers, schedulers, and security systems that ensure efficient and secure resource allocation and utilization. Application Layer: This layer includes the applications or services that run on the grid and use its resources to perform computational tasks.  Applications may be scientific simulations, data analytics, or other types of processing-intensive tasks that require significant computing resources. User Layer: This layer provides the interface for users to interact with the grid and its resources.  Users may submit jobs or tasks to the grid, monitor their progress, and receive results through a web interface, command-line interface, or other user-friendly application. ",1
758,"Some popular groupware software includes:Microsoft Teams: a chat-based collaboration platform that allows users to collaborate on projects, share files, and hold video meetings. Slack: a messaging platform that provides team collaboration features such as file sharing, chat, and project management tools. Trello: a project management tool that allows teams to organize and prioritize tasks on a visual board, assign tasks to team members, and track progress. Asana: a project and task management tool that allows teams to organize and track tasks, set due dates, and collaborate on projects. Google Workspace: a suite of productivity tools that includes email, calendar, document collaboration, and video conferencing tools. Zoom: a video conferencing platform that allows users to hold virtual meetings and webinars. Basecamp: a project management tool that allows teams to manage tasks, share files, and communicate through a centralized platform. GitHub: a software development platform that provides tools for code management, version control, and collaboration among developers. ",1
759,"Andrew Grove (1936-2016) was a Hungarian-American businessman and engineer who co-founded the semiconductor company Intel Corporation in 1968.  Grove served as CEO of Intel from 1987 to 1998 and as Chairman of the Board from 1997 to 2005. Grove was born in Hungary and survived the Holocaust before emigrating to the United States in 1957.  He earned a Bachelor of Chemical Engineering degree from the City College of New York and a Ph. D.  in chemical engineering from the University of California, Berkeley. ",1
760,"Fault tolerance is the ability of a system or component to continue functioning in the event of a failure or malfunction of one or more of its components.  In other words, a fault-tolerant system is designed to maintain a certain level of functionality and availability even when one or more components fail. Fault tolerance is important in many areas of computing, including hardware, software, and networks.  Examples of fault-tolerant systems include redundant power supplies in a data center, backup servers and storage systems, and network failover mechanisms. ",1
761,"There are several ways to achieve fault tolerance in computing systems.  Some common techniques include:Redundancy: This involves duplicating critical components or systems and configuring them to take over in the event of a failure.  For example, a fault-tolerant computer system may use multiple processors, hard drives, and network interfaces to ensure that there are backup components available to take over in case of failure. Replication: This involves maintaining multiple copies of critical data or software across different systems or locations.  If one copy becomes unavailable or corrupted, another copy can be used in its place. Error detection and correction: This involves using techniques such as checksums, parity bits, or error-correcting codes to detect and correct errors in data or transmissions.  This can help prevent data corruption or loss caused by hardware or software failures. Graceful degradation: This involves designing a system to continue functioning at a reduced level of performance or capacity in the event of a failure.  This can be useful in situations where complete redundancy is not feasible or cost-effective, such as in mobile devices or cloud computing environments. Load balancing: This involves distributing workloads across multiple systems to prevent any one system from becoming overwhelmed or failing.  Load balancing can help ensure that a system remains available and responsive even in the face of high demand or unexpected failures. ",1
762,"a fault-tolerant system depends on a combination of redundancy, error detection and correction, system monitoring and management, system recovery and failover, and careful system design and architecture to provide reliable operation in the face of failures or malfunctions. ",1
763,"Fiber optics is a technology used in telecommunications and networking that involves the transmission of data and information over thin, flexible glass or plastic fibers.  The fibers use light to transmit signals over long distances, offering high-speed and high-bandwidth communication capabilities. In a fiber optic system, light is generated by a laser or LED and transmitted through the fiber optic cable.  The light travels through the fiber by repeatedly bouncing off the walls of the fiber, which are coated with a highly reflective material.  The light is guided through the fiber by a process called total internal reflection, which allows it to travel over long distances without significant signal loss or degradation. ",1
764,"The development of optical fiber technology can be traced back to the early 20th century, when scientists and engineers began to explore the use of light to transmit information over long distances. In the 1920s and 1930s, researchers began to investigate the properties of glass and other transparent materials as potential transmission media for light.  They discovered that light could be guided along a thin glass fiber by reflecting off the walls of the fiber, and that this process, called total internal reflection, could be used to transmit information. In the 1950s and 1960s, researchers in the United States and Europe made significant advances in the development of fiber optics technology.  They developed new manufacturing techniques to produce high-quality glass fibers with low attenuation (loss of signal strength), and they developed new methods for connecting fibers together and transmitting light signals over long distances. In the 1970s and 1980s, the development of new types of lasers and other light sources made it possible to transmit signals over much longer distances and at much higher data rates.  This led to the widespread adoption of fiber optic technology in telecommunications and other applications. Today, optical fiber technology is used in a wide range of applications, including telecommunications, networking, medical equipment, and sensing technologies.  It has revolutionized the way we communicate and share information, enabling high-speed data transmission and communication over long distances with unprecedented reliability and efficiency. ",1
765,"Optical fiber technology offers several advantages over traditional copper-based communication systems, including:High bandwidth: Optical fibers can transmit large amounts of data over long distances at high speeds, making them ideal for high-bandwidth applications such as video streaming, cloud computing, and online gaming. Low attenuation: Optical fibers have low attenuation, which means that they can transmit signals over long distances without significant signal loss or degradation. Immunity to electromagnetic interference: Optical fibers are immune to electromagnetic interference, making them more reliable in high-noise environments. Small size and lightweight: Optical fibers are small and lightweight, making them easy to install and transport. Security: Optical fibers are difficult to tap or intercept, making them more secure than traditional copper-based communication systems. Durability: Optical fibers are highly durable and resistant to damage from moisture, temperature fluctuations, and other environmental factors. Cost-effective: Optical fibers are becoming increasingly cost-effective as the technology becomes more widely adopted and production costs decrease. ",1
766,"A file is a collection of data or information that is stored on a computer or other digital storage medium, such as a flash drive, hard drive, or cloud storage service.  Files can contain text, images, audio, video, software, or any other type of digital information. ",1
767,"A file system is a method used by operating systems to organize, store, and retrieve files on a computer or other digital storage medium, such as a hard drive or flash drive.  It provides a way for users and software programs to access and manage files, and it defines the structure and layout of the storage medium. A file system typically consists of two main components: the file structure and the file management system.  The file structure defines how files are organized and stored on the storage medium, including the directory structure and the file allocation table.  The file management system provides tools and utilities for creating, modifying, and deleting files, as well as for searching for and accessing files. ",1
768,"Files are an essential part of modern computing and have a wide range of applications.  Here are some common applications of files:Storing data: Files are commonly used to store different types of data, such as text, images, videos, and audio. Sharing information: Files can be easily shared between users or systems, making it possible to collaborate on projects and exchange information. Archiving: Files can be used for long-term storage of important data, such as financial records, legal documents, and medical records. Data backup: Files are often used for backing up important data to protect against loss or corruption. Software installation: Files are used to install software programs on a computer or other device. Configuration files: Files are used to store configuration settings for software applications, operating systems, and other system settings. Log files: Files are used to record system events and errors for troubleshooting and debugging purposes. Web content: Files are used to store web pages, images, videos, and other content for online consumption. ",1
769,"A file server is a computer or device that is dedicated to storing and managing files that can be accessed by other computers on a network. File servers typically have large storage capacities and are optimized for serving files to multiple users simultaneously.  They can be used to centralize file storage, making it easier for users to share files and collaborate on projects. In a typical file server setup, the server is connected to the network and contains one or more shared folders that users can access from their own computers.  The server may also provide access control features, such as user authentication and file permissions, to ensure that only authorized users can access or modify files. File servers are commonly used in businesses and organizations where multiple users need to access and share files.  They can also be used in home networks to centralize file storage and enable easier file sharing between family members. ",1
770,"There are several advantages to using a file server:Centralized storage: A file server allows you to centralize file storage and manage all of your files in one location.  This makes it easier to organize, backup, and manage files. Improved collaboration: File servers allow multiple users to access and edit files simultaneously, making it easier for teams to collaborate on projects. Enhanced security: File servers can be configured with access control features, such as user authentication and file permissions, to ensure that only authorized users can access or modify files.  This helps to protect against unauthorized access or data breaches. Simplified backup and recovery: Because files are stored centrally on a file server, it's easier to backup and recover data in the event of a system failure or other disaster. Better performance: File servers are designed to handle large amounts of data and provide fast access to files.  This can improve overall system performance, especially in environments where multiple users are accessing files simultaneously. Cost-effective: By centralizing file storage and management, a file server can reduce the need for individual storage devices on each computer in a network.  This can lead to cost savings over time. ",1
771,"File sharing refers to the process of making files available to other users over a network or the internet.  This can be done in a variety of ways, including through a file server, email attachment, cloud storage service, or peer-to-peer (P2P) network. A peer-to-peer (P2P) network is a type of network where computers or devices communicate directly with each other, rather than through a central server.  P2P networks are commonly used for file sharing, as they allow users to share files with others without the need for a central file server.  In a P2P network, each user's computer acts as both a client and a server, enabling files to be shared directly between users. In a P2P file sharing network, users can search for and download files from other users who have made those files available for sharing.  Examples of popular P2P file sharing networks include BitTorrent, eDonkey, and Gnutella.  While P2P networks can facilitate fast and efficient file sharing, they can also pose security risks and be used for illegal activities such as copyright infringement. ",1
772,"BitTorrent is a peer-to-peer (P2P) file sharing protocol that enables users to share large files over the internet.  The protocol was developed by Bram Cohen in 2001 and has since become one of the most popular P2P file sharing methods on the internet. BitTorrent works by breaking large files into smaller pieces and distributing those pieces across multiple computers on the network.  Users can download and upload different pieces of the file simultaneously, allowing for faster download speeds and more efficient use of network resources. BitTorrent has been used to share a wide variety of files, including movies, music, software, and games.  While the protocol itself is not illegal, it has been associated with copyright infringement and piracy, as users can easily share copyrighted content without permission.  As a result, many countries have taken steps to block or restrict access to BitTorrent and other P2P file sharing networks. ",1
773,"The internet has given rise to a wide range of legal issues, many of which are unique to the online world.  Some of the most common legal issues in the internet world include:Online privacy: Issues related to the collection, use, and sharing of personal information online, such as through social media or online shopping. Intellectual property: Issues related to the ownership, use, and infringement of digital content, such as music, videos, and images. Cyberbullying: Issues related to the use of the internet to harass, intimidate, or bully others, often through social media or online forums. Online fraud: Issues related to the use of the internet to commit fraud, such as phishing scams, identity theft, or online investment scams. Cybercrime: Issues related to the commission of traditional crimes, such as theft, fraud, or harassment, through the use of digital technology. Online censorship: Issues related to the control and regulation of internet content, such as government censorship or restrictions on free speech. E-commerce: Issues related to the sale and purchase of goods and services online, such as online payment security, consumer protection, and contract disputes. ",1
774,"File transfer protocols are methods for transferring files over a network, such as the internet.  There are several different file transfer protocols in use today, each with its own advantages and disadvantages. Some common file transfer protocols include:FTP (File Transfer Protocol): A standard protocol used for transferring files between computers on a network.  FTP is a relatively simple protocol, but it is not secure and does not support encryption. SFTP (Secure File Transfer Protocol): A more secure version of FTP that encrypts data during transfer.  SFTP is often used for transferring sensitive data, such as financial or medical records. FTPS (FTP over SSL): Another secure version of FTP that encrypts data during transfer.  FTPS uses SSL/TLS encryption, which is commonly used for secure web connections. HTTP (Hypertext Transfer Protocol): The protocol used for transferring data over the web, including web pages, images, and other types of files.  While not specifically designed for file transfer, HTTP can be used to transfer files, particularly smaller files. HTTPS (HTTP Secure): A secure version of HTTP that uses SSL/TLS encryption to protect data during transfer.  HTTPS is commonly used for secure web connections and is often used for transferring sensitive data. SCP (Secure Copy Protocol): A secure file transfer protocol that uses SSH (Secure Shell) to encrypt data during transfer.  SCP is commonly used for transferring files between Unix/Linux servers. ",1
775,"File transfer can be implemented in several different ways, depending on the requirements and constraints of the system.  Some common implementations of file transfer include:Client-server file transfer: In this implementation, files are transferred between a client computer and a server computer.  The client sends a request for a file to the server, which then sends the file back to the client.  This is commonly used for file transfer over the internet, such as downloading files from a website. Peer-to-peer file transfer: In this implementation, files are transferred directly between two or more computers on a network, without the need for a centralized server.  Peer-to-peer file transfer is often used for sharing files between individuals or for file sharing networks, such as BitTorrent. Cloud-based file transfer: In this implementation, files are transferred between a local computer and a remote cloud server, which stores and manages the files.  Cloud-based file transfer is commonly used for storing and sharing files online, such as with cloud storage services like Google Drive or Dropbox. Physical media transfer: In this implementation, files are transferred using physical media, such as USB drives or CDs/DVDs.  Physical media transfer is commonly used for transferring large files or for situations where internet access is not available or practical. ",1
776,"The role of computers in the film industry began in the 1970s with the advent of computer-generated imagery (CGI).  At that time, computers were used primarily to generate simple 2D graphics and effects, such as titles and special effects for TV commercials. In the 1980s, the capabilities of computers improved, and they began to be used for more complex 3D graphics and effects in films.  One of the first films to use computer-generated imagery extensively was ""Tron"" (1982), which used CGI to create the film's distinctive digital world. As the technology continued to improve, the use of CGI in films became more widespread, with films like ""Jurassic Park"" (1993) and ""The Matrix"" (1999) using advanced CGI to create realistic creatures and special effects. Today, computers play a vital role in nearly every aspect of the film industry, from pre-production to post-production.  They are used to create and edit scripts, design sets and costumes, film and edit footage, and create special effects and visual effects.  Additionally, computers are used for marketing and distribution, such as creating trailers and promoting films on social media and streaming platforms. ",1
777,"A finite state machine (FSM), also known as a finite automaton, is a mathematical model used to represent a system that can be in one of a finite number of states, and can transition between those states in response to input.  FSMs are used in a variety of applications, including software engineering, circuit design, natural language processing, and many more. An FSM is typically defined by a set of states, a set of inputs, a set of transitions between states, and a set of output actions or behaviors.  The transitions between states are often represented as a directed graph, where each state is a node and each transition is an edge.  The FSM starts in an initial state and transitions between states based on the input it receives.  The output actions or behaviors can be triggered by either the input or the state transition. ",1
778,"Finite state machines (FSMs) have a wide range of applications in many fields, including computer science, engineering, natural language processing, and more.  Here are some common applications of FSMs:Control Systems: FSMs are often used to control the behavior of a system based on its inputs and internal state.  This includes digital circuits, robotics, and automation systems. Software Engineering: FSMs are used in software design and development to model the behavior of complex systems, such as network protocols, user interfaces, and video games. Natural Language Processing: FSMs are used to model the behavior of natural language grammars, allowing for the recognition and generation of sentences in a particular language. Compiler Design: FSMs are used in the lexing and parsing phases of compiler design, where they help to identify and group tokens in the input language. Pattern Recognition: FSMs are used in machine learning and image processing to recognize patterns in data, such as speech, handwriting, and facial recognition. Verification and Testing: FSMs are used to verify the correctness of a system or software design by modeling its behavior and checking it against a given specification or set of requirements. ",1
779,"In the context of computer networking, a firewall is a security system that monitors and controls incoming and outgoing network traffic based on a set of predefined security rules.  It acts as a barrier between an internal network and the internet, or between different segments of an internal network, allowing only authorized traffic to pass through and blocking unauthorized or potentially malicious traffic. Firewalls can be implemented as software or hardware, and can be configured to operate at various levels of the network stack, such as the application layer, transport layer, or network layer.  They use a variety of techniques to identify and block unwanted traffic, such as packet filtering, stateful inspection, and deep packet inspection. ",1
780,"Firewalls serve several important functions in computer networks, including:Traffic filtering: Firewalls can be configured to block or allow incoming and outgoing network traffic based on predefined rules.  This allows the firewall to filter out unwanted or malicious traffic, such as viruses, malware, spam, and unauthorized access attempts. Access control: Firewalls can control access to network resources by allowing or denying traffic based on user identity, network location, time of day, or other factors.  This helps to ensure that only authorized users can access sensitive data or resources. Network address translation (NAT): Firewalls can perform NAT, which allows multiple devices on a private network to share a single public IP address.  This provides an additional layer of security by hiding the internal IP addresses from the internet. Logging and auditing: Firewalls can log network traffic and generate reports on network activity, including attempted attacks and unauthorized access attempts.  This information can be used for troubleshooting, monitoring, and compliance purposes. Virtual private network (VPN): Some firewalls include VPN functionality, which allows remote users to securely connect to the network over the internet.  This enables remote workers to access network resources from outside the office in a secure manner. ",1
781,"FireWire, also known as IEEE 1394, is a high-speed serial bus interface standard used for connecting peripheral devices to computers.  It was developed by Apple in the 1990s and later standardized by the Institute of Electrical and Electronics Engineers (IEEE). FireWire provides a fast and reliable interface for connecting devices such as hard drives, digital cameras, and audio interfaces to a computer.  It supports data transfer rates of up to 400 Mbps (FireWire 400) or 800 Mbps (FireWire 800), which is faster than USB 2. 0. ",1
782,"FireWire, or IEEE 1394, was widely used in the past as a high-speed serial bus interface for connecting peripheral devices to computers.  Some common uses of FireWire included:External hard drives: FireWire was a popular interface for connecting external hard drives to computers, as it provided fast data transfer rates and reliable performance. Digital cameras: Many digital cameras supported FireWire connectivity, allowing users to transfer large numbers of photos or videos quickly and efficiently. Audio interfaces: FireWire was widely used in professional audio recording and production, as it provided low-latency, high-bandwidth connectivity for audio interfaces and digital mixing consoles. Video cameras and digital video recorders: FireWire was a common interface for connecting video cameras and digital video recorders to computers for video editing and transfer. Other peripherals: FireWire was also used for connecting other peripherals such as scanners, printers, and network adapters. ",1
783,"In computers, a flag is a binary value or a status indicator used to represent a condition or a specific state of a processor, a software program, or a hardware device.  Flags are used extensively in computer programming and digital circuits to control and coordinate various operations. Flags are typically represented by a single bit in a processor's register or memory location.  They can be set, cleared, or modified based on the results of arithmetic, logical, or other operations. ",1
784,"In computers, the term ""flash mob"" typically refers to a group of individuals who organize a spontaneous and coordinated online activity, such as a social media campaign or an online protest.  Flash mobs often use social media platforms and other online tools to mobilize large groups of people quickly and efficiently. Flash mobs can take many different forms, ranging from coordinated tweets or Facebook posts to more elaborate online events such as virtual marches or protests.  They are often used to draw attention to a particular cause, raise awareness about an issue, or promote a message or idea. ",1
785,"Smart mobs are a type of online collective action in which large groups of people use mobile technologies and social media platforms to coordinate and participate in a shared activity.  The term ""smart mob"" was coined by author Howard Rheingold in his 2002 book ""Smart Mobs: The Next Social Revolution. ""Smart mobs typically use mobile devices such as smartphones or tablets to connect with one another and to share information, photos, and videos.  They often rely on social media platforms such as Twitter, Facebook, or WhatsApp to coordinate their activities and to communicate with one another in real-time. ",1
786,"A flash drive, also known as a USB flash drive or a thumb drive, is a small, portable data storage device that uses flash memory to store and transfer data.  Flash drives typically have a USB connector on one end and come in a variety of sizes and storage capacities, ranging from a few gigabytes to several terabytes. Flash drives are widely used for storing and transporting digital files such as documents, photos, music, and videos.  They are popular because of their small size, durability, and ease of use.  They can be plugged into any computer or other device with a USB port, making it easy to transfer files between devices or to share files with others. ",1
787," flat panel display is a type of electronic display technology that uses flat, thin panels to display visual information.  Flat panel displays are widely used in a variety of electronic devices, including televisions, computer monitors, smartphones, and tablets. There are several types of flat panel display technologies, including:LCD (liquid crystal display): LCD displays use liquid crystals to selectively filter light and produce an image.  They are widely used in computer monitors, televisions, and other devices. LED (light-emitting diode): LED displays use an array of light-emitting diodes to produce an image.  They are commonly used in large-scale displays such as stadium screens and billboards. OLED (organic light-emitting diode): OLED displays use organic compounds that emit light when an electric current is applied.  They are used in high-end televisions and smartphones. ",1
788,"A floppy disk is a type of magnetic storage device that was widely used in the late 20th century to store and transfer data between computers.  Floppy disks were first introduced in the 1970s and were commonly used for data backup, software distribution, and file transfer. Floppy disks typically consisted of a flexible plastic disk coated with a magnetic material that could store data in the form of magnetized spots on the surface of the disk.  They came in a variety of sizes and formats, including 5. 25-inch and 3. 5-inch disks, and had relatively low storage capacities compared to modern storage devices, ranging from a few hundred kilobytes to a few megabytes. ",1
789,"A flowchart is a visual representation of a process or algorithm that uses symbols and arrows to illustrate the steps and decision points involved in the process.  Flowcharts are commonly used in software development, engineering, and other fields to help visualize and communicate complex processes or systems. In a flowchart, each step in the process is represented by a symbol or shape, such as a rectangle, diamond, or circle.  Arrows are used to show the flow of the process, indicating the sequence of steps and any decision points or loops in the process. Flowcharts can be used to model a wide range of processes, from simple procedures to complex systems involving multiple components and decision points.  They can help identify inefficiencies or bottlenecks in a process and can be used to develop and test new processes or software systems. ",1
790,"In computers, the word ""font"" refers to a set of characters (letters, numbers, symbols, and punctuation marks) that share a similar design or style.  These characters are usually used to create text on a computer screen or in printed documents. Fonts are typically created by designers or type foundries and are stored as digital files on a computer.  Each font file contains information about the shapes and sizes of the characters, as well as information about the spacing between characters and other typographic features. In software applications such as word processors or graphics editors, users can select a font from a list and apply it to the text they are working on.  This will change the appearance of the text to match the selected font's style and design.  There are a wide variety of fonts available for use, ranging from simple and traditional to ornate and decorative. ",1
791,"There are two primary ways to store font data in computers:Bitmap fonts: Bitmap fonts, also known as raster fonts, store the glyph shapes as a bitmap image.  Each character is represented by a set of pixels, with each pixel representing either the foreground or background of the character.  Bitmap fonts are typically smaller in file size and can be quickly displayed on screen, but they can appear jagged or pixelated when enlarged or printed at high resolutions. Outline fonts: Outline fonts, also known as vector fonts, store the glyph shapes as mathematical outlines that describe the shapes of the characters.  The outlines are usually defined using Bezier curves, which can be scaled and adjusted without losing quality.  Outline fonts are typically larger in file size and can take longer to render on screen, but they offer greater flexibility and can be printed at high resolutions without appearing jagged or pixelated. ",1
792,"The basic building block of a Forth program is a word, which can be either a built-in primitive word or a user-defined word.  Words are defined using a combination of other words, which are arranged in a hierarchical structure.  The stack plays a central role in Forth programming, with data being pushed onto and popped off of the stack as needed by the program. Forth programs typically consist of a series of words that are executed in a specific order.  The structure of a Forth program is often represented as a tree, with the words forming the nodes of the tree and the data flowing through the branches. Forth has a simple syntax, with words being defined using a postfix notation.  This means that the arguments to a word come after the word itself, separated by spaces.  For example, to add two numbers in Forth, you would use the following syntax:2 3 +This code pushes the numbers 2 and 3 onto the stack, and then executes the addition operation using the + word. ",1
793,"key feature of Forth is its extensibility.  Forth is designed to be a highly modular language, with words being defined and composed in a hierarchical structure.  This means that Forth programs can be easily extended and customized to meet specific needs, and that new words can be defined and added to the language as needed. ",1
794,"FORTRAN (short for ""FORmula TRANslation"") is a high-level programming language that was originally developed by IBM in the 1950s.  It was the first widely used programming language and is still used today in some scientific and engineering applications. FORTRAN was designed to be a language that could be used for scientific and engineering calculations, which often involve complex mathematical formulas.  It was also designed to be efficient, so that it could run on early computers with limited memory and processing power. FORTRAN has evolved over the years and has several versions, including FORTRAN 77, Fortran 90, Fortran 95, and Fortran 2003.  The language has influenced the development of many other programming languages and is still used in areas such as numerical analysis, scientific computing, and simulation. ",1
795,"In computing, fractals refer to mathematical objects that exhibit self-similar patterns at different scales.  Fractals are generated using complex mathematical equations, and the resulting patterns can be extremely complex and beautiful. Fractals are used in many areas of computing, including computer graphics, image compression, and scientific modeling.  In computer graphics, fractals can be used to create natural-looking landscapes and other complex patterns that would be difficult to generate using traditional methods. One of the most famous fractals is the Mandelbrot set, which is generated by iterating a simple equation over a complex plane.  The resulting image is a complex and intricate pattern that exhibits self-similarity at different scales.  The Mandelbrot set has been used in many areas of science and engineering, including the study of chaos theory, the design of antennas, and the analysis of financial data. ",1
796,"Fractals have many computing applications across a wide range of fields, including computer graphics, scientific modeling, image compression, and data analysis.  Some specific computing applications of fractals are:Computer graphics: Fractals are often used to generate complex, natural-looking patterns in computer graphics, such as landscapes, clouds, and trees.  Fractal-based algorithms are used in 3D modeling software, video game engines, and special effects software. Scientific modeling: Fractal geometry is used in many areas of science and engineering to model complex systems and phenomena, such as fluid flow, turbulence, and geological formations.  Fractal-based models are used in weather forecasting, environmental modeling, and material science. Image compression: Fractal image compression is a technique that uses self-similarity in images to achieve high compression ratios without significant loss of image quality.  Fractal-based compression algorithms are used in many image and video compression standards, such as JPEG and MPEG. Data analysis: Fractal analysis is a method used to analyze complex data sets, such as financial data or physiological signals, by identifying patterns of self-similarity.  Fractal-based algorithms are used in many fields, such as economics, biology, and psychology, to analyze complex data sets and extract meaningful information. ",1
797,"Functional programming languages have many applications in various domains, including:Scientific computing: Functional programming languages are well-suited for scientific computing and numerical analysis, as they provide a concise and expressive way to write complex mathematical equations.  Some popular functional programming languages used in scientific computing include Haskell, R, and MATLAB. Web development: Functional programming languages can be used in web development to build scalable and maintainable web applications.  For example, the Elm programming language is a functional programming language that compiles to JavaScript and is used to build user interfaces. Distributed systems: Functional programming languages are often used in distributed systems, as they provide a way to write code that is less prone to errors and is easier to reason about.  Functional programming languages like Erlang and Scala are often used in building distributed systems. Machine learning: Functional programming languages provide a concise and declarative way to express machine learning models and algorithms.  Languages like Python, R, and Julia have strong functional programming capabilities and are widely used in the machine learning community. Data processing and analysis: Functional programming languages are well-suited for data processing and analysis, as they provide a way to write code that is easily parallelizable and can be optimized for performance.  Languages like Scala and Clojure are often used in big data processing and analysis. ",1
798,"Fuzzy logic is a mathematical framework for dealing with uncertainty and imprecision in decision-making.  Unlike traditional logic, which deals with ""crisp"" or binary values (true or false, 0 or 1), fuzzy logic allows for values that are partially true or partially false. Fuzzy logic is based on the idea of ""fuzzy sets,"" which are sets that allow for degrees of membership.  For example, in a traditional set, an element is either a member or it is not.  But in a fuzzy set, an element can have a degree of membership between 0 and 1.  For example, the statement ""the temperature is hot"" could be assigned a degree of membership based on how hot it is. ",1
799,"eBay is an online marketplace where people can buy and sell a wide variety of goods and services.  It was founded in 1995 and has since become one of the largest e-commerce platforms in the world, with operations in many countries. On eBay, individuals and businesses can list items for sale, and buyers can browse and purchase items using a bidding or fixed-price system.  The site offers a diverse range of products, including electronics, clothing, collectibles, and much more. eBay also provides tools and services to help sellers manage their listings, process payments, and ship items to buyers.  Additionally, the site has a feedback system that allows buyers and sellers to rate and review each other, which helps establish trust and reputation within the community. ",1
800,"E-books, or electronic books, are digital versions of traditional printed books that can be read on electronic devices such as e-readers, tablets, smartphones, and computers.  E-books are typically distributed in digital formats, such as PDF, EPUB, or MOBI, and can include features such as text search, note-taking, highlighting, and bookmarking. ",1
801,"Digital libraries are collections of digital materials, such as books, articles, images, videos, and audio recordings, that are made available online through electronic databases or digital repositories.  These materials can be accessed remotely and are often available to users for free or through paid subscriptions. ",1
802,"Digital libraries offer several advantages over traditional libraries, including:Accessibility: Digital libraries are accessible from anywhere in the world, as long as you have an internet connection.  This means that users can access a vast collection of materials at any time, without having to physically visit a library. Convenience: Digital libraries allow users to search for and access materials quickly and easily from their electronic devices, such as laptops, tablets, or smartphones. Cost-effectiveness: Digital libraries eliminate the costs associated with maintaining a physical library, such as building maintenance, staff salaries, and printing and shipping costs. Preservation: Digital libraries help preserve rare and fragile materials, such as ancient manuscripts and historical documents, by providing digital copies that can be accessed and studied without damaging the original. Collaboration: Digital libraries can facilitate collaboration and knowledge-sharing among researchers, academics, and students from different parts of the world, as they can access and work with the same materials online. Customization: Digital libraries offer users the ability to customize their searches and filter results based on specific criteria, such as language, publication date, and author. ",1
803,"E-commerce has become an increasingly popular way for consumers to shop for goods and services.  Here are some of the most popular e-commerce sectors:Apparel and Fashion: This sector includes everything from clothing and accessories to footwear and jewelry.  It is one of the largest and most competitive e-commerce sectors, with many popular brands and retailers offering online shopping options. Electronics: Electronics is another popular e-commerce sector, as consumers can easily compare prices and features of products such as smartphones, laptops, and other tech gadgets online. Home Goods: This sector includes everything from furniture and home decor to kitchen appliances and bedding.  Online retailers in this sector often offer a wide selection of products and competitive prices. Health and Beauty: Online sales of health and beauty products have been on the rise in recent years, with many consumers preferring the convenience of online shopping for items such as skincare products, vitamins, and supplements. Food and Beverage: Online grocery shopping has become increasingly popular in recent years, with many consumers opting for home delivery or pickup options.  In addition, many specialty food and beverage products are now available for purchase online. Travel and Accommodations: With the rise of online booking platforms, travel and accommodations have become a popular e-commerce sector.  Consumers can easily book flights, hotels, and rental cars online. ",1
804,"E-commerce has become a popular way for consumers to shop for goods and services.  However, with the increasing use of e-commerce, security and privacy issues have become a concern for consumers.  Here are some of the common security and privacy issues in e-commerce:Cybersecurity: Online transactions involve the exchange of sensitive information such as credit card details, personal information, and passwords.  Therefore, e-commerce sites need to have strong cybersecurity measures to prevent data breaches, hacking, and other cyber threats. Payment fraud: Payment fraud is a common issue in e-commerce, where fraudsters use stolen credit card information to make fraudulent purchases.  Online retailers need to have measures in place to detect and prevent payment fraud, such as fraud detection algorithms and multi-factor authentication. Privacy: Consumers are often hesitant to share their personal information online, as they fear it may be used for malicious purposes.  Therefore, e-commerce sites need to have clear privacy policies and ensure that consumers' personal information is secure and not shared with third parties without their consent. Phishing scams: Phishing scams are a common issue in e-commerce, where fraudsters send emails or messages that appear to be from legitimate retailers, asking for personal information or login credentials.  Online retailers need to educate their customers on how to spot and avoid phishing scams. Malware: Malware, such as viruses and Trojans, can infect e-commerce sites and compromise customers' personal information.  Online retailers need to have strong antivirus software and regularly update their systems to prevent malware attacks. ",1
805,"Computer science is a field of study that focuses on the theoretical and practical aspects of computing and information processing.  It encompasses a range of sub-disciplines, including software engineering, computer architecture, artificial intelligence, human-computer interaction, database systems, computer graphics, and algorithms and data structures. At its core, computer science is concerned with the development of algorithms, which are step-by-step procedures for solving problems.  These algorithms can be used to process and manipulate data, automate tasks, and create intelligent systems that can learn and make decisions. Computer science also involves the design and development of software and hardware systems, including operating systems, programming languages, and computer networks.  This requires a deep understanding of the underlying mathematical and logical principles that govern these systems, as well as the ability to write efficient and effective code. In addition to technical skills, computer science also involves creativity and problem-solving.  Computer scientists must be able to think creatively to design new algorithms and systems, and they must be able to apply their knowledge to solve real-world problems. Overall, computer science is a rapidly evolving field that plays a critical role in modern society, powering everything from the internet and social media to healthcare and scientific research.  Its interdisciplinary nature means that computer scientists can work in a wide range of industries and fields, making it a versatile and rewarding career path. ",1
806,"The history of basic programming languages dates back to the mid-1960s.  Early computer programming languages were designed to be easy to use and understand, making them accessible to a wider range of people.  The first basic programming language was FORTRAN (Formula Translation), which was developed in the mid-1950s and used primarily for scientific and engineering applications.  In 1964, John Kemeny and Thomas Kurtz at Dartmouth College created BASIC (Beginners All-purpose Symbolic Instruction Code), which was designed to be easy to learn and use for beginners.  BASIC was widely adopted and became popular among hobbyist and amateur computer users.  In the late 1960s and early 1970s, other basic programming languages were developed, including COBOL (Common Business-Oriented Language), used for business applications, and Pascal, used for teaching programming concepts.  These languages set the foundation for modern programming and helped make programming more accessible and user-friendly.  Over the years, basic programming languages have evolved to include more advanced features, making them suitable for a wider range of applications, but their simplicity and accessibility continue to be their defining characteristics. ",1
807,"A timing attack is a type of security attack that exploits the variability in the time taken by a computer system to perform specific operations in order to extract sensitive information or gain unauthorized access.  The attack is based on the observation of the time differences between operations, which can reveal information about encryption keys, passwords, or other sensitive data.  This information can then be used to compromise the security of the system.  Timing attacks can be carried out both locally, on the attacker's own machine, or remotely, over a network. ",1
808,"The Arithmetic Logic Unit (ALU) is a fundamental component of a computer's central processing unit (CPU) that performs arithmetic and logical operations.  The ALU is responsible for performing basic calculations and comparisons, such as addition, subtraction, multiplication, division, and logic operations such as AND, OR, NOT, XOR, and others.  The ALU receives inputs from the CPU's internal register and memory, performs the specified operation, and then stores the result back in the register or memory.  The operation performed by the ALU is determined by the instruction sent to the CPU by the computer's software.  The performance and efficiency of the ALU are critical to the overall performance of the computer, as it is responsible for executing the majority of the operations performed by the CPU.  The design of the ALU, including the number of bits it can operate on and the number of operations it can perform in a single clock cycle, is an important factor in determining the performance of the computer.  In modern computers, the ALU is integrated into the CPU as part of the digital circuit, and its design is constantly being improved to increase performance and efficiency. ",1
809,"An applet is a small, lightweight Java program that is intended to be run within a web browser.  Applets are embedded in HTML pages and can be executed within a web browser's Java Virtual Machine (JVM).  They are often used to provide interactive and dynamic content, such as animations, games, and other graphical user interface (GUI) elements.  The applets are typically written in Java, a programming language that is known for its platform independence, meaning that the same Java applet can run on different operating systems and devices.  This makes applets ideal for creating web-based applications that can be accessed from any device with a web browser and a JVM.  Applets are usually restricted in terms of the resources they can access and the actions they can perform, for security reasons.  This helps prevent malicious applets from accessing sensitive information on the user's computer or from performing harmful actions.  Applets have been largely replaced by alternative technologies such as JavaServer Pages (JSP), JavaServer Faces (JSF), and JavaScript, which offer greater flexibility and better performance for web-based applications.  However, applets are still used in some areas, such as scientific simulations, educational games, and other specialized applications. ",1
810,"A chipset is a group of electronic components in a computer or device that manages the data flow between the processor, memory, and other peripherals.  The chipset acts as an intermediary between the various components of the system, controlling the communication and coordinating the exchange of information between them.  The chipset is typically composed of two parts: the northbridge and the southbridge.  The northbridge manages communication between the processor, memory, and high-speed devices, such as graphics card, while the southbridge manages communication with lower-speed peripherals, such as USB and audio devices.  The chipset plays a critical role in the performance and stability of a computer, as it is responsible for controlling many of the system's functions, such as memory and I/O management, interrupt handling, power management, and others.  The chipset also determines the capabilities of the system, such as the supported memory types and speeds, the number of USB ports, and other features.  The design and features of the chipset can have a significant impact on the overall performance and functionality of a computer, and chipset manufacturers continuously improve and update their products to provide better performance and support new technologies. ",1
811,"A class is a blueprint or template for creating objects in object-oriented programming.  A class defines the properties and behaviors that objects created from the class will have.  Classes are a key concept in object-oriented programming, as they provide a way to encapsulate data and logic into reusable and modular components.  A class can define data members, also known as fields or attributes, which represent the state of an object, as well as methods, which represent the behavior of an object.  Classes can also define constructors, which are special methods used to create and initialize objects of the class, and destructors, which are used to clean up resources when an object is no longer needed.  Classes can also have inheritance relationships, where one class can inherit the properties and behaviors of another class.  This allows for the creation of specialized classes that build upon the properties and behaviors of more general classes. In summary, a class is a blueprint for creating objects, encapsulating both data and behavior into a reusable component, and providing the foundation for the object-oriented programming paradigm. ",1
812,"Access Control Matrix (ACM) is a security model used to specify the access rights of subjects to objects in a computer system.  The ACM is a table or matrix that lists all the subjects (users, processes, programs) and objects (files, directories, devices) in a system, along with the access rights that each subject has to each object.  The access rights can include read, write, execute, and delete permissions. In an ACM, each row represents a subject and each column represents an object.  Each cell in the matrix indicates the access rights of the corresponding subject to the corresponding object.  For example, a cell may contain the value ""rwx"" to indicate that the subject has read, write, and execute permissions to the object. The ACM provides a way to control and manage access to resources in a system by ensuring that only authorized users or processes can access specific objects.  It is also useful for auditing and tracking user activity, as it provides a clear view of who has access to what resources in the system. ACMs can be implemented using various access control models, such as discretionary access control (DAC), mandatory access control (MAC), and role-based access control (RBAC).  Each of these models uses different methods for assigning access rights to subjects based on their identity, job function, or security clearance level. ",1
813,"Active OS fingerprinting is a technique used to identify the operating system running on a remote system or device by sending specific network packets and analyzing the responses.  This technique involves actively probing the target system by sending network packets that elicit specific responses from the operating system. Active OS fingerprinting is typically performed by security professionals or network administrators to identify vulnerable systems and determine the appropriate security measures to protect the network.  Attackers may also use this technique to identify potential targets for exploits or attacks. The process of active OS fingerprinting involves sending a series of specially crafted packets to the target system and analyzing the responses.  The packets may contain specific flags or options that trigger certain responses from the operating system.  These responses can then be compared to a database of known operating system fingerprints to determine the operating system running on the target system. Active OS fingerprinting is considered an invasive technique as it involves actively probing the target system, and it may trigger security alerts or firewalls on the target system.  However, it is often more accurate than passive fingerprinting techniques, which rely on observing network traffic and may be less reliable in some situations. ",1
814,"Active Server Pages (ASP) is a server-side scripting language used for creating dynamic web pages and web applications.  It was first introduced by Microsoft in the late 1990s as a technology for building server-side web applications on Windows servers. ASP scripts are written in a scripting language such as VBScript or JScript and are embedded in HTML pages.  When a user requests an ASP page, the web server executes the ASP script on the server and generates dynamic HTML pages to be sent back to the client's browser. ASP provides a wide range of functionality for creating dynamic web pages, including database connectivity, form processing, session management, and authentication.  It can also be used with other web technologies such as Cascading Style Sheets (CSS), JavaScript, and XML. In addition to classic ASP, Microsoft also developed ASP. NET, which is a more modern web application framework that supports multiple programming languages, including C#, Visual Basic, and F#.  ASP. NET is designed to be more modular, scalable, and secure than classic ASP and provides a wide range of built-in features for creating dynamic web applications. ",1
815,"ActiveX is a set of technologies developed by Microsoft that allows software components to be reused across different applications and platforms.  ActiveX is based on the Component Object Model (COM) and allows components to be developed in various programming languages such as C++, Visual Basic, and Java. ActiveX components are designed to be installed on a client's computer and accessed by other applications, such as web browsers or office applications.  They can provide a range of functionality, including user interface controls, data access components, and multimedia playback components. ActiveX components can be downloaded and installed automatically from web servers when a user visits a website that uses them.  This feature is often used by web developers to provide additional functionality to their web pages, such as multimedia playback, interactive forms, and dynamic content. While ActiveX provides many benefits in terms of code reuse and functionality, it has also been criticized for its security vulnerabilities.  ActiveX controls can potentially allow malicious code to be executed on a user's computer if they are not properly secured or validated.  As a result, many web browsers have implemented security measures to prevent unauthorized installation of ActiveX controls. ",1
816,"Advisory and Notification Markup Language (ANML) is an XML-based language used for defining notification and alert messages in a machine-readable format.  ANML was developed to standardize the format of notifications and alerts, making it easier for different systems and applications to exchange this information. ANML is designed to be flexible and extensible, allowing users to define their own custom elements and attributes.  It includes a set of predefined elements and attributes for common notification and alert scenarios, such as device failures, network events, and security breaches.  ANML messages can also include instructions for how the recipient system should respond to the notification, such as acknowledging receipt, taking corrective action, or notifying a human operator. ANML is used in a variety of applications, including network management, security monitoring, and industrial control systems.  It is an open standard, which means that anyone can use and implement it without paying licensing fees. ",1
817,"In computer science, an algorithm is a set of instructions that specifies a sequence of steps to be taken to solve a problem or accomplish a specific task.  An algorithm is typically designed to operate on some input data, manipulate that data in some way, and produce a desired output. Algorithms are a fundamental concept in computer science, as they provide a way to describe and analyze the behavior of programs.  Algorithms can be expressed in various ways, including natural language, pseudocode, flowcharts, and programming languages.  They can be simple or complex, and can range from basic mathematical operations to complex machine learning models. An algorithm is considered efficient if it can solve a problem in a reasonable amount of time and using a reasonable amount of memory.  The complexity of an algorithm is usually measured in terms of its time and space complexity, which are related to the amount of time and memory required for the algorithm to complete its task. Algorithms are used in a wide variety of applications, including search engines, machine learning, cryptography, and data analysis.  They are a key component of computer science, and the development of new algorithms is an ongoing area of research and innovation. ",1
818,"In computer science, Apache refers to the Apache HTTP Server, which is an open-source web server software that is used to serve web pages over the internet.  Apache is one of the most widely used web servers in the world, and it is used by many popular websites and web applications. Apache is developed and maintained by the Apache Software Foundation, and it is available for multiple operating systems, including Windows, Linux, and macOS.  It is written in the C programming language and is highly customizable, with support for various plugins and modules that extend its functionality. Apache is a powerful and flexible web server that supports a variety of protocols and technologies, including HTTP, HTTPS, CGI, FastCGI, and more.  It also includes features like virtual hosting, authentication and access control, and URL rewriting, which allow developers to create complex and dynamic web applications. Apache is known for its stability, security, and performance, and it is often used in combination with other software technologies like PHP, MySQL, and Python to create complete web application stacks.  It is also highly extensible, with a large and active community of developers who contribute to its ongoing development and support. ",1
819,"ASCII (American Standard Code for Information Interchange) is a character encoding standard used to represent text in computers and other devices.  It was developed in the 1960s and has been widely used in computer systems and communication protocols ever since. ASCII defines a set of 128 characters, including letters, numbers, punctuation marks, and control characters, each of which is represented by a unique 7-bit binary code.  The ASCII character set includes uppercase and lowercase letters, digits, basic punctuation, and various control characters such as carriage return and line feed. ASCII is a widely used standard and is supported by virtually all modern computer systems.  It is used in various applications, such as email, text files, and programming languages.  However, because it only includes 128 characters, it has limitations in representing non-Latin languages and complex symbols, which led to the development of other character encoding standards like Unicode. Despite the limitations, ASCII remains an important standard in computing and is still used extensively in certain applications, such as programming and data communication, where it provides a simple and reliable means of representing text. ",1
820,"ADSL (Asymmetric Digital Subscriber Line) is a type of broadband internet connection technology that uses existing copper telephone lines to transmit digital data at high speeds.  ADSL allows users to access the internet and other online services while still being able to make phone calls on the same line. ADSL is ""asymmetric"" because it allows for faster download speeds than upload speeds.  This is because it is designed to accommodate the typical internet usage patterns of most people, who tend to download more data than they upload.  In an ADSL connection, the bandwidth is divided into two channels, with one channel being used for downloading data at high speeds, while the other channel is used for uploading data at lower speeds. ADSL requires a special modem to be installed on the user's premises, which communicates with the internet service provider's (ISP) equipment at the telephone exchange.  The modem uses a technology called frequency division multiplexing (FDM) to separate the voice and data signals on the same phone line. ADSL is widely available and is one of the most popular types of broadband internet connections worldwide.  It is generally less expensive than other high-speed internet options like fiber-optic internet and cable internet, and it is suitable for many common online activities, such as web browsing, streaming video and audio, and online gaming.  However, ADSL speeds can be affected by factors like the distance from the user's premises to the telephone exchange and the quality of the phone line, which can result in slower speeds in some cases. ",1
821,"In computer science, an attack refers to any deliberate and malicious attempt to compromise the security or integrity of a computer system, network, or application.  There are various types of attacks, each with its own methods and objectives, but all are aimed at exploiting vulnerabilities or weaknesses in the target system. Some common types of computer attacks include:Malware attacks - these involve the use of malicious software like viruses, trojans, and worms to gain unauthorized access to a system, steal data, or cause damage to the system. Phishing attacks - these involve the use of deceptive emails or websites to trick users into divulging sensitive information like passwords or financial information. Denial-of-service (DoS) attacks - these involve overwhelming a target system with traffic or requests to disrupt its normal operation and make it inaccessible to users. Man-in-the-middle (MitM) attacks - these involve intercepting communications between two parties to steal data or manipulate the communication. Password attacks - these involve trying to guess or crack passwords to gain access to a system or application. Social engineering attacks - these involve using psychological manipulation techniques to trick people into revealing sensitive information or performing actions that compromise security. Attacks can have serious consequences, including data theft, system downtime, financial loss, and reputational damage.  Protecting against attacks requires implementing strong security measures, such as firewalls, antivirus software, encryption, and user authentication protocols, as well as ongoing monitoring and updates to stay ahead of evolving threats. ",1
822,"Authentication is the process of verifying the identity of a user or system, typically in order to grant access to a resource or system.  Authentication ensures that only authorized users can access the resource or system, and that their actions can be attributed to their unique identity. Authentication can be accomplished using various methods, including:Username and password - the most common form of authentication, where the user enters a username and password that is verified by the system. Biometric authentication - using physical traits like fingerprints, facial recognition, or iris scans to verify a user's identity. Multi-factor authentication - requiring multiple forms of authentication, such as a password and a security token, to increase security. Single sign-on - allowing users to log in once and access multiple resources without needing to enter their credentials again. Authentication is a crucial component of security in computer systems and networks, as it helps to prevent unauthorized access, data breaches, and other security incidents.  Proper authentication practices involve implementing strong passwords or passphrases, regularly updating passwords, using multi-factor authentication wherever possible, and restricting access to sensitive resources to only authorized users. ",1
823,"In computer science, a backdoor is a hidden method of bypassing normal authentication or security controls in a system, allowing a user to gain unauthorized access to the system or its data.  A backdoor is usually created by the system's designer or developer as a means of accessing the system in case of an emergency or for troubleshooting purposes. However, backdoors can also be created maliciously by attackers to gain unauthorized access to a system or network.  Backdoors can be hidden in various parts of a system, including software applications, operating systems, and network infrastructure. Backdoors can be difficult to detect, as they are designed to remain hidden and bypass normal security controls.  However, they can be discovered through regular security audits, network monitoring, and analysis of system logs. Backdoors are considered a serious security threat, as they can allow attackers to access sensitive information, steal data, and carry out other malicious activities.  Preventing backdoors requires implementing strong security controls, such as access controls, authentication mechanisms, and encryption, and regularly monitoring and testing systems for vulnerabilities. ",1
824,"In computer science, a batch file is a text file containing a series of commands that can be executed by a computer's command-line interpreter, typically in a batch processing mode.  Batch files are commonly used in Windows operating systems to automate repetitive tasks, simplify administrative tasks, and perform system maintenance. Batch files are created using a text editor, and can contain a variety of commands and instructions, such as file manipulation, system configuration, and network management.  Batch files typically have a "". bat"" file extension, and can be run by double-clicking on the file or by entering its name in the command prompt. Batch files are useful for automating tasks that would otherwise require manual input or repetitive typing, and can save time and increase productivity.  For example, a batch file could be created to automatically backup important files, install software updates, or configure system settings. However, batch files can also be used maliciously to execute unauthorized commands or scripts, and can be a security risk if not properly secured.  It is important to be cautious when downloading and running batch files from unknown sources, and to ensure that the commands contained in the batch file are safe and secure. ",1
825,"In computer science, a binder refers to a software tool that is used to combine two or more files or programs into a single executable file.  Binders are commonly used by malware authors to merge a legitimate program or file with a malicious payload, allowing the malware to execute without being detected by antivirus software or other security measures. Binders work by taking two or more files and merging them into a single executable file, which can then be run on a target system.  Binders can be designed to include a variety of malicious payloads, including backdoors, keyloggers, and other types of malware. Binders can be used to bypass security measures that are designed to detect and prevent malware infections, as the merged files may appear to be a legitimate program or file.  This can make it more difficult for security professionals to detect and remove the malware, and can lead to serious security breaches and data loss. Binders are a security threat, and it is important to use strong security measures, such as antivirus software and network monitoring tools, to detect and prevent malware infections.  It is also important to be cautious when downloading and running software or files from unknown sources, and to ensure that all software and systems are kept up-to-date with the latest security patches and updates. ",1
826,"Biometric identification systems are a type of authentication system that uses unique biological characteristics to verify the identity of an individual.  These systems rely on biometric data, which can include fingerprints, iris scans, facial recognition, voiceprints, and other physiological or behavioral traits. Biometric identification systems typically work by capturing biometric data from an individual and comparing it to a stored database of biometric data to verify the individual's identity.  The system may also perform additional verification steps, such as checking against a list of authorized users or requiring the user to enter a password or PIN. Biometric identification systems offer several advantages over traditional authentication systems, such as:Increased security - Biometric data is unique to each individual and cannot be easily replicated or stolen, making it more difficult for attackers to gain unauthorized access. Convenience - Biometric identification systems are easy to use and can provide fast and accurate authentication, eliminating the need for users to remember passwords or carry physical tokens. Cost-effective - Biometric identification systems can be cost-effective in the long term, as they eliminate the need for expensive physical tokens or manual verification processes. However, biometric identification systems also have some limitations and potential drawbacks.  These can include:Privacy concerns - Biometric data is highly personal and sensitive, and there may be concerns about how it is collected, stored, and used. Accuracy and reliability - Biometric identification systems may not always be accurate or reliable, as factors such as lighting conditions or changes in an individual's appearance can affect the accuracy of the biometric data. Implementation challenges - Biometric identification systems can be complex and costly to implement, requiring specialized hardware and software, and may require significant training and support for users. Overall, biometric identification systems offer a promising approach to authentication and security, but must be implemented carefully and with appropriate safeguards to ensure privacy and accuracy. ",1
827,"Bluetooth is a wireless technology that allows devices to communicate with each other over short distances without the need for cables or wires.  Bluetooth is commonly used for connecting devices such as smartphones, laptops, headphones, speakers, and smartwatches. Bluetooth technology works by using radio waves to transmit data between devices.  When two Bluetooth-enabled devices are within range of each other, they can establish a connection and begin communicating.  Bluetooth technology uses a set of standardized protocols and profiles to ensure compatibility between different devices. Bluetooth technology offers several advantages over traditional wired connections, including:Convenience - Bluetooth allows for easy and wireless connections between devices, eliminating the need for cables or wires. Compatibility - Bluetooth is a standardized technology, ensuring that devices from different manufacturers can communicate with each other. Low power consumption - Bluetooth technology is designed to use low power, making it ideal for use in portable devices such as smartphones and smartwatches. Security - Bluetooth uses encryption and other security measures to ensure that data transmitted between devices is secure and private. However, Bluetooth technology also has some limitations, including:Range - Bluetooth connections are typically limited to a range of around 10 meters, which can restrict the use of some devices. Interference - Bluetooth connections can be affected by other wireless devices in the vicinity, which can cause interference or signal degradation. Overall, Bluetooth technology is a popular and widely used wireless technology that offers convenience, compatibility, and low power consumption for connecting devices over short distances. ",1
828,"The Blue Screen of Death (BSOD) is an error screen displayed on a Windows computer when the operating system encounters a critical error that it cannot recover from.  The screen appears as a blue background with white text and is also known as a ""stop error"" or a ""system crash. ""The BSOD is typically caused by hardware or software issues, such as faulty drivers, system files, or memory problems.  When a critical error occurs, Windows displays the BSOD to prevent further damage to the system and to allow the user to troubleshoot the problem. The BSOD screen provides a message that includes a stop code, which can help identify the cause of the error.  The stop code is a hexadecimal code that indicates the type of error that occurred, such as a driver issue or a memory problem. When a BSOD occurs, the computer will typically restart automatically, although it may also display a memory dump or other diagnostic information.  To resolve the issue, users can try to identify the cause of the error using the stop code and other diagnostic information provided on the BSOD screen. BSODs can be frustrating and disruptive, but they are an important safety feature of the Windows operating system that helps prevent further damage to the system when critical errors occur. ",1
829,"Broadband refers to a type of high-speed internet connection that provides faster data transfer rates than traditional dial-up connections.  Broadband connections are typically ""always on,"" meaning that users can access the internet at any time without having to dial in. Broadband connections can use a variety of technologies, including digital subscriber lines (DSL), cable modems, fiber optic cables, and wireless connections such as Wi-Fi and satellite.  These technologies allow for faster data transfer rates, which enable users to access and download content such as videos, music, and files more quickly. ",1
830,"A Bulletin Board System (BBS) is a computer system that allows users to communicate with each other and share information through a text-based interface. BBSes were popular in the 1980s and 1990s before the widespread adoption of the internet.  Users could dial into the system using a modem and interact with other users through message boards, chat rooms, and file sharing. BBSes were often run by individuals or small groups of enthusiasts and were tailored to specific communities, such as hobbyists, gamers, or hackers.  They offered a platform for users to share their interests, knowledge, and experiences, and were often characterized by a sense of community and camaraderie. Today, BBSes are less common, but some still exist as a niche hobby or retro computing project.  They have been largely replaced by online forums, social media, and other web-based communication platforms. ",1
831,"A Business Continuity Plan (BCP) is a set of strategies and procedures that a business or organization has in place to ensure that essential business functions can continue in the event of a disruption. The purpose of a BCP is to minimize the impact of a disruption on business operations and to facilitate a timely and effective recovery.  A BCP typically includes a detailed analysis of potential risks and threats, such as natural disasters, cyber attacks, or power outages, and outlines specific steps to take in the event of a disruption. A typical BCP may include provisions for:Emergency response and communication proceduresBackup and recovery of critical data and systemsAlternative work locations and equipmentContinuity of key business processesTraining and awareness for employeesImplementing a BCP requires a comprehensive and ongoing effort to identify potential risks, evaluate their impact on business operations, and develop and test strategies to ensure continuity.  By having a well-designed and tested BCP in place, businesses can reduce the risk of disruption and minimize the impact of unexpected events on their operations and bottom line. ",1
832,"Disk cache is a technology used to improve the performance of a computer's hard drive by temporarily storing frequently accessed data in a fast memory buffer. When a computer accesses data from a hard drive, it takes time to read the data from the physical disk.  Disk cache works by keeping a copy of frequently accessed data in a faster memory buffer, such as RAM, so that it can be accessed more quickly.  This can significantly improve the speed of data retrieval and overall system performance. Disk cache operates in two main ways:Read cache: This involves storing frequently accessed data from the hard drive in the cache memory.  When the computer needs to read the same data again, it can be retrieved quickly from the cache instead of being read from the hard drive again. Write cache: This involves temporarily storing data that is being written to the hard drive in the cache memory.  This allows the computer to continue working while the data is being written to the slower hard drive, improving overall system performance. However, while disk cache can improve performance, there is a risk of data loss if the cache is not properly managed.  If power is lost or the system crashes before the data is written back to the hard drive, the cached data can be lost.  Therefore, disk cache systems typically incorporate safeguards to ensure that cached data is properly saved before being purged from the cache memory. ",1
833,"File cache is a type of cache memory used by computer operating systems to improve the performance of file system access. When a file is accessed, the operating system will store a copy of the file's contents in the file cache.  If the file is accessed again, the operating system can retrieve the file from the file cache instead of reading it from the slower hard drive.  This can significantly improve the speed of file access and overall system performance. File cache works by utilizing the available memory in a computer's RAM to store frequently accessed files.  The operating system manages the file cache, determining which files should be cached based on their access patterns and available memory.  The size of the file cache is typically dynamic and can vary based on the available system memory and the demands of other processes running on the system. File cache is particularly effective for improving the performance of applications that frequently access small files, such as web browsers and document editors.  By keeping frequently accessed files in cache, these applications can retrieve them quickly and efficiently, improving overall performance and reducing wait times. However, like any cache, file cache has limitations.  If the file being accessed is larger than the available cache memory, the operating system may need to read the file from the hard drive, resulting in slower performance.  Additionally, the effectiveness of the file cache can be affected by the access patterns of other processes running on the system, which can lead to cache ""thrashing"" and reduced performance. ",1
834,"Web cache, also known as a browser cache or HTTP cache, is a technology used by web browsers and web servers to improve the speed and efficiency of web page access. When a web page is loaded, the web browser typically downloads all of the associated resources, such as images, scripts, and stylesheets, from the web server.  With web cache, the browser can store a local copy of these resources in cache memory, so that if the user returns to the same page, the browser can load the resources from cache instead of re-downloading them from the server.  This can result in faster page load times, reduced network traffic, and lower server load. Web cache works by utilizing the available memory in a computer's RAM to store frequently accessed web page resources.  The cache can be managed by the web browser, which determines which resources should be cached based on their access patterns and available memory.  Web cache can also be implemented at the server level, where the server stores a cache of frequently accessed pages or resources to reduce server load and improve response times. ",1
835,"A Computer Security Incident Response Team (CSIRT) is a group of professionals responsible for quickly identifying and responding to computer security incidents.  They follow established procedures to contain, analyze, and mitigate incidents, and may also be responsible for ongoing monitoring, vulnerability identification, and policy development. ",1
836,"A Certification Authority (CA) is a trusted entity that issues digital certificates to verify the identity of individuals, organizations, or other entities.  A digital certificate is a form of electronic identification that contains information about the identity of the certificate holder, including their public key, which can be used to encrypt and decrypt messages. CAs are responsible for verifying the identity of certificate holders and issuing certificates that are trusted by other entities in the digital ecosystem.  This helps to establish secure communications between parties and ensure the integrity and confidentiality of data transmitted over the internet. CAs operate under a hierarchical structure, with root CAs at the top of the hierarchy and subordinate CAs below them.  Root CAs issue certificates to subordinate CAs, which can then issue certificates to end-users.  This structure ensures that trust is maintained throughout the digital ecosystem and that certificates are only issued to trusted parties. ",1
837,"In computer science, a checksum is a value calculated from a set of data, typically a file or a message, used to detect errors that may have occurred during transmission or storage. The checksum is usually a small fixed-size value that is generated using an algorithm, such as the cyclic redundancy check (CRC) algorithm.  The algorithm takes the data as input and produces a unique checksum value that is different for each set of data. Checksums are commonly used to verify the integrity of data during transmission or storage.  When a file is transmitted over a network or stored on a disk, its checksum is calculated and transmitted or stored alongside the file.  When the file is received or accessed, the checksum is recalculated and compared to the original checksum.  If the two checksums match, it is assumed that the data has not been corrupted or modified during transmission or storage. Checksums can also be used to detect accidental or intentional changes to data.  If even a single bit of the data is changed, the checksum value will be different, indicating that an error has occurred.  Checksums can be used to detect errors in a wide range of applications, including network protocols, file systems, and databases. ",1
838,"A clickwrap agreement is an online contract where the user must click a button or check a box to agree to the terms and conditions before using a software or service.  It's commonly used for efficiency and convenience, and typically includes legal terms and limitations of liability.  Courts generally recognize clickwrap agreements as enforceable contracts. ",1
839,"Command and Control Warfare (C2W) is a military strategy that involves the use of various tactics to disrupt or disable an adversary's command and control systems while protecting one's own systems. C2W aims to achieve information superiority by denying an adversary access to critical information, degrading their decision-making ability, and reducing their effectiveness on the battlefield. Some of the tactics used in C2W include electronic warfare, deception, and psychological operations.  Electronic warfare involves using electromagnetic radiation to disrupt an adversary's electronic systems.  Deception involves misleading an adversary about one's own capabilities and intentions.  Psychological operations involve using information and propaganda to influence an adversary's behavior and decision-making. ",1
840,"Computer Metal Oxide Semiconductor (CMOS) is a low-power technology used in computer systems to store configuration data, such as the system clock and boot sequence.  It is powered by a small battery and is located on the motherboard.  CMOS allows for reliable storage of critical configuration data. ",1
841,"Confidentiality is the practice of keeping information private and protected from unauthorized access or disclosure, which is legally required in many contexts.  It involves safeguarding information and limiting access to authorized individuals with a legitimate need to know.  Confidentiality is important for trust, privacy, and protection of sensitive information, and violating it can result in legal consequences or damage to relationships. ",1
842,"A cookie is a small piece of data that is stored on a user's computer by a website they visit. Cookies are commonly used in computer science to help websites remember user preferences, login information, or other details about a user's interaction with the website. When a user visits a website, the website sends a cookie to the user's web browser, which stores the cookie on the user's computer.  The next time the user visits the website, the browser sends the cookie back to the website, allowing the website to recognize the user and retrieve any previously stored information. Cookies can be used for various purposes, such as keeping a user logged in to a website, remembering items in a shopping cart, or tracking user behavior for analytics purposes. While cookies are generally harmless, they can also be used for tracking and advertising purposes, which has led to concerns about user privacy.  Modern web browsers typically provide options for users to manage cookies, including the ability to block or delete cookies from certain websites. ",1
843,"Critical Infrastructure Protection (CIP) refers to a set of measures and strategies designed to safeguard the security and resilience of critical infrastructure systems, such as power grids, water supplies, transportation systems, and communication networks. CIP involves identifying and assessing the risks to critical infrastructure systems, developing and implementing security policies and procedures, and deploying various technical solutions to detect and prevent cyber threats and physical attacks. CIP is important for ensuring the continuity of essential services and protecting public safety and national security. In many countries, CIP is regulated by government agencies or industry standards, and organizations that operate critical infrastructure systems are required to implement specific security measures and report any incidents or vulnerabilities to relevant authorities. ",1
844,"Critical Information Infrastructure (CII) refers to the information and communication technology (ICT) systems and networks that are vital to the functioning of a society or an economy. CII includes various types of infrastructure, such as telecommunication networks, financial systems, transportation systems, energy grids, and government databases, that are essential for providing services and enabling economic activity. Because CII is essential for the functioning of society, it is also a potential target for cyber attacks or other forms of disruption.  Protecting CII is therefore critical for ensuring the resilience and continuity of essential services, as well as protecting national security. Many countries have established policies and regulations for protecting CII, including implementing specific security measures, conducting risk assessments, and establishing incident response plans.  In some cases, government agencies may also collaborate with private sector organizations that operate CII to coordinate security efforts and share information about potential threats or vulnerabilities. ",1
845,"""Cyberpunk"" is a term that originated in the science fiction genre and refers to a subgenre of science fiction that often focuses on a dystopian future where technology has advanced to the point of becoming an integral part of society, but has also brought about significant social and cultural changes. Cyberpunk typically features protagonists who are hackers, rebels, or outcasts who use advanced technology and their skills to navigate the often corrupt and oppressive society they live in.  The genre is often characterized by a dark, gritty aesthetic, and themes of artificial intelligence, virtual reality, and the blurring of boundaries between human and machine. In recent years, ""cyberpunk"" has also become associated with a broader cultural movement that draws inspiration from the genre's themes and aesthetic.  This can include fashion, music, and other forms of art that incorporate elements of cyberpunk into their design and presentation. ",1
846,"Cyclic Redundancy Check (CRC) is an error-detection code used in digital communication networks and storage devices.  It works by generating a code for transmitted data using a mathematical algorithm, and checking the received data against the generated code.  If the two match, the data is assumed to be error-free.  It is widely used for error detection, but it cannot correct errors once they are detected. ",1
847,"The ""darknet"" refers to a private and encrypted network that is hidden from the public internet and is only accessible through specialized software, configurations, and authorization.  Darknets are often used to provide anonymous and untraceable communication channels for individuals or groups who want to keep their online activities private or hidden from surveillance. The most well-known darknet is the Tor network, which is a decentralized and anonymous network that allows users to browse the internet anonymously, access hidden services, and communicate privately.  However, not all darknets are used for legitimate purposes, and some are used to facilitate illegal activities such as drug trafficking, hacking, and online marketplaces for illicit goods and services. It's important to note that the term ""darknet"" is often conflated with the ""deep web,"" which refers to parts of the internet that are not indexed by search engines and are not easily accessible through standard web browsers.  While some parts of the deep web may be accessed through darknets, the two terms are not interchangeable, and not all parts of the deep web are illicit or illegal. ",1
848,"The Data Encryption Standard (DES) is a symmetric-key encryption algorithm that was widely used to protect data confidentiality in electronic communication and storage systems in the 1970s and 1980s. The DES algorithm works by using a single secret key to encrypt and decrypt data.  The key is used to perform a series of mathematical operations on the plaintext to produce the ciphertext, and the same key is used to reverse the operations and recover the plaintext from the ciphertext.  The key size for DES is 56 bits, and the algorithm uses a block cipher with a fixed block size of 64 bits. Although DES was considered a secure encryption standard for many years, advances in computing power and cryptographic attacks eventually rendered the algorithm vulnerable to brute-force attacks.  As a result, the Advanced Encryption Standard (AES) was adopted as a replacement for DES in the early 2000s.  Despite its obsolescence, DES still remains in use in some legacy systems and applications. ",1
849,"In computer science, a Demilitarized Zone (DMZ) is a network segment that is isolated from the internal network and is exposed to the public internet.  The purpose of a DMZ is to provide a secure location for servers and services that need to be accessible from the internet, such as web servers, email servers, and file transfer servers, while protecting the internal network from external threats. A DMZ is typically implemented by placing a firewall or a router between the internal network and the DMZ.  The firewall is configured to allow traffic from the internet to the DMZ while blocking traffic to the internal network.  This allows external users to access services in the DMZ, while protecting the internal network from attacks that may originate from the internet. However, it's important to note that a DMZ is not a completely secure solution and does not provide full protection against all types of attacks.  Additional security measures such as intrusion detection and prevention systems, strong authentication, and access control policies should be implemented to ensure the security of the DMZ and the internal network. ",1
850,"A Denial of Service (DoS) attack is a type of cyber attack in which an attacker attempts to make a network, website, or service unavailable to users by overwhelming it with traffic or other forms of disruptive activity. DoS attacks typically involve flooding a targeted system with a large volume of traffic, such as network packets or web requests, in order to consume the system's resources and make it unavailable to legitimate users.  Attackers may also use other methods to disrupt the targeted system, such as sending malformed data, exploiting vulnerabilities in the system's software, or launching attacks from multiple sources to make it more difficult to block the traffic. DoS attacks can be launched from a single source or from multiple sources, in which case it is called a Distributed Denial of Service (DDoS) attack.  DDoS attacks are typically carried out using botnets, which are networks of compromised computers that are controlled remotely by the attacker. DoS attacks can have serious consequences for organizations, including lost revenue, damage to reputation, and legal liability.  To protect against DoS attacks, organizations can implement measures such as network filtering, load balancing, and anti-DDoS software, as well as practicing good security hygiene, such as keeping software up-to-date and using strong authentication methods. ",1
851,"A Digital Certificate, also known as a public key certificate, is an electronic document that is used to verify the authenticity of a digital entity, such as a website, server, or person. Digital certificates work by linking a public key to an identity, such as the name of a website or organization.  The certificate is issued by a trusted third party, called a Certificate Authority (CA), who verifies the identity of the entity and signs the certificate with their own digital signature.  This creates a chain of trust, in which a user can trust the entity's identity because they trust the CA that issued the certificate. When a user connects to a website or server that has a digital certificate, their browser or application checks the certificate to ensure that it is valid and issued by a trusted CA.  If the certificate is valid, the user's browser or application can establish a secure connection with the entity using encryption to protect the data that is transmitted between them. Digital certificates are commonly used to secure online transactions, such as online shopping and banking, as well as to protect sensitive data and communications.  They play a crucial role in establishing trust and security on the internet, and are an essential component of many security protocols, such as SSL/TLS. ",1
852,"Digital Rights Management (DRM) refers to a set of technologies and techniques used to control access to and usage of digital content, such as music, movies, and ebooks. DRM is used to protect the intellectual property rights of content creators and distributors by preventing unauthorized copying, sharing, and distribution of digital content.  It typically involves encryption and digital watermarking techniques that are applied to the content to prevent it from being copied or used without permission. DRM can take different forms, such as password protection, digital certificates, and software that controls access to the content.  Some DRM technologies require users to be connected to the internet in order to access the content, and may limit the number of times that the content can be accessed or shared. While DRM can help protect the intellectual property rights of content creators and distributors, it has been criticized for limiting the rights of users to access and use digital content that they have purchased.  Critics argue that DRM can be overly restrictive and can interfere with legitimate uses of content, such as fair use or personal backups.  In some cases, DRM can also be circumvented by determined users, which raises questions about its effectiveness as a means of protecting digital content. ",1
853,"A Digital Signature is a cryptographic technique used to authenticate the integrity and origin of a digital message or document in computer security.  It provides assurance that the message or document has not been altered in transit and that it was signed by the intended signer. Digital signatures are based on a mathematical algorithm that creates a unique digital fingerprint of the message or document, called a hash.  The hash is then encrypted using the signer's private key, which is known only to the signer, to create a digital signature.  The recipient of the message or document can then verify the signature using the signer's public key, which can be obtained from a trusted third party, such as a Certificate Authority. The process of verifying a digital signature involves recreating the hash of the original message or document and comparing it to the decrypted digital signature.  If the two hashes match, then the signature is considered valid and the recipient can be confident that the message or document has not been tampered with and was signed by the intended signer. Digital signatures are used in a variety of applications, including email, software distribution, and online transactions.  They provide a secure way to authenticate the identity of the signer and ensure the integrity of the signed message or document. ",1
854,"Public Key Infrastructure (PKI) is a system of hardware, software, and policies used to manage digital certificates and public-private key pairs used in cryptographic systems. PKI uses asymmetric encryption algorithms to generate public and private keys for use in secure communication.  The public key can be freely shared with anyone, while the private key is kept secret by the key owner.  A digital certificate is used to bind a public key to the identity of the key owner, such as a website or an individual. PKI systems provide a secure method for verifying the identity of users and devices, ensuring the integrity of data in transit, and enabling secure communication between parties.  They are used in a variety of applications, including secure web browsing (via HTTPS), email encryption, and secure online transactions. PKI is based on a hierarchical trust model, where trusted third-party entities called Certificate Authorities (CAs) issue digital certificates to entities that have been verified to be who they claim to be.  The CA's signature on the certificate serves as a guarantee of the certificate's authenticity.  ",1
855,"A digital watermark is an electronic identification embedded in a digital file to track its origin and usage.  It can be used to protect copyright, embed metadata, and verify authenticity.  Techniques include altering bits, embedding information in the frequency domain, or color channels.  However, digital watermarks may infringe privacy and may not prevent unauthorized use if easily removable. ",1
856,"A Disaster Recovery Plan (DRP) is a documented and structured approach that organizations use to prepare for, respond to, and recover from disasters or disruptions that may affect their critical business operations. A DRP typically involves a series of procedures, policies, and strategies that outline how an organization will continue its operations and recover its systems, data, and infrastructure in the event of a disaster or disruption. The goal of a DRP is to minimize the impact of a disaster or disruption on an organization's operations, reputation, and bottom line by ensuring the availability and recoverability of critical systems and data. DRP procedures may include steps for backing up and restoring data, relocating personnel, activating alternative communication methods, and restoring critical applications and systems. An effective DRP should be regularly reviewed, tested, and updated to ensure that it remains relevant and effective in the face of changing threats, technologies, and business needs. ",1
857,"A DDoS attack is a cyber attack in which multiple compromised computers or devices overwhelm a targeted website, server, or network with a large volume of traffic or requests.  The aim is to make the system unavailable or unusable.  DDoS attacks can cause significant damage to an organization's reputation and bottom line.  To protect against DDoS attacks, organizations can use various techniques, but they can be difficult to defend against if carried out using sophisticated techniques or targeted vulnerabilities. ",1
858,"The Domain Name System (DNS) is a system used to translate human-readable domain names, such as www. example. com, into machine-readable IP addresses, such as 192. 0. 2. 1. DNS is a hierarchical and decentralized naming system that provides a way for computers to locate and connect to other computers and services on the Internet. When a user enters a domain name into a web browser or other network application, the application sends a request to a DNS server to resolve the domain name into an IP address.  The DNS server then checks its own cache or forwards the request to other DNS servers until it finds the IP address associated with the domain name. DNS also provides other important functions, such as domain name registration, delegation, and management, as well as security features like DNSSEC (DNS Security Extensions) that help prevent DNS attacks and ensure the authenticity of DNS information. DNS is a critical component of the Internet infrastructure and is used by billions of people and devices worldwide every day. ",1
859,"A Dynamic Link Library (DLL) is a shared library of code and data that can be used by multiple programs simultaneously in a Windows operating system. DLLs contain reusable code and data that can be loaded into memory when needed by an application, rather than being loaded into memory for every instance of the application.  This allows for more efficient use of system resources and reduces the overall memory footprint of applications. DLLs can include a wide range of functionality, such as device drivers, graphics libraries, sound libraries, and more.  They are typically used to improve application performance, reduce development time and cost, and simplify software maintenance and updates. However, DLLs can also pose security risks if they are compromised or maliciously modified, as they can be used to execute arbitrary code or access sensitive system resources.  Therefore, it is important to ensure that DLLs are properly maintained, updated, and secured. ",1
860,"e-Government (Electronic Government) refers to the use of information and communication technologies (ICTs) by government organizations to provide public services to citizens, businesses, and other government agencies. e-Government aims to improve the efficiency, transparency, and accessibility of government services and operations, as well as enhance citizen participation and engagement in the decision-making process.  Examples of e-Government services include online tax filing, vehicle registration, permit applications, and access to public records. e-Government can also facilitate the exchange of information and data between different government agencies, leading to better coordination and integration of government services. However, implementing e-Government services can also pose challenges related to privacy and security, as well as access to technology and digital literacy among citizens.  Effective e-Government initiatives require careful planning, infrastructure development, and stakeholder engagement to ensure their success. ",1
861,"eMail (short for electronic mail) is a digital communication method that allows users to send and receive messages and files over the Internet or other computer networks. eMail works by using a mail client or web-based email service to create, send, receive, and manage messages.  An email message typically includes a subject line, recipient's email address, sender's email address, message body, and optional file attachments. eMail is widely used for personal and professional communication, including business correspondence, marketing campaigns, customer support, and more.  It is a fast and convenient way to exchange information and collaborate with others, regardless of location. However, eMail also poses security risks related to phishing, spam, malware, and other cyber threats.  To mitigate these risks, users can take measures such as using strong passwords, enabling two-factor authentication, and avoiding opening suspicious emails or attachments. ",1
862,"Encryption is a process of encoding information in such a way that only authorized parties can access it.  In other words, encryption transforms plain text or data into an unreadable format that can only be deciphered with the correct decryption key or password. Encryption is widely used for protecting sensitive data, such as financial transactions, personal information, and confidential communications, from unauthorized access and interception.  Encryption can be applied to data at rest (stored on a device or server) or data in transit (being transmitted over a network). There are two main types of encryption: symmetric encryption and asymmetric encryption.  Symmetric encryption uses the same key to both encrypt and decrypt data, while asymmetric encryption uses two keys, one for encryption and one for decryption. Encryption algorithms vary in their strength and complexity, with stronger encryption algorithms providing a higher level of security but requiring more processing power and resources.  Some common encryption standards include AES, RSA, and DES. Encryption is an important tool for protecting the privacy and security of digital information, and it is used in many different contexts, such as online banking, e-commerce, email communication, and more. ",1
863,"An End User License Agreement (EULA) is a legal contract between a software owner and the end user that outlines the terms and conditions for using the software, including any restrictions and limitations of liability.  By accepting the EULA, the end user agrees to abide by the specified conditions of use. ",1
864,"Extensible Markup Language (XML) is a markup language that is used for encoding documents in a format that is both human-readable and machine-readable. XML uses tags to define elements and attributes that can be used to structure and describe data in a hierarchical manner.  This makes XML a flexible and versatile language that can be used to represent a wide variety of data types and document structures. XML is widely used in web development, database management, and other areas where structured data needs to be exchanged between different systems and platforms.  XML documents can be validated using a Document Type Definition (DTD) or an XML Schema, which ensures that the document conforms to a specific set of rules and guidelines. One of the main advantages of XML is its platform independence.  XML documents can be created and processed on any platform or operating system, which makes it an ideal choice for data exchange and integration in heterogeneous computing environments. Overall, XML is a powerful tool for structuring and describing data, and it is widely used in many different industries and applications. ",1
865,"An extranet is a private network that allows authorized external users to access a company's internal network resources over the internet. An extranet typically uses the same technology as the internet, such as web browsers and the Hypertext Transfer Protocol (HTTP), but provides secure access to specific resources that are not publicly available.  Examples of resources that can be accessed through an extranet include databases, applications, and documents. Extranets are commonly used by businesses to collaborate with partners, customers, and suppliers in a secure and controlled manner.  By providing authorized external users with access to specific resources, businesses can improve communication and streamline workflows, while maintaining control over sensitive data and intellectual property. Overall, extranets are a valuable tool for businesses that need to share information and collaborate with external partners, while maintaining the security and privacy of their internal network resources. ",1
866,"Burden of proof refers to the responsibility of a party in a legal proceeding to provide evidence to support their claim or defense.  In a criminal case, the prosecution must prove guilt beyond a reasonable doubt, while in a civil case, the burden of proof is on the plaintiff to prove their case by a preponderance of the evidence.  The concept ensures that legal decisions are based on evidence rather than speculation and helps prevent unjust decisions. ",1
867,"In machine learning, a false negative is when a model incorrectly predicts a negative outcome for a positive example.  It can have serious consequences in cases such as medical diagnosis or fraud detection, so it's important to optimize the model's performance to reduce false negatives. ",1
868,"A firewall is a network security system that monitors and controls incoming and outgoing network traffic based on a set of security rules.  It acts as a barrier between a private internal network and the public internet or other untrusted networks, allowing only authorized traffic to pass through while blocking unauthorized or potentially harmful traffic. Firewalls can be implemented as software or hardware devices, and they typically use one or more of the following techniques to filter traffic:Packet filtering: Examines the source and destination addresses and other packet header information to determine whether to allow or block the packet. Stateful inspection: Tracks the state of network connections and allows only legitimate traffic to pass through based on that state. Application-level gateway: Analyzes the content of application-layer protocols such as HTTP or FTP to filter traffic based on application-specific rules. Firewalls are an important component of network security and can help prevent unauthorized access, data breaches, and other types of cyber attacks. ",1
869,"Firmware is a type of software embedded in a hardware device that provides low-level control for the device to operate and interact with other components.  It's programmed during manufacturing, cannot be easily modified by end-users, and can be updated by the manufacturer to fix bugs or add new features.  It's used in devices such as printers, cameras, and home appliances, as well as in specialized computing devices called embedded systems. ",1
870,"Challenge-Handshake Authentication Protocol (CHAP) is a network authentication protocol used to verify the identity of a user or device attempting to access a network.  CHAP operates by first establishing a connection between the client and server, after which the server sends a challenge message to the client. The challenge is a random number or string that is combined with a secret key, known only to the client and server.  The client then generates a response message by applying a one-way hashing function to the challenge and secret key, and sends it back to the server. The server uses the same hashing function and secret key to independently generate its own expected response, and compares it to the response received from the client.  If the two responses match, the client is authenticated and granted access to the network. CHAP provides a more secure form of authentication than simpler protocols such as Password Authentication Protocol (PAP), which sends passwords in clear text.  CHAP also supports periodic re-authentication during a session to prevent unauthorized access. Overall, CHAP is a widely used network authentication protocol that provides a higher level of security and authentication than simpler protocols. ",1
871,"Hexadecimal is a numbering system that uses a base of 16, commonly used in computer programming and digital electronics.  It uses digits 0-9 and letters A-F to represent values from 0 to 15.  Hexadecimal provides a more concise and human-readable way to represent binary numbers, and is used to represent memory addresses, color values, and other numerical data. ",1
872,"A honeynet is a network of computers or devices designed to attract and monitor cyber attackers.  It simulates a real network and can be used for research, intelligence gathering, incident response, training, and testing security measures.  Honeynets are a valuable tool for improving cybersecurity and protecting against potential attacks. ",1
873,"A honeypot is a decoy computer system or device that appears to be a real and valuable target to attract and detect cyber attackers.  It is monitored by security professionals to gather information about attackers and their methods, and can be used for research, intelligence gathering, incident response, and diversion of attackers.  Honeypots are a valuable tool for improving cybersecurity measures. ",1
874,"In computer networks, a host refers to any device or computer that is connected to a network and can communicate with other devices on that network using a unique identifier, such as an IP address.  Hosts are essential for enabling communication and data transfer between devices and users on the network. ",1
875,"HTML is a standard markup language used for creating web pages and applications.  It defines the structure and layout of web pages using various tags and attributes, and allows developers to create text, images, links, forms, and multimedia content.  HTML is a platform-independent language and can be combined with other technologies like CSS and JavaScript to add further functionality and interactivity. ",1
876,"HTTP is a protocol for transmitting data over the internet between web servers and clients, such as web browsers.  It defines how data is formatted and transmitted, and enables web servers and clients to communicate and exchange data efficiently. ",1
877,"HTTPS is a secure version of the HTTP protocol used for transmitting data securely over the internet.  It encrypts data using a cryptographic protocol, such as TLS, to protect sensitive information from interception and unauthorized access.  HTTPS is commonly used for transmitting sensitive information, such as financial transactions and user authentication, and is becoming the standard for all web traffic. ",1
878,"Integrated Services Digital Network (ISDN) is a set of communication standards used for transmitting digital signals over telephone lines.  It enables the transmission of voice, video, and data over the same digital network, and provides faster and more reliable connections than traditional analog phone lines. ISDN supports two types of channels: B channels and D channels.  B channels carry voice, data, and video signals, while D channels carry signaling information and control messages.  ISDN also supports various data transfer rates, ranging from 64 kbps to 2 Mbps, and can be used for various applications, such as video conferencing, faxing, and internet access. However, ISDN has largely been replaced by newer technologies, such as digital subscriber line (DSL) and fiber optic internet, which provide faster and more efficient data transmission over telephone lines. ",1
879,"Internet Protocol version 6 (IPv6) is the latest version of the Internet Protocol (IP), which is the communications protocol used to send and receive data over the internet.  IPv6 was developed to address the limitations of the previous version, IPv4, which was designed in the 1980s and has since run out of available IP addresses. IPv6 uses a 128-bit address format, compared to the 32-bit format used by IPv4, allowing for a significantly larger number of available IP addresses.  This is important as the growth of the internet and the number of devices connected to it has increased dramatically in recent years.  IPv6 also includes additional features, such as improved security and network auto-configuration. Although IPv6 has been available for many years, the adoption has been relatively slow.  This is due to several factors, including the complexity of transitioning from IPv4 to IPv6 and the fact that IPv4 is still widely used and supported.  However, as the demand for IP addresses continues to increase, the transition to IPv6 is becoming more necessary, and many organizations and internet service providers are gradually making the switch. ",1
880,"An ISP is a company that provides internet access and related services to individuals and organizations.  They connect customers to the internet and manage the infrastructure and equipment necessary for reliable internet access.  ISPs charge customers a fee for their services, and there are many different providers operating worldwide. ",1
881,"Java applets are small programs written in the Java programming language that can be embedded into web pages and run by web browsers.  They can perform a variety of tasks, but have declined in popularity due to security concerns and the rise of other web technologies. ",1
882,"The Java sandbox is a security feature that restricts the actions of untrusted or potentially malicious code running within the Java Virtual Machine (JVM).  It limits the code's access to system resources and prevents it from consuming excessive resources, providing a secure environment for executing untrusted code. ",1
883,"JavaScript is a programming language used for creating interactive and dynamic web pages, as well as for other applications such as mobile app development and game development.  It runs on the client-side of the web and is executed by web browsers, allowing for real-time updates and interactivity on web pages. ",1
884,"Keystroke monitoring is the process of tracking and recording the keys that are pressed on a computer keyboard, usually for monitoring employee activity or detecting unauthorized access to computer systems.  It can be performed using hardware or software-based methods, but is controversial due to privacy concerns and potential for unethical use. ",1
885,"A logic bomb is a type of malicious code or software that is intentionally inserted into a computer system to perform a specific action or cause damage at a predetermined time or under specific conditions.  Logic bombs can be programmed to execute when certain conditions are met, such as when a specific date or time is reached, when a certain program is executed, or when a specific user performs a certain action. The purpose of a logic bomb is often to cause damage or disrupt normal system operations, such as by deleting files, corrupting data, or shutting down critical services.  Logic bombs can be difficult to detect and prevent, as they can be designed to lie dormant for long periods of time before executing. Logic bombs are illegal and considered a serious threat to computer security.  They are often used by hackers, disgruntled employees, or other malicious actors to cause damage or gain unauthorized access to computer systems.  Preventative measures against logic bombs include regularly updating system software and monitoring system activity for suspicious behavior. ",1
886,"The Master Boot Record (MBR) is a piece of code at the beginning of a hard disk that initiates the boot process and contains information about the disk's partition table and boot loader.  It can be vulnerable to malware attacks, and preventative measures such as antivirus software and backups are recommended. ",1
887,"Moore's Law states that the number of transistors on a microchip will double every two years while the cost of producing these chips will decrease, leading to a rapid increase in computing power and decrease in cost of electronics.  It has been a driving force behind technology advancement and the development of new technologies, but its future is uncertain due to physical and economic limitations. ",1
888,"Open source in computer science refers to software that is made freely available to the public with its source code included, allowing anyone to view, modify, and distribute it.  It is often created and maintained by a community of developers and has benefits such as transparency, flexibility, and accessibility.  It is widely used in various applications. ",1
889,"An operating system (OS) is a software program that manages computer hardware and software resources, providing an interface for users to interact with the computer and run applications.  It manages system resources, provides security features, and manages system updates and software installations.  Examples include Windows, macOS, Linux, and Android. ",1
890,"Parallel processing is a type of computing in which multiple processors or cores work together to execute a task simultaneously, allowing for faster processing and greater efficiency.  It's commonly used in scientific simulations, data analytics, and multimedia processing.  There are two main types: shared memory and distributed memory.  Parallel processing is becoming increasingly important in the age of big data and complex computational tasks. ",1
891,"A computer password is a sequence of characters used to authenticate a user and grant access to a computer or system.  They help prevent unauthorized access and are required for logging in to a computer, user account, network, or application.  Strong passwords should be unique, complex, and regularly changed.  Passwords are a fundamental component of computer security. ",1
892,"Ubiquitous computing, also known as pervasive computing, is a concept in computer science in which computing and communication technologies are seamlessly integrated into everyday objects and environments, allowing for constant and pervasive access to information and communication. The goal of ubiquitous computing is to create a world where technology is so seamlessly integrated into our surroundings that it becomes invisible and ubiquitous.  This can be achieved through the use of sensors, wireless communication, and the internet of things (IoT), which allows objects to communicate with each other and with the internet. Examples of ubiquitous computing include smart homes, wearables, and smart cities, where objects and systems are interconnected and can communicate with each other to provide personalized and context-aware services. Overall, ubiquitous computing aims to create a world where technology is seamlessly integrated into our everyday lives, enhancing our experiences and making our environments more efficient, convenient, and personalized. ",1
893,"PHP is a server-side scripting language used primarily for web development.  It's designed to create dynamic web pages and applications and can be embedded into HTML code. PHP code is executed on the server side, which means that the output of the code is sent to the web browser in the form of HTML, CSS, and JavaScript.  PHP can be used to perform various tasks such as handling forms, processing user input, interacting with databases, and generating dynamic content. PHP is open-source software, meaning that it's free to use and modify, and is supported by a large community of developers who contribute to its development and provide support and resources for users.  It's widely used by web developers for building websites and web applications, and is compatible with most web servers and operating systems. Overall, PHP is a popular and powerful scripting language for web development, with a wide range of applications and a large community of users and developers. ",1
894,Ping is a computer network utility used to test the connectivity and response time of network devices.  It sends a packet of data to a specific IP address or hostname and measures the round-trip time it takes for the data to travel between two devices.  It's commonly used for network troubleshooting and monitoring. ,1
895,"Port Address Translation (PAT), also known as Network Address Port Translation (NAPT), is a technique used in computer networking to allow multiple devices on a private network to share a single public IP address. PAT works by assigning a unique port number to each device on the private network, which allows the router to keep track of which device is associated with each incoming or outgoing connection.  When a device on the private network initiates a connection to the internet, the router replaces the private IP address of the device with the public IP address of the router, and uses the unique port number to identify the device. PAT is commonly used in small office or home networks, where a limited number of public IP addresses are available, and multiple devices need to access the internet simultaneously.  It's also used by internet service providers (ISPs) to conserve public IP addresses and manage network traffic. Overall, PAT is a useful technique for managing network resources and allowing multiple devices to share a single public IP address. ",1
896,"Port scanning is a technique used to discover open ports and services on a computer or network device.  A port is a communication endpoint in a network device, and each service or application running on the device typically uses a specific port to communicate with other devices on the network. Port scanning works by sending a series of network packets to a target device, each packet attempting to establish a connection on a different port.  If a connection is successful, the scanner can determine the port status (open, closed, or filtered) and potentially identify the service or application running on the port. Port scanning can be used for legitimate purposes, such as network management and security testing, but it can also be used for malicious activities, such as identifying vulnerable services or attempting to gain unauthorized access to a network. Network administrators and security professionals often use port scanning tools to identify open ports and services on their own networks, and to detect and prevent port scanning attempts by unauthorized users. ",1
897,"Pretty Good Privacy (PGP) is an encryption software used to secure emails, files, and other forms of digital communication.  It uses a combination of symmetric-key and public-key cryptography to encrypt and decrypt messages, and provides digital signature functionality for verifying message authenticity and integrity.  It was developed by Phil Zimmermann in 1991 as a free, open-source solution for secure communication over the internet. ",1
898,"In computer science, a protocol refers to a set of rules and procedures that dictate how data is transmitted over a network.  Protocols ensure that devices and systems can communicate with each other effectively and efficiently, even if they are made by different manufacturers or use different operating systems. A protocol defines the format and structure of data packets, as well as the order and timing of their transmission.  It also specifies how errors and other issues should be handled, and what actions should be taken in the event of a problem. There are many different protocols used in computer networks, ranging from low-level protocols such as Ethernet and TCP/IP to application-level protocols such as HTTP and FTP.  Each protocol has its own specific purpose and functionality, and they work together to create a seamless and reliable communication network. ",1
899,"Radio Frequency Identification (RFID) is a technology that uses radio waves to wirelessly identify and track objects or people.  RFID systems consist of three main components: a tag or transponder, a reader or interrogator, and a backend system for processing the data. The RFID tag contains a small microchip and an antenna, which can be attached to or embedded within an object.  When the tag is exposed to radio waves from a nearby reader, it powers up and sends back a signal containing its unique identifier. RFID technology has a wide range of applications, from tracking inventory and assets in supply chain management to monitoring patients in hospitals and controlling access to secure areas.  It provides a more efficient and automated way to collect data and streamline processes, compared to manual methods such as barcode scanning. ",1
900,"Remote Access Tools (RATs) are software tools that allow users to remotely access and control another computer or device over a network connection.  RATs can be used for a variety of legitimate purposes, such as IT support, remote working, and online collaboration. However, RATs can also be used maliciously, such as in the case of cyber attacks or unauthorized surveillance.  Hackers can use RATs to gain unauthorized access to a victim's computer or network, allowing them to steal sensitive data, install malware, or carry out other nefarious activities. RATs can be disguised as legitimate software, making them difficult to detect and remove.  To protect against RATs, it is important to use reputable antivirus and firewall software, keep software up to date with the latest security patches, and be cautious when downloading or installing software from untrusted sources. ",1
901,"The Rijndael algorithm is a symmetric key block cipher algorithm used for encryption and decryption of digital data.  It was selected as the Advanced Encryption Standard (AES) in 2001, replacing the Data Encryption Standard (DES). The Rijndael algorithm operates on fixed block sizes of 128 bits and supports key sizes of 128, 192, or 256 bits.  It uses a series of mathematical operations, including substitution, permutation, and XOR operations, to transform plaintext into ciphertext. The Rijndael algorithm is considered secure and widely used in many applications, including secure communication protocols, digital rights management, and data storage encryption.  It is also used in many popular software applications, including Adobe Acrobat and Microsoft Office. ",1
902,"Role-Based Access Control (RBAC) is a method of access control used to restrict access to computer resources based on a user's role or position within an organization.  With RBAC, access is granted based on predefined roles that are associated with certain privileges and permissions. RBAC allows administrators to manage access control at a higher level of abstraction, simplifying the process of granting and revoking access.  In an RBAC system, users are assigned roles based on their job function, and these roles are associated with specific permissions and access levels. For example, a hospital might use RBAC to control access to patient records.  Doctors and nurses would be assigned roles that allow them to access patient records relevant to their job function, while administrative staff would have a different set of roles and access privileges. RBAC provides several benefits, including increased security, simplified administration, and better compliance with regulatory requirements.  It is commonly used in enterprise-level systems and is supported by many modern operating systems, databases, and network devices. ",1
903,"A network router is a networking device that forwards data packets between computer networks.  It operates at the network layer of the OSI model and uses routing tables to determine the best path for data packets to reach their destination. Routers are commonly used in homes, businesses, and service provider networks to connect multiple networks and facilitate communication between devices.  They can be wired or wireless and may have a variety of features, including built-in firewalls, virtual private network (VPN) support, and Quality of Service (QoS) settings to prioritize traffic. In a typical home or small business network, the router is often the central point of connectivity, connecting devices to the Internet or other networks.  It can be configured to assign IP addresses to devices on the network, manage network traffic, and provide basic security features. Large service provider networks may use more complex routers with advanced routing protocols, such as Border Gateway Protocol (BGP), to manage traffic and connect to other networks. ",1
904,"A script kiddie is an inexperienced hacker who uses pre-made software tools to carry out attacks on computer systems or networks.  They lack the skills and knowledge required to develop their own hacking tools or write their own code.  They are often motivated by a desire for peer recognition and may engage in activities such as defacing websites or launching denial-of-service attacks.  Although considered a nuisance, their actions can still cause damage or disruption. ",1
905,"A sector in a hard disk is a small section of the disk's magnetic storage surface that is used to store a fixed amount of data.  It is typically the smallest unit of data that can be read from or written to the disk.  Sectors are organized into tracks, which are concentric circles on the disk's surface.  The number of sectors per track and the number of tracks per disk surface determine the storage capacity of the hard disk.  Modern hard disks have sector sizes of 512 bytes or 4 kilobytes, although larger sector sizes are becoming more common. ",1
906,"A secure hash is a cryptographic function that takes an input (such as a message or data file) and produces a fixed-size output, called a hash or message digest.  The hash is designed to be a unique and irreversible representation of the input data, meaning that even a small change in the input will result in a vastly different hash value.  Secure hash functions are commonly used for message integrity checking, digital signatures, and password storage.  Examples of popular secure hash functions include SHA-256 and MD5. ",1
907,"Secure Socket Layer (SSL) is a security protocol that is used to establish an encrypted link between a client (such as a web browser) and a server (such as a web server).  This encrypted link ensures that any data that is transmitted between the client and server is protected from eavesdropping and tampering.  SSL uses a combination of symmetric and asymmetric encryption algorithms to encrypt the data, and it also provides mechanisms for authenticating the server and, optionally, the client.  SSL has been succeeded by Transport Layer Security (TLS), but the term ""SSL"" is still commonly used to refer to this type of security protocol. ",1
908,"Signature-based intrusion detection systems (IDS) are security mechanisms that examine network traffic or system logs for specific patterns or signatures that match known security threats, such as viruses, worms, or other malware.  These patterns or signatures are typically derived from previous attacks and are stored in a database within the IDS.  When the IDS detects a signature match, it generates an alert or takes other predefined actions to mitigate the threat.  Signature-based IDS can be effective at detecting known attacks, but they are limited to identifying only those attacks for which signatures exist.  New or unknown attacks may not be detected by these systems.  As such, signature-based IDS are often used in conjunction with other security measures, such as behavior-based IDS and firewalls, to provide a more comprehensive defense against cyber threats. ",1
909,Smurfing is a type of cyber attack that floods a network with ICMP echo requests using a spoofed IP address to amplify the traffic and overwhelm the target.  It's often used in DDoS attacks and can be mitigated by filtering incoming traffic and blocking broadcast traffic. ,1
910,"A sniffer software, also known as a packet analyzer or network analyzer, is a tool that captures and analyzes data packets transmitted over a network.  It allows users to inspect network traffic, identify security threats, troubleshoot network issues, and monitor network performance.  Sniffer software can be used for both legitimate network monitoring purposes and malicious activities such as stealing sensitive data or passwords. ",1
911,"Social engineering is a technique used to manipulate people into divulging sensitive information, granting access to secure areas, or performing actions that they wouldn't normally do.  It involves the use of psychological tactics such as deception, persuasion, and intimidation to exploit human weaknesses and gain unauthorized access to information or systems.  Social engineering attacks can take many forms, including phishing scams, baiting, pretexting, and quid pro quo.  They can be conducted over the phone, email, social media, or in person.  Social engineering is a common tactic used by hackers and cyber criminals to gain access to sensitive information or systems. ",1
912,"P2P spoofing is a type of network attack that involves disguising malicious traffic as legitimate peer-to-peer (P2P) traffic.  In a P2P network, users connect directly to each other and share files or data without the need for a central server.  P2P spoofing can occur when an attacker uses a software program to mimic a legitimate P2P user, but with malicious intent.  The attacker can then use this fake identity to distribute malware or engage in other malicious activities, such as stealing sensitive information or launching a distributed denial-of-service (DDoS) attack.  P2P spoofing can be difficult to detect because it appears to be legitimate traffic.  However, there are methods and tools available to detect and prevent P2P spoofing attacks. ",1
913,"Steganography is the practice of concealing a message or data within another file, image, or message to avoid detection.  The hidden message is usually encrypted before being concealed, and can only be accessed by someone who knows how to extract it.  This technique is often used to covertly transmit sensitive or secret information without being detected.  Steganography is different from cryptography, which focuses on making a message unreadable by scrambling its content. ",1
914,"A stream cipher is a type of encryption algorithm that encrypts data one bit or byte at a time.  It operates on data streams and converts plaintext into ciphertext using a key, which is a sequence of random or pseudorandom bits or bytes.  Unlike block ciphers, which encrypt data in fixed-sized blocks, stream ciphers encrypt data in a continuous stream.  This makes them well-suited for applications where data is transmitted in real-time, such as online streaming or communication systems.  Stream ciphers can be vulnerable to attacks if the key is not properly managed or if the underlying random number generator is weak. ",1
915,"E-government, or electronic government, refers to the use of digital technologies, such as the internet, mobile devices, and other digital communication channels, to improve the delivery of government services to citizens and businesses.  It involves the use of technology to provide public services, including online forms, databases, and other digital resources. E-government can enhance government transparency, efficiency, and responsiveness, as well as increase citizen participation in the policymaking process.  It also offers citizens and businesses more convenient and accessible ways to interact with government services, such as paying taxes, accessing public records, and submitting applications for permits and licenses. Examples of e-government services include online voting, electronic tax filing, online health records, digital identity management, and online public service delivery. ",1
916,"Eiffel refers to the Eiffel programming language, which is a high-level, object-oriented programming language designed by Bertrand Meyer in the mid-1980s.  The language is named after Gustave Eiffel, the French engineer who designed the Eiffel Tower. Eiffel is known for its focus on software engineering principles such as design-by-contract, automatic memory management, and the use of the Uniform Access Principle.  The language's syntax is designed to be simple, readable, and expressive, with features that encourage modular programming, extensibility, and maintainability. Eiffel is commonly used in developing mission-critical software systems, such as aerospace, finance, and medical applications, where reliability, safety, and security are of paramount importance.  The language is also used in education and research, particularly in the study of software engineering and formal methods. ",1
917,"Eiffel has been implemented and used in a variety of settings, including commercial applications, academic research, and open source software development.  Here are some examples of Eiffel implementations and uses:Commercial applications: Eiffel has been used to develop commercial software applications in fields such as finance, healthcare, and aerospace.  For example, the Eiffel-based language technology company Eiffel Software has developed software for financial institutions, medical device manufacturers, and other industries. Academic research: Eiffel has been used in academic research to explore formal methods, software engineering principles, and programming language design.  For example, researchers at the University of Illinois at Urbana-Champaign used Eiffel to develop a runtime verification tool for software testing. Open source software: Eiffel has been used in a number of open source software projects, including the EiffelStudio development environment and the SmartEiffel compiler.  These projects have made Eiffel accessible to a wider audience and helped to promote the language's adoption. Safety-critical systems: Eiffel has been used to develop software for safety-critical systems, such as aircraft control systems and medical devices.  The language's support for formal methods and design-by-contract principles make it well-suited for developing high-assurance software systems. Overall, Eiffel's strengths in software engineering, formal methods, and high-assurance systems make it a valuable tool for a range of applications and settings. ",1
918,"Electronic voting systems can be classified into three main categories:Direct Recording Electronic (DRE) Systems: In this type of system, voters directly cast their votes using an electronic interface, such as a touchscreen or button.  The votes are stored on a memory device, typically a digital card or hard drive, and can be tallied automatically. Optical Scan Systems: In this type of system, voters mark their choices on a paper ballot, which is then scanned and tallied electronically.  The system reads the marks on the ballot, such as circles or checkboxes, using optical character recognition (OCR) or image scanning technology. Remote Electronic Voting Systems: This type of system allows voters to cast their ballots from a remote location using a computer or mobile device.  This type of system may use encryption and other security measures to ensure that the votes are secure and confidential. ",1
919,"Early voting is a type of voting system that allows eligible voters to cast their ballots before the scheduled election day.  Early voting typically takes place in the days or weeks leading up to the election day, and is usually conducted at designated polling places or through mail-in ballots. Early voting systems vary depending on the location and jurisdiction.  In some cases, voters may be required to provide a valid reason for why they cannot vote on election day, such as work or travel obligations.  In other cases, early voting is available to all registered voters regardless of the reason. Early voting can offer several advantages, such as increased convenience and accessibility for voters who may have difficulty making it to the polls on election day due to work or personal commitments.  It can also help to reduce long lines and wait times at polling places on election day, which can lead to greater voter turnout and participation. Early voting systems can also pose some challenges, such as the need for additional polling places and staff to manage the early voting process, which can increase costs for local jurisdictions.  There may also be concerns about the security and accuracy of early voting systems, particularly with mail-in ballots, which may be subject to tampering or fraud. ",1
920,"A touch screen is a type of display that allows users to interact with a computer or electronic device by touching the screen directly with their fingers or a stylus.  Touch screens can be found on a wide range of devices, including smartphones, tablets, computers, and kiosks. There are several types of touch screens, each with its own technology and method for detecting touch inputs:Resistive Touch Screens: These screens consist of two layers of flexible material, with a conductive coating on each layer.  When the screen is pressed, the layers come into contact and the touch point is detected by measuring changes in electrical resistance. Capacitive Touch Screens: These screens use a conductive layer that stores electrical charges.  When a finger or stylus touches the screen, it changes the electrical field and the touch point is detected by measuring these changes. Infrared Touch Screens: These screens use infrared sensors to detect touch inputs.  When the screen is touched, the infrared light is interrupted and the touch point is detected based on the location of the interruption. Surface Acoustic Wave Touch Screens: These screens use ultrasonic waves to detect touch inputs.  When the screen is touched, the waves are disrupted and the touch point is detected based on the location of the disruption. ",1
921,"Optical scan is a technology used in voting systems and other applications to read and tally data from paper forms or ballots.  Optical scan systems use a specialized scanner that can read marks made on paper and convert them into digital data that can be analyzed and processed electronically. In voting systems, optical scan is often used for paper ballots.  Voters mark their choices on a paper ballot by filling in bubbles, completing arrows, or making other designated marks.  The ballots are then collected and fed through an optical scanner, which reads the marks and tallies the votes electronically.  The results can be tabulated quickly and accurately, with less potential for errors compared to manual counting. Optical scan technology can also be used in other applications, such as scanning and processing surveys, forms, and other types of paper-based data collection.  The technology can save time and reduce errors compared to manual data entry, as well as provide greater flexibility in terms of data analysis and reporting. ",1
922,"Email, short for electronic mail, is a method of exchanging digital messages over the internet or other computer networks.  It is one of the most widely used forms of communication in the digital age. An email message typically consists of a text message, along with any attachments or multimedia content that the sender wishes to include.  The message is composed using an email client or webmail service, and is sent to one or more recipients by specifying their email address. Email messages are typically delivered within seconds or minutes, depending on network traffic and other factors.  Recipients can read, reply to, and forward email messages as needed, and can also store them for future reference. ",1
923,"n embedded system is a computer system that is integrated into a larger device or system and is designed to perform specific functions within that system.  Unlike general-purpose computers, which are designed to be versatile and flexible, embedded systems are typically designed for a specific task or set of tasks, and may be optimized for performance, power consumption, and other factors. Embedded systems can be found in a wide range of devices and systems, such as cars, home appliances, medical devices, industrial equipment, and more.  They may be programmed using a variety of programming languages and tools, depending on the requirements of the system. ",1
924,"Emulation refers to the process of simulating the behavior of one computer system on another computer system.  The purpose of emulation is to allow software or hardware designed for one system to run on a different system, typically with different hardware or software configurations.  To emulate a system, software known as an emulator must be installed on the host system.  The emulator simulates the behavior of the original hardware or software platform, allowing the system to run software designed for the original platform. Emulation can be resource-intensive, as the host system must dedicate significant processing power and memory resources to simulating the behavior of the emulated system.  As a result, emulation may not always be feasible for more complex systems or applications.  However, for simpler systems and applications, emulation can be a valuable tool for preserving legacy software and hardware, or for testing and development purposes. ",1
925,"Encapsulation is a fundamental concept in object-oriented programming (OOP) that refers to the practice of hiding the internal details of an object and exposing only the essential features or interfaces that other objects need to interact with it. In encapsulation, the internal state of an object is kept private, and can only be accessed or modified through the object's public methods or interfaces.  This protects the integrity of the object's data and ensures that it is accessed and modified in a controlled, safe manner. ",1
926,"Encryption is the process of converting information or data into a secret code or cipher, so that only authorized individuals or systems can access and interpret the information.  Encryption is used to protect sensitive information, such as personal data, financial information, or confidential communications, from unauthorized access or interception. Encryption works by using an algorithm or mathematical formula to transform the original data into a code or cipher, called ciphertext.  The encryption process typically involves a secret key or password that is used to scramble the data, and only authorized individuals or systems with the corresponding key or password can decrypt the ciphertext and access the original data. ",1
927,"Enterprise computing refers to the use of information technology (IT) to support and optimize the operations of large organizations, such as businesses, government agencies, or non-profit organizations.  Enterprise computing encompasses a wide range of technologies, applications, and services that are used to manage and automate various aspects of an organization's operations, including:Enterprise resource planning (ERP) systems: ERP systems integrate and automate various business processes, such as finance, accounting, human resources, and inventory management, into a single software platform. Customer relationship management (CRM) systems: CRM systems are used to manage and analyze customer interactions and data throughout the customer lifecycle, with the goal of improving customer retention and loyalty. Business intelligence (BI) and analytics tools: BI and analytics tools are used to analyze and interpret large amounts of data, such as sales data or customer behavior data, to identify trends, patterns, and insights that can be used to inform business decisions. Supply chain management (SCM) systems: SCM systems are used to manage and optimize the flow of goods and services from suppliers to customers, with the goal of reducing costs, improving efficiency, and enhancing customer satisfaction. Collaboration and communication tools: Collaboration and communication tools, such as email, instant messaging, and video conferencing, are used to facilitate communication and collaboration among employees, partners, and customers. ",1
928,"Entrepreneurs in computing are individuals who create and develop innovative ideas and businesses related to computing technology.  They use their expertise in computing to identify new opportunities, solve problems, and create value for their customers and society as a whole. Entrepreneurs in computing are responsible for creating and developing new products, services, and technologies that drive innovation and growth in the computing industry.  They often work in areas such as software development, web design, mobile app development, data analytics, cybersecurity, and cloud computing. Entrepreneurs in computing are known for their ability to identify market needs and develop solutions that meet those needs in creative and effective ways.  They often start their businesses with little or no funding, relying on their own skills and resources to build their companies from the ground up. ",1
929,"The ergonomics of computing is the study of how to design and arrange computer workstations and other computer-related equipment to optimize human comfort, safety, and efficiency while using computers.  It involves designing computer equipment and workspaces in a way that reduces the risk of injury, strain, or discomfort caused by repetitive movements, poor posture, or other factors. Ergonomics is an important consideration in computing because prolonged use of computers can lead to a range of physical and mental health issues, such as eye strain, back and neck pain, carpal tunnel syndrome, and stress.  By designing computer workstations and equipment with ergonomics in mind, the risk of developing these problems can be significantly reduced. ",1
930,"Error correction is the process of detecting and correcting errors that occur during the transmission or storage of data.  In computing, errors can occur due to a variety of factors such as data corruption, interference, or noise in the communication channel, or hardware malfunctions. Error correction codes are used to detect and correct errors in data.  These codes are added to the data before transmission or storage, and they are designed to allow the receiver to detect and correct any errors that may occur during the process. One common error correction code is the Hamming code, which is a binary code that adds extra bits to the data being transmitted.  These extra bits are calculated based on the original data and are used to detect and correct errors in the transmitted data.  Another commonly used error correction code is the Reed-Solomon code, which is used to correct errors in block codes, such as those used in CDs and DVDs. Error correction is important in computing because it ensures the accuracy and reliability of data transmission and storage.  Without error correction, data could be lost or corrupted during transmission, resulting in errors or incorrect information being received.  Error correction helps to ensure that data is transmitted and stored accurately, which is critical for many applications, such as data transfer, online transactions, and digital communication. ",1
931,"Error handling refers to the process of detecting, reporting, and resolving errors or exceptions that occur during the execution of a computer program or system.  Errors can occur due to a variety of reasons, such as incorrect input, hardware malfunctions, or programming errors. When an error occurs, the program or system must be able to identify the error and take appropriate action to resolve it.  This may involve notifying the user of the error, logging the error for later analysis, or attempting to recover from the error by executing an alternative path or retrying the operation. There are several techniques for error handling in computing, including:Exception handling: This involves using try-catch statements or other constructs to identify and handle exceptions in a program. Error codes: This involves using predefined error codes to identify specific types of errors and taking appropriate action based on the error code. Error messages: This involves providing descriptive error messages to users to help them understand what went wrong and how to fix it. Logging: This involves recording error information in logs for later analysis and troubleshooting. ",1
932,"An expert system is a computer program that uses artificial intelligence (AI) techniques, such as knowledge representation, reasoning, and decision-making algorithms, to solve problems or make decisions in a specific domain.  The goal of an expert system is to simulate the decision-making ability of a human expert in a particular field by capturing and representing their knowledge and expertise. ",1
933,"Expert systems typically consist of two main components: a knowledge base and an inference engine.  The knowledge base contains the rules, facts, and other information relevant to the problem domain.  The inference engine uses this knowledge to reason about the problem, make decisions, and provide recommendations or solutions. ",1
934,"Data is a collection of facts, figures, or other pieces of information that can be processed or analyzed to gain insights or knowledge.  Data can take many forms, such as text, numbers, images, audio, or video, and can be stored in various formats, such as databases, spreadsheets, or files. Data can be either raw or processed.  Raw data is unorganized and unstructured, while processed data has been organized, structured, or analyzed in some way to make it more useful or meaningful.  Data can also be classified as qualitative or quantitative.  Qualitative data is descriptive and subjective, while quantitative data is numerical and objective. ",1
935,"Data abstraction is a programming concept that allows complex data to be represented in a simplified way, by hiding the implementation details of data and only showing the essential features to the user.  It is a technique of breaking down a complex system into simpler, more manageable components, which makes it easier to design, develop, and maintain software systems. In data abstraction, the user interacts with a simplified interface that exposes only the relevant properties or behaviors of the data, while hiding the underlying implementation details.  This allows the user to focus on the essential aspects of the data without being burdened by unnecessary complexity. ",1
936,"There are several advantages of using data abstraction in software development, including:Simplifies complexity: Data abstraction simplifies the complexity of a software system by breaking it down into smaller, more manageable components.  By abstracting away the details of how data is stored and manipulated, developers can focus on the essential properties and behaviors of the data. Enhances modularity: By encapsulating data within objects or abstract data types, data abstraction promotes modularity and information hiding.  This makes it easier to develop and maintain large software systems, as changes made to one part of the system are less likely to affect other parts of the system. Improves code reusability: Data abstraction makes it easier to reuse code in different parts of a software system, as the same data structures and functions can be used in different contexts without requiring significant modification. Enhances security: By hiding the implementation details of data, data abstraction can help to enhance security by preventing unauthorized access to sensitive data. Facilitates testing and debugging: Data abstraction can make it easier to test and debug software systems, as it provides a clear separation between the interface and implementation of data structures and functions.  This can make it easier to identify and isolate errors or bugs within the system. ",1
937,"Data acquisition (DAQ) is the process of acquiring or collecting data from various sources, such as sensors, instruments, and other devices, in order to analyze, monitor, or control a system or process.  The data acquisition process typically involves the conversion of analog signals from sensors or devices into digital signals that can be processed by a computer or other digital device. In practical terms, data acquisition involves connecting sensors or instruments to a computer or other digital device using a data acquisition system, which may include hardware components such as amplifiers, filters, and analog-to-digital converters (ADCs), as well as software applications or programming languages for controlling and processing the data. ",1
938,"A data acquisition system typically consists of the following components:Sensors/Transducers: These are devices that convert physical parameters, such as temperature, pressure, or humidity, into electrical signals. Signal Conditioning: This component prepares the electrical signals from the sensors for digitization.  It may include amplification, filtering, isolation, or other signal processing techniques. Analog-to-Digital Converter (ADC): The ADC converts the analog electrical signals from the sensors into digital signals that can be processed by a computer or other digital device. Data Storage: This component stores the digital signals for later analysis.  It may include hard drives, solid-state drives, memory cards, or other storage devices. Data Transmission: This component transmits the digital signals from the data acquisition system to a computer or other digital device for further analysis. Computer Interface: This component connects the data acquisition system to a computer or other digital device.  It may include a USB, Ethernet, or other type of interface. Software: The software controls the data acquisition system and manages the data acquisition process.  It may include drivers for the data acquisition hardware, data logging software, or other applications. ",1
939,"Data acquisition has numerous applications across various industries and fields, some of which are:Scientific Research: Data acquisition systems are used in scientific research to gather and analyze data from experiments and observations.  This can include monitoring biological signals, environmental conditions, and physical phenomena. Industrial Automation: Data acquisition systems are used in industrial automation to monitor and control production processes, including measuring temperature, pressure, flow rates, and other parameters. Aerospace and Defense: Data acquisition systems are used in aerospace and defense applications to gather data on aircraft performance, missile guidance, and other critical systems. Medical Monitoring: Data acquisition systems are used in medical applications to monitor patient vital signs, such as heart rate, blood pressure, and respiratory rate. Automotive Testing: Data acquisition systems are used in automotive testing to gather data on vehicle performance, including acceleration, braking, and fuel efficiency. Structural Health Monitoring: Data acquisition systems are used in structural health monitoring to assess the health and safety of buildings, bridges, and other structures by monitoring vibration, strain, and other parameters. ",1
940,"Data administration is a process that involves the management and governance of an organization's data assets.  It encompasses a set of activities that are designed to ensure the availability, security, accuracy, and accessibility of data.  The main goal of data administration is to ensure that an organization's data is managed in a way that supports the organization's business objectives. ",1
941,"Data security is the practice of protecting digital data from unauthorized access, theft, destruction, alteration, or misuse.  It involves implementing measures and protocols to ensure the confidentiality, integrity, and availability of data. Data security involves a combination of physical, administrative, and technical safeguards to protect data from both intentional and unintentional threats.  Physical measures may include security cameras, locks, and access control systems.  Administrative measures may include security policies, employee training, and access controls.  Technical measures may include encryption, firewalls, and intrusion detection systems. Data security is essential for maintaining the privacy of sensitive information, such as personal identification information (PII), financial information, and confidential business data.  A data breach can result in significant financial and reputational damage, legal liability, and loss of customer trust. ",1
942,"Data integrity refers to the accuracy, completeness, and consistency of data.  It ensures that data is reliable and trustworthy, and that it can be used effectively for decision-making, analysis, and other business processes. Data integrity can be compromised by a variety of factors, including:Human error: This can include data entry errors, data processing errors, or mistakes in data manipulation. Malicious intent: This can include deliberate tampering with data, unauthorized access to data, or other malicious activities. System errors: This can include software bugs, hardware failures, or other system issues that affect the accuracy or completeness of data. ",1
943,"Data accessibility refers to the ease with which data can be accessed and used by authorized individuals or systems.  It is a critical aspect of data management, as it enables organizations to leverage their data assets to support business objectives. Data accessibility can be influenced by several factors, including:Data storage: The location and format of data can affect its accessibility.  Data that is stored in a centralized location, in a standardized format, and with clear data definitions is typically more accessible than data that is dispersed across multiple locations or in varied formats. Data security: Access controls and other security measures can affect data accessibility.  Data that is highly secure may be more difficult to access, while data that is less secure may be more vulnerable to unauthorized access. Data governance: Clear data governance policies and procedures can help to ensure that data is accessible to those who need it.  This can include establishing clear data ownership, developing data management processes, and ensuring that data is properly classified and labeled. Data integration: Integrating data from multiple sources into a single, unified view can increase data accessibility.  This can include developing data integration processes, establishing data integration standards, and using data integration tools. ",1
944,"DBMS stands for Database Management System.  It is a software system that is used to manage, store, and retrieve data in a database.  A DBMS is designed to provide a way to store and organize large amounts of data, as well as to provide a means of accessing that data quickly and efficiently. ",1
945,"Data communications refer to the transmission of data from one device or system to another.  It is a process of exchanging information between two or more devices, such as computers, smartphones, tablets, or other electronic devices. Data communications can take place over various types of communication channels, including wired and wireless networks, satellite links, or other types of communication infrastructure. ",1
946,"Data communications have numerous applications in various fields, including:Business: Data communications are used to facilitate communication between different departments, branches, and locations of a company.  They are also used for online transactions, remote work, and collaboration among team members. Education: Data communications are used in online learning environments, where students and teachers can communicate, collaborate, and share resources remotely. Healthcare: Data communications are used for telemedicine, where doctors and patients can communicate and share medical information remotely. Finance: Data communications are used for online banking, stock trading, and other financial transactions. Transportation: Data communications are used in traffic control systems, air traffic control systems, and GPS tracking systems. Entertainment: Data communications are used for online gaming, streaming media, and social media. Government: Data communications are used for public safety, emergency management, and public services. ",1
947,"Data compression is the process of reducing the amount of space required to store or transmit digital data by removing redundant or non-essential information from the data.  This is achieved by encoding the data using an algorithm that can be used to recreate the original data accurately. Data compression is used to save storage space, reduce the amount of time required to transmit data over a network, and reduce the cost of data storage and transmission.  The effectiveness of data compression depends on the type of data being compressed, as well as the compression algorithm used. ",1
948,"Data conversion is the process of changing data from one format or representation to another.  This can include converting data from analog to digital or from one digital format to another.  Data conversion can be necessary when data needs to be transferred from one system to another that uses a different format, or when data needs to be processed by a different application that requires a different format. ",1
949,"A data dictionary is a database or document that provides information about the data used in a database or information system.  It contains a list of all data elements or fields used in a database, along with their definitions, characteristics, relationships to other data elements, and any constraints or rules governing their use. A data dictionary can be used to provide a standard and consistent understanding of data elements across an organization or project team.  It can also be used as a reference tool for database designers, programmers, and users to ensure that everyone is using data in a consistent and accurate manner. ",1
950,"Data mining is the process of analyzing large datasets to discover hidden patterns, relationships, and insights.  It involves using advanced statistical and computational techniques to identify meaningful correlations and trends within the data, which can then be used to inform business decisions or improve processes. ",1
951,"In computer science, data structures are a way of organizing and storing data in a computer's memory or on disk so that it can be accessed and manipulated efficiently.  Data structures provide a way to represent complex data and algorithms in a compact and organized form. ",1
952,"There are many types of data structures, each with its own strengths and weaknesses depending on the specific application.  Here are some common types of data structures:Arrays: A collection of elements of the same type stored in contiguous memory locations. Linked lists: A data structure in which each element (node) contains a value and a reference to the next element. Trees: A hierarchical data structure in which each element (node) has one parent node and zero or more child nodes. Graphs: A data structure consisting of vertices (nodes) and edges connecting them, used to represent relationships between objects. Hash tables: A data structure that uses a hash function to map keys to values, providing fast access to elements. Queues: A collection of elements in which elements are added at one end (the back) and removed from the other end (the front) in a first-in, first-out (FIFO) order. Stacks: A collection of elements in which elements are added and removed from the same end (the top) in a last-in, first-out (LIFO) order. ",1
953,"Structured data types are data types that are made up of multiple elements or components, each with its own data type.  These elements can be combined to form more complex data structures that represent real-world objects or concepts. Some examples of structured data types include:Arrays: A collection of elements of the same data type, accessed using an index or a pointer. Structures: A collection of elements of different data types, grouped together under a single name. Classes: Similar to structures, but with additional features like inheritance and encapsulation. Records: A collection of elements of different data types, used to represent a single entity or concept. Enumerations: A set of named constants, used to represent a discrete set of values. Pointers: A variable that holds the memory address of another variable or data structure. Structured data types provide a way to organize and manipulate complex data in a more manageable way.  By grouping related data elements together, structured data types make it easier to represent real-world objects and concepts in software programs.  They also provide a way to pass multiple values between functions or modules in a program, making it easier to write modular and reusable code. ",1
954,"A data warehouse is a large and centralized repository of data that is designed to support business intelligence (BI) activities such as reporting, analytics, and data mining.  It is a system that collects, integrates, organizes, and stores data from a wide variety of sources, including transactional databases, external systems, and data streams. The data in a data warehouse is typically organized around a specific subject or theme, such as sales, marketing, or customer service.  It is also typically structured in a way that is optimized for querying and analysis, with data organized into tables, columns, and rows. Data warehouses are designed to support complex queries and analytics that can uncover insights and patterns that might not be visible in individual transactional databases.  They also often provide tools for data transformation, cleansing, and enrichment to ensure that the data is accurate and consistent. Overall, the goal of a data warehouse is to provide a single, unified view of an organization's data that can be used for strategic decision-making and analysis. ",1
955,"A Decision Support System (DSS) is a computer-based information system that provides analytical tools and models to help individuals and organizations make informed decisions.  It is a software system that helps users to analyze complex business problems and identify solutions based on data analysis, modeling, and visualization. A DSS typically consists of three main components: a database or data warehouse that stores relevant data, a user interface that allows users to interact with the system and provide input, and a set of analytical tools and models that help users to analyze the data and make decisions. DSSs can be used in a wide range of applications, such as financial planning, inventory management, supply chain optimization, and marketing analysis.  They can also be used in various industries, including healthcare, finance, manufacturing, and transportation. ",1
956,"Dell is an American multinational computer technology company that designs, develops, manufactures, and sells a wide range of computer hardware and software products and services.  The company was founded in 1984 by Michael Dell, who started the business by selling computer upgrades and parts out of his dorm room at the University of Texas at Austin. Today, Dell is one of the largest technology companies in the world, with operations in over 180 countries and more than 165,000 employees worldwide.  The company offers a wide range of products and services, including personal computers, servers, storage devices, network switches, software, and peripherals. Dell is known for its direct sales model, which allows customers to order products online or over the phone and have them delivered directly to their doorstep.  This model has allowed the company to reduce costs, improve efficiency, and provide customers with a high level of customization and support. ",1
957,"In computing, a demon (sometimes spelled daemon) is a computer program or process that runs in the background and performs specific tasks or services without requiring user intervention.  Demons are typically used in operating systems and network servers to manage resources, monitor system performance, and provide various types of services. One of the key characteristics of a demon is that it runs continuously in the background, waiting for specific events or conditions to occur before it executes a particular task.  For example, a network demon might monitor network traffic and respond to specific types of requests, such as file transfers or email messages.  A system demon might manage system resources such as memory or disk space, or perform regular maintenance tasks such as backups or system updates. Demons are often associated with Unix and Unix-like operating systems, where they are managed by the system's init process.  In these systems, demons are typically started at boot time and run continuously until the system is shut down or the demon is explicitly stopped. The term ""demon"" is sometimes used interchangeably with the term ""service"" or ""background process"".  However, in some contexts, ""demon"" specifically refers to a program or process that runs continuously and provides a specific type of service or functionality, while ""service"" may refer more generally to any program or process that provides a service or function on a computer system. ",1
958,"Design patterns are reusable solutions to commonly occurring problems in software design.  They are essentially templates for solving design problems that arise during software development.  Design patterns provide a common language and framework for developers to communicate and share best practices. Design patterns were first popularized in the 1990s by a book called ""Design Patterns: Elements of Reusable Object-Oriented Software"" by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides (commonly referred to as the ""Gang of Four"" or ""GoF"").  The book presented 23 design patterns organized into three categories: creational, structural, and behavioral. Creational patterns are used to create objects in a flexible and reusable way, while structural patterns deal with the composition of classes and objects to form larger structures.  Behavioral patterns focus on communication between objects and the assignment of responsibilities. ",1
959,"In software engineering, a pattern is a proven solution to a recurring problem in software design.  Patterns help software developers to create efficient, scalable, and maintainable software systems by providing a set of best practices and reusable components. Patterns can be categorized into several types, including:Creational patterns: These patterns deal with object creation mechanisms, trying to create objects in a manner that suits the situation the developer is in.  Examples of creational patterns include Singleton, Factory, Abstract Factory, and Builder. Structural patterns: These patterns focus on object composition, providing ways to organize and structure classes and objects to solve design problems.  Examples of structural patterns include Adapter, Bridge, Decorator, Facade, Flyweight, and Proxy. Behavioral patterns: These patterns define communication patterns between objects and classes to solve common design problems.  Examples of behavioral patterns include Chain of Responsibility, Command, Interpreter, Iterator, Mediator, Memento, Observer, State, Strategy, Template Method, and Visitor. ",1
960,"Desktop publishing (DTP) is the process of using computer software and hardware to create visual communications, such as brochures, flyers, newsletters, magazines, books, and other printed materials.  DTP involves using specialized software to design layouts, format text, and incorporate images and other graphics into a document. With desktop publishing, designers have greater control over the appearance of their documents than with traditional typesetting methods.  They can use a variety of fonts, sizes, and colors, and add images and other visual elements to create visually appealing and informative documents. ",1
961,"A device driver is a software program that allows the operating system to communicate with and control a specific hardware device such as a printer, scanner, or graphics card.  Device drivers act as a bridge between the hardware device and the operating system, enabling the device to perform its intended functions. When a device is first installed or connected to a computer, the operating system checks to see if a compatible driver is already installed.  If it is not, the operating system will prompt the user to install the necessary driver software.  Once installed, the device driver allows the operating system to interact with the device, sending and receiving data and commands as needed. Device drivers are specific to the hardware they control and are often updated to fix bugs or add new features.  It is important to keep device drivers up-to-date to ensure optimal performance and compatibility with other software and hardware components. ",1
962,"Digital cash, also known as electronic cash or e-cash, refers to a form of digital currency that enables secure online transactions without the need for physical cash or credit cards.  Digital cash is essentially a digital representation of real-world currency that can be used to buy goods and services online, transfer money between individuals, and conduct other types of financial transactions. Digital cash typically operates through the use of encryption techniques and digital signatures to ensure the authenticity and security of the transactions.  When a user wants to make a purchase or transfer money, they typically use a digital wallet or mobile app to initiate the transaction.  The digital cash is then transferred from the user's account to the recipient's account, usually through a secure payment gateway. One of the main advantages of digital cash is its convenience and ease of use.  Users can make transactions from anywhere with an internet connection, without the need for physical cash or credit cards.  Digital cash is also more secure than traditional payment methods, as it is protected by encryption and digital signatures that make it difficult to counterfeit or hack. ",1
963,"Digital convergence refers to the merging of various forms of media and communication technologies into a single, integrated system.  This can include the integration of telecommunications, computing, and media technologies, and the ability to access and share information across multiple devices and platforms. In practice, digital convergence can take many different forms.  For example, smartphones are a prime example of digital convergence, as they combine a wide range of features and technologies, including voice and video communication, internet access, social media, and entertainment.  Similarly, smart homes bring together various digital technologies such as security systems, home automation, and entertainment systems. ",1
964," digital dashboard is a data visualization tool that displays a collection of key performance indicators (KPIs) and other important metrics in a single, easy-to-use interface.  Digital dashboards are typically used in business settings to help managers and executives monitor and analyze important data, such as sales figures, financial performance, and customer satisfaction. Digital dashboards are designed to be highly customizable, allowing users to choose which metrics and KPIs they want to display, as well as how the data is presented.  Dashboards can include a variety of different data visualizations, such as charts, graphs, tables, and gauges, which help users quickly and easily understand complex data sets. ",1
965,"he term digital divide refers to the unequal access to and use of digital technologies and the internet among different individuals and communities.  This can be due to a variety of factors, such as geographic location, socio-economic status, education level, and age. In many cases, individuals and communities with limited access to digital technologies and the internet may face significant disadvantages in terms of education, employment opportunities, and access to essential services such as healthcare and government services.  This can exacerbate existing social and economic inequalities and hinder overall economic growth and development. ",1
966,"Digital Rights Management (DRM) is a technology that is used to protect and manage digital content, such as music, videos, e-books, and software.  DRM is designed to prevent unauthorized access, copying, distribution, and modification of digital content. DRM typically involves the use of encryption and other technologies to control access to digital content and enforce usage restrictions.  For example, DRM may limit the number of devices that a user can access content on, restrict the amount of time content can be accessed, or prevent users from sharing or copying the content. DRM can be implemented in a variety of ways, depending on the type of digital content and the business model of the content provider.  For example, some music and video streaming services use DRM to prevent users from downloading and sharing content, while e-book providers may use DRM to restrict access to their content to specific devices or apps. While DRM can be effective in protecting digital content and ensuring that content creators and distributors are fairly compensated for their work, it has also been criticized for limiting user rights and freedoms.  Some critics argue that DRM can be overly restrictive and may interfere with users' ability to access and use digital content in legitimate ways. ",1
967,"Distributed computing is a method of computing in which a large number of computers work together to solve a complex problem or perform a task.  The computers in a distributed computing system are connected over a network, and each computer in the network contributes its processing power and resources to the task at hand. Distributed computing is used in a wide range of applications, including scientific research, data analysis, and large-scale simulations.  For example, a distributed computing system may be used to analyze large datasets, search for patterns in complex data, or simulate complex physical systems. ",1
968,"Distributed computing has a wide range of applications across many different fields, including science, engineering, finance, and entertainment.  Some of the key applications of distributed computing include:Scientific research: Distributed computing is widely used in scientific research to perform large-scale simulations, model complex physical systems, and analyze massive amounts of data.  Projects such as SETI@home and Folding@home use distributed computing to analyze signals from outer space and simulate protein folding, respectively. Big data analytics: Distributed computing is also used in big data analytics to process and analyze large volumes of data.  Distributed computing frameworks such as Apache Hadoop and Apache Spark allow for the distributed processing of large datasets across multiple computers. Cloud computing: Cloud computing is a form of distributed computing that allows for on-demand access to computing resources over a network.  Cloud computing services such as Amazon Web Services (AWS) and Microsoft Azure provide users with access to vast computing resources that can be used for a wide range of applications. Content delivery: Distributed computing is used in content delivery networks (CDNs) to deliver content such as web pages, videos, and images to users around the world.  CDNs distribute content across multiple servers located in different geographic locations, reducing latency and improving performance. ",1
969,"The Domain Name System (DNS) is a system that translates human-readable domain names into IP addresses, which are the numerical addresses that computers use to identify each other on the internet.  DNS allows users to access websites and other resources using easy-to-remember domain names, rather than having to remember the numerical IP addresses. When a user enters a domain name into a web browser, the browser sends a request to a DNS server to resolve the domain name into an IP address.  The DNS server then looks up the IP address associated with the domain name and returns it to the browser, which can then connect to the appropriate server to access the website or other resource. DNS is hierarchical and distributed, with a global network of DNS servers that work together to resolve domain names.  At the top of the hierarchy are the root servers, which store information about the top-level domain names (such as . com, . org, and . net).  Below the root servers are the authoritative DNS servers, which store information about individual domain names and their associated IP addresses. ",1
970,"The Document Object Model (DOM) is a programming interface for web documents.  It represents the web page as a tree-like structure, with each element on the page (such as headings, paragraphs, images, and links) represented as a node in the tree. The DOM is accessed using scripting languages such as JavaScript, and provides a way for developers to manipulate the content and structure of web pages dynamically.  By using the DOM, developers can create interactive web applications that respond to user actions, update content dynamically, and provide a rich user experience. ",1
971,"Digital Subscriber Line (DSL) is a type of broadband internet technology that uses existing telephone lines to provide high-speed internet access.  DSL works by dividing the available bandwidth on a telephone line into two channels: one for voice communication and one for data communication. DSL technology is designed to work with existing copper telephone lines, which are already installed in most homes and businesses.  DSL modems are used to convert the digital signals from a computer into the analog signals used by the telephone line, and vice versa. ",1
972,"Digital Video Recording (DVR) is a technology that allows users to record, store, and playback digital video content.  DVRs are commonly used in home entertainment systems, security systems, and other applications where video recording is required. A typical DVR consists of a digital tuner, a hard drive for storing recorded video content, and software for managing and playing back recorded content.  The digital tuner is used to receive television signals, which are then recorded onto the hard drive for later playback. DVRs provide several advantages over traditional analog video recording systems.  They offer higher quality video, greater storage capacity, and the ability to pause, rewind, and fast-forward live television broadcasts.  DVRs also allow users to schedule recordings of their favorite shows, so they can watch them later at their convenience. ",1
973,"C is a general-purpose, procedural programming language that was developed by Dennis Ritchie at Bell Labs in the early 1970s.  It is widely used for system programming, such as writing operating systems, device drivers, and embedded systems, as well as for developing applications in various domains such as finance, healthcare, and gaming. C is a low-level language, meaning that it provides direct access to computer hardware and system resources, such as memory and CPU registers.  This makes C a powerful language for developing applications that require close control over hardware resources and efficient memory management. C is also a structured language, meaning that it provides constructs for organizing code into logical structures, such as functions, loops, and conditional statements.  This makes it easier to write and maintain complex programs. ",1
974,"C# (pronounced ""C sharp"") is a modern, object-oriented programming language developed by Microsoft as part of the . NET platform.  It was first released in 2002 and has since become one of the most popular programming languages for developing desktop applications, web applications, and mobile apps. C# is a strongly typed language, which means that variables must be declared with a specific data type before they can be used.  It also supports several programming paradigms, including procedural programming, object-oriented programming, and functional programming. One of the key features of C# is its integration with the . NET platform, which provides a large library of pre-built classes and functions for common programming tasks.  This makes it easier for developers to write code and reduces the amount of time and effort required to develop applications. C# is used to develop a wide range of applications, including desktop applications, web applications, and mobile apps.  It is particularly popular for developing Windows applications, and is commonly used for developing games, business applications, and scientific applications. ",1
975,"C++ is a powerful and versatile object-oriented programming language that was developed in the 1980s as an extension of the C programming language.  It was designed to provide developers with a more powerful and flexible language that could be used to create a wide range of applications, from system software to games and multimedia applications. C++ is a compiled language, which means that it is converted into machine code by a compiler before being executed.  This gives C++ programs excellent performance and makes it a popular choice for developing applications that require fast execution times. One of the key features of C++ is its support for object-oriented programming (OOP).  This allows developers to organize their code into classes and objects, which can be used to encapsulate data and behavior, making code more modular, reusable, and easier to maintain. C++ also includes support for templates, which allow developers to write generic code that can be used with a wide range of data types.  This makes it easier to write code that is both flexible and efficient. ",1
976,"C++ has been one of the most popular programming languages for over three decades, and its growth has been driven by several factors. Performance: C++ is a compiled language that is known for its high performance and efficient use of system resources.  This has made it a popular choice for developing applications that require fast execution times, such as games, scientific applications, and system software. Object-oriented programming: C++ was designed to support object-oriented programming (OOP), which allows developers to organize their code into classes and objects, making code more modular and easier to maintain.  This has made C++ a popular choice for developing large, complex applications. Compatibility: C++ is a widely used language that is supported by a wide range of operating systems and hardware architectures, making it an ideal choice for developing cross-platform applications. Libraries: C++ has a large and growing library of pre-built classes and functions that can be used to perform common programming tasks.  This has made it easier for developers to write code and reduced the amount of time and effort required to develop applications. Community: C++ has a large and active community of developers, who contribute to open-source libraries and share their knowledge through forums, blogs, and other online resources.  This has helped to drive the growth of the language and ensure that it remains up-to-date with the latest trends and technologies. ",1
977,"n computing, cache refers to a high-speed data storage layer that stores recently used data and instructions to provide faster access to that data in the future. When data is requested by a program, the computer first checks if the data is available in the cache memory.  If it is, the computer retrieves the data from the cache instead of going to the original source, such as the hard drive or network, which is slower.  This results in faster access to frequently used data, which in turn improves the overall performance of the computer system. Caches can be found in many different parts of a computer system, including the processor, hard drive, and web browser.  They can be either hardware-based, such as a processor cache, or software-based, such as a web browser cache. ",1
978,"The CPU cache is a small and fast memory storage area located on the processor chip that is used to store frequently used data and instructions.  Its purpose is to speed up the processing time of the CPU by reducing the number of times it needs to access the slower main memory or RAM. When a program requests data or instructions from the main memory, the CPU first checks if the requested data or instruction is present in the cache.  If it is, the CPU retrieves the data from the cache instead of going to the main memory.  This process is much faster because the cache is located on the processor chip and has a much shorter access time than the main memory. ",1
979,"Disk cache is a mechanism used by computer operating systems to improve disk I/O (input/output) performance. When a program reads data from a disk, the operating system caches the data in memory, so that the next time the program needs to access the same data, it can be retrieved quickly from memory instead of having to read it from the disk again.  Similarly, when a program writes data to a disk, the operating system caches the data in memory and writes it to the disk at a later time when it is more efficient to do so. The disk cache can be either a dedicated portion of RAM or a portion of the hard drive that has been set aside for this purpose.  By caching frequently accessed data in memory, the operating system can reduce the amount of time spent waiting for disk I/O operations, which can improve overall system performance. ",1
980,"Network cache is a mechanism used to improve network performance by storing frequently accessed data closer to the user or application, rather than retrieving it repeatedly from a remote server. Network cache can operate at different levels, such as in web browsers, content delivery networks (CDNs), and proxy servers.  For example, web browsers use a cache to store web pages, images, and other resources that are frequently accessed by the user, so that they can be retrieved quickly without having to download them from the internet again.  CDNs use a network of servers located in different regions to cache content, so that users can access it from a server that is geographically closer to them, reducing the latency and improving the performance. Proxy servers also use cache to store frequently accessed content locally, reducing the load on the upstream server and improving the response time for subsequent requests.  This is particularly useful in corporate networks where multiple users access the same content, such as internal web pages, and email attachments. ",1
981,"Electronic calculators are handheld devices or software applications that are used to perform mathematical calculations, including basic arithmetic operations (addition, subtraction, multiplication, and division), as well as more complex functions such as square roots, percentages, exponentials, logarithms, and trigonometric functions. The first electronic calculator was developed in the 1960s and was a significant improvement over mechanical calculators, which were slow and cumbersome.  Electronic calculators are much faster and more accurate than mechanical calculators, and they can perform more complex calculations. Electronic calculators typically have a numeric keypad, a display screen, and a microprocessor that performs the calculations.  They can also have memory functions that allow users to store and recall numbers or perform multiple calculations at once. ",1
982,"Computing has had a significant impact on the design, manufacturing, and operation of cars, leading to the development of various computer-based systems and technologies in the automotive industry.  Some examples of how computing is used in cars are:Engine Management Systems: Modern cars have sophisticated engine management systems that use computer algorithms to control the fuel injection, ignition timing, and other aspects of engine operation.  These systems use sensors to monitor various parameters like engine temperature, air/fuel ratio, and throttle position, and adjust the engine parameters accordingly to optimize performance, fuel efficiency, and emissions. Infotainment Systems: Cars today are equipped with advanced infotainment systems that provide entertainment, navigation, and communication functions.  These systems use touchscreens, voice commands, and other input devices to allow drivers and passengers to control music, phone calls, messaging, and other functions while driving. Autonomous Driving: The development of self-driving cars is one of the most exciting areas of research in the automotive industry.  Self-driving cars use a variety of sensors, including cameras, radar, and lidar, to detect and analyze their surroundings and make decisions about how to navigate.  These systems use complex computer algorithms to control the vehicle's speed, steering, and braking, allowing it to operate autonomously. Safety Systems: Many modern cars come equipped with advanced safety systems that use computing technology to detect and prevent accidents.  For example, anti-lock braking systems (ABS) use sensors and computer algorithms to prevent skidding and maintain control of the vehicle during emergency braking, while electronic stability control (ESC) systems use sensors and actuators to prevent the vehicle from sliding or spinning out of control. ",1
983,"Smart cars are vehicles equipped with advanced computing and communication technologies that enable them to collect, analyze, and share data to provide a wide range of features and services.  These technologies include:Advanced Driver Assistance Systems (ADAS): Smart cars are equipped with a variety of sensors and cameras that enable ADAS features such as lane departure warning, automatic emergency braking, adaptive cruise control, and blind spot monitoring.  These features use computer algorithms to analyze data from the sensors and provide alerts or take action to prevent accidents. Connectivity: Smart cars are connected to the internet and other vehicles, enabling them to share data about traffic conditions, road hazards, and weather conditions.  This connectivity also allows for over-the-air software updates, remote diagnostics, and other services. Infotainment: Smart cars offer advanced infotainment systems that provide features such as internet connectivity, voice commands, music streaming, and navigation.  These systems are often integrated with the vehicle's ADAS features to provide real-time information and alerts. Self-driving: Some smart cars are capable of autonomous driving, using a combination of sensors, cameras, and computer algorithms to navigate roads without human intervention.  These cars are still in development and are not yet widely available. Energy Efficiency: Smart cars often use advanced technologies such as regenerative braking, stop-start systems, and aerodynamic designs to improve fuel efficiency and reduce emissions. ",1
984,"Cascading Style Sheets (CSS) is a language used for describing the visual presentation of a web page, including its layout, colors, fonts, and other design elements.  CSS is used in conjunction with HTML and JavaScript to create dynamic and visually appealing web pages. CSS works by defining styles for HTML elements, such as headings, paragraphs, and links, using selectors and properties.  Selectors are used to target specific HTML elements, while properties define the visual appearance of those elements.  For example, the following CSS code sets the font family, color, and background color for all headings in a web page",1
985,"Computer-aided software engineering (CASE) refers to the use of computer-based tools and methods to support software development and engineering activities.  CASE tools are designed to automate and streamline various tasks in the software development process, including requirements analysis, design, coding, testing, and maintenance. CASE tools can be divided into several categories, including:Upper CASE: These tools are used in the early stages of software development to support activities such as requirements analysis, system modeling, and design.  Examples of upper CASE tools include entity-relationship diagrams (ERD), data flow diagrams (DFD), and Unified Modeling Language (UML) tools. Lower CASE: These tools are used in the later stages of software development to support activities such as coding, debugging, and testing.  Examples of lower CASE tools include code generators, debuggers, and testing frameworks. Integrated CASE (ICASE): These tools integrate both upper and lower CASE tools, providing a complete software development environment.  ICASE tools often include features such as version control, project management, and team collaboration tools. ",1
986,"Design tools are computer software or web applications used to create, modify, and manipulate visual elements such as graphics, illustrations, animations, and layouts.  These tools are used by designers, artists, and creatives to bring their ideas to life and communicate visually. Some popular design tools include:Graphic design software: These tools are used to create and edit graphics, such as logos, posters, and advertisements.  Examples include Adobe Photoshop, Illustrator, and InDesign. Prototyping tools: These tools are used to create interactive prototypes of websites, apps, or other digital products.  Examples include Sketch, Figma, and Adobe XD. 3D modeling software: These tools are used to create 3D models of objects, characters, and environments.  Examples include Autodesk 3ds Max, Maya, and Blender. Animation software: These tools are used to create animated graphics, videos, and special effects.  Examples include Adobe After Effects, Autodesk Maya, and Cinema 4D. Web design software: These tools are used to create and edit web pages, using technologies such as HTML, CSS, and JavaScript.  Examples include Adobe Dreamweaver, WordPress, and Wix. ",1
987,"Analysis tools are computer software or web applications that are used to analyze data, identify patterns, and draw insights from large and complex datasets.  These tools are used by analysts, researchers, and data scientists to process and interpret data, in order to make informed decisions and predictions. Some popular analysis tools include:Statistical analysis software: These tools are used to perform statistical analysis on data, such as hypothesis testing, regression analysis, and clustering.  Examples include R, SAS, and SPSS. Data visualization software: These tools are used to create charts, graphs, and other visual representations of data, in order to help users understand and communicate data insights.  Examples include Tableau, Power BI, and QlikView. Business intelligence software: These tools are used to analyze data from various sources, such as sales data, customer data, and financial data, in order to make informed business decisions.  Examples include SAP BusinessObjects, IBM Cognos, and Oracle Business Intelligence. Machine learning software: These tools are used to train and deploy machine learning models, in order to make predictions and automate decision-making.  Examples include TensorFlow, Scikit-learn, and Keras. Big data tools: These tools are used to process and analyze large and complex datasets, such as those generated by social media, sensors, and IoT devices.  Examples include Apache Hadoop, Spark, and Cassandra. ",1
988,"CD-ROM stands for Compact Disc Read-Only Memory.  It is a type of optical disc used to store digital data, such as software, music, video, and documents. A CD-ROM disc is a round, flat disc made of plastic with a diameter of 120 mm (4. 7 inches) and a thickness of 1. 2 mm (0. 05 inches).  The disc is coated with a thin layer of aluminum or silver, and the data is stored on the disc as microscopic pits and lands.  The disc is read by a laser beam in a CD-ROM drive, which detects the differences in reflectivity between the pits and lands. CD-ROMs have a capacity of up to 700 MB of data, which is equivalent to about 80 minutes of audio.  They were first introduced in 1985 and quickly became popular for distributing software and other digital content.  CD-ROMs are widely used because they are inexpensive, portable, and can be read by a wide range of devices, including computers, game consoles, and standalone CD players. ",1
989,"Recordable CDs, also known as CD-Rs, are a type of optical disc that can be written to or recorded with digital data, such as music, video, or software.  Unlike read-only CD-ROMs, CD-Rs can be used to store new data and are widely used for data backup, archiving, and distribution. A recordable CD is similar in appearance to a CD-ROM, with a diameter of 120 mm (4. 7 inches) and a thickness of 1. 2 mm (0. 05 inches).  The disc is made of a polycarbonate substrate coated with a layer of dye and a layer of reflective material.  When the disc is burned, the laser beam heats up the dye and creates microscopic pits and lands that can be read by a CD player or drive. ",1
990,"DVD-ROM is a type of optical disc that is used to store digital data.  The acronym ""DVD"" stands for Digital Versatile Disc or Digital Video Disc.  The ""ROM"" in DVD-ROM stands for Read-Only Memory, which means that the data on the disc can only be read and not written or erased. A DVD-ROM can store up to 4. 7 gigabytes (GB) of data on a single-sided, single-layer disc, and up to 8. 5 GB on a single-sided, dual-layer disc.  This makes it an excellent medium for storing large amounts of data, including movies, music, software, and other digital files. DVD-ROMs are commonly used for distributing commercial movies, software applications, and games.  They can be played on DVD-ROM drives, which are available in most computers and DVD players. The data on a DVD-ROM is stored in a series of pits and lands on the surface of the disc.  A laser in the DVD-ROM drive reads the pits and lands as the disc spins, translating them into digital data that can be used by the computer or other device. ",1
991,"Cellular automata are mathematical models consisting of a grid of cells, each of which can be in one of a finite number of states, and a set of rules that determine how the cells change state over time.  Cellular automata have been used in a wide variety of fields, including:Physics: Cellular automata can be used to model physical systems, such as the behavior of fluids, gases, and solids.  They can also be used to simulate the behavior of subatomic particles and the evolution of the universe. Biology: Cellular automata have been used to model biological systems, such as the spread of diseases, the growth of bacterial colonies, and the behavior of animal populations. Computer Science: Cellular automata have been used in computer science for a variety of applications, including cryptography, image processing, and artificial intelligence. Economics: Cellular automata have been used to model economic systems, such as the behavior of stock markets and the spread of financial crises. Sociology: Cellular automata have been used to model social systems, such as the spread of rumors, the diffusion of innovations, and the emergence of social norms. Art: Cellular automata have been used as a tool for creating art and music, often by using the patterns generated by the rules to create visual or auditory stimuli. ",1
992,"Censorship refers to the suppression or control of information or ideas by an authority or group in order to prevent their dissemination or expression.  When it comes to the internet, censorship refers to the control or suppression of information that is transmitted over the internet. Censorship on the internet can take various forms.  Governments may use technical measures such as website blocking, content filtering, and IP address blocking to restrict access to certain websites or information.  They may also monitor online activities and communications to identify and punish individuals who engage in prohibited activities, such as dissent or criticism of the government. Private companies, such as social media platforms and search engines, may also engage in censorship by removing or limiting access to certain content or users based on their policies or community standards.  This is often done to prevent the spread of harmful or offensive content, such as hate speech or misinformation. Advocates of internet censorship argue that it is necessary to protect national security, public order, and social values.  They may also claim that certain types of content, such as pornography or extremist propaganda, are harmful to individuals or society and should be restricted. Critics of internet censorship argue that it infringes on freedom of expression and access to information, which are fundamental human rights.  They may also argue that censorship can be used as a tool for political control and repression, and that it can stifle creativity and innovation. ",1
993,"CGI stands for ""Computer Generated Imagery"" and refers to the use of computer graphics to create visual content for films, television shows, video games, and other media.  CGI can range from simple 2D animations to complex 3D models and visual effects. CGI involves the use of specialized software programs to create digital images and manipulate them in a virtual environment.  This allows artists and designers to create highly realistic and detailed graphics, as well as complex animations and simulations. CGI is widely used in the entertainment industry for creating special effects, such as explosions, fire, water, and other natural phenomena.  It is also used to create virtual sets and environments that would be difficult or impossible to film in the real world. In addition to entertainment, CGI is used in fields such as architecture, engineering, and medicine to create visualizations and simulations of complex systems and structures.  It is also used in advertising and marketing to create eye-catching visuals and animations. Overall, CGI has revolutionized the way visual content is created and has opened up new possibilities for creative expression and storytelling in various industries. ",1
994,"In computer programming, a character is a single symbol, such as a letter, number, or punctuation mark, that can be represented in a computer's memory using a binary code.  Characters are used to represent textual data, such as words and sentences, in computer programs. A string is a sequence of characters that are treated as a single unit in a computer program.  Strings are typically used to represent text data, such as names, addresses, and other types of textual information.  In many programming languages, strings are denoted by enclosing the sequence of characters within quotation marks. Strings are useful for manipulating text data in computer programs.  Common operations on strings include concatenation (combining two or more strings into a single string), substring extraction (extracting a portion of a string), and searching for specific characters or patterns within a string. In addition to representing plain text, strings can also be used to represent other types of data, such as binary data or encoded data.  For example, a string could be used to represent a sequence of bytes that make up an image or a sound file. ",1
995,"String-oriented languages are programming languages that provide specialized features and functions for working with strings, which are sequences of characters used to represent textual data.  These languages are designed to make it easier to manipulate and process strings, which are a common type of data in many applications, such as text editors, databases, and web applications. Some examples of string-oriented languages include:Perl: Perl is a high-level, interpreted language that is widely used for web development, system administration, and network programming.  It provides powerful features for working with strings, including regular expressions, string interpolation, and string manipulation functions. Python: Python is a popular interpreted language that is widely used for scientific computing, data analysis, and web development.  It includes a rich set of string manipulation functions, such as string slicing, concatenation, and searching. Ruby: Ruby is a high-level, object-oriented language that is known for its simplicity and flexibility.  It provides powerful string manipulation features, such as regular expressions, string interpolation, and string slicing. JavaScript: JavaScript is a widely used language for web development that provides powerful string manipulation functions, such as string concatenation, substring extraction, and search functions. ",1
996,"In the computing industry, commercial services refer to products and services that are provided by businesses or organizations in exchange for payment.  These services are typically offered to customers who require specific computing-related goods or services. Some examples of commercial services in computing include:Cloud computing services: These services provide computing resources, such as servers, storage, and software applications, to customers over the internet.  Examples of cloud computing services include Amazon Web Services (AWS), Microsoft Azure, and Google Cloud. Software as a Service (SaaS): SaaS providers offer software applications that are hosted on their servers and accessed by customers over the internet.  Examples of SaaS applications include Microsoft Office 365, Salesforce, and Dropbox. Managed IT services: These services provide IT support, maintenance, and management for businesses and organizations.  Managed IT service providers may offer services such as network monitoring, security management, and software updates. Web hosting: Web hosting services provide storage and access to websites and web applications over the internet.  Examples of web hosting providers include GoDaddy, Bluehost, and HostGator. Data storage and backup: These services provide storage and backup solutions for data, such as files, documents, and databases.  Examples of data storage and backup providers include Dropbox, Google Drive, and Carbonite. Overall, commercial services in computing are an important part of the industry, providing businesses and organizations with access to computing resources and services that they may not have the expertise or resources to manage in-house. ",1
997,"In computing, distributed services refer to a model of providing services or applications that are spread across multiple networked computers, often referred to as nodes.  The purpose of this model is to increase scalability, fault tolerance, and flexibility of the service or application. Distributed services allow a system to provide services to its users even if some of the nodes in the system fail.  They also provide a way to scale up or down the system's capacity by adding or removing nodes as needed.  This can be particularly useful for applications that require high availability or that need to process large amounts of data. Some examples of distributed services include:Distributed file systems: These systems provide a way to store and access files across multiple networked computers.  Examples include Hadoop Distributed File System (HDFS) and GlusterFS. Distributed databases: These databases are spread across multiple nodes and allow for the storage and retrieval of large amounts of data.  Examples include Apache Cassandra and MongoDB. Distributed computing frameworks: These frameworks provide a way to distribute computing tasks across multiple nodes in order to speed up processing.  Examples include Apache Spark and Apache Hadoop. Distributed messaging systems: These systems provide a way for nodes in a distributed system to communicate with each other.  Examples include Apache Kafka and RabbitMQ. Distributed web services: These services allow for the distribution of web-based applications across multiple nodes.  Examples include Amazon Web Services (AWS) Elastic Load Balancing (ELB) and Microsoft Azure Load Balancer. Overall, distributed services are an important part of computing, allowing for the creation of large-scale, fault-tolerant, and highly available systems that can handle a variety of workloads. ",1
998,"Chatterbots, also known as chatbots or simply bots, are computer programs designed to simulate conversation with human users, primarily through text-based communication.  These bots use artificial intelligence (AI) techniques, such as natural language processing (NLP), to understand and respond to user input in a way that mimics human conversation. Chatterbots can be programmed to perform a variety of tasks, such as answering customer service inquiries, providing information about products or services, and even engaging in casual conversation.  Some popular examples of chatbots include Apple's Siri, Amazon's Alexa, and Google Assistant. Chatbots can be designed to operate through various platforms such as messaging apps, social media platforms, and websites.  In recent years, advancements in AI and machine learning have enabled chatbots to become increasingly sophisticated, allowing them to understand context, recognize sentiment, and even learn from previous interactions with users. ",1
999,"Chess has a long history of being used as a benchmark for measuring the capabilities of computers, specifically in the area of artificial intelligence (AI).  Chess is a complex game that requires strategy, foresight, and decision-making, making it a challenging task for computers to master. The first notable example of computer chess was the 1950s when computer scientists began experimenting with programming computers to play chess.  However, it wasn't until the 1970s when the first computer chess tournament was held, and the first commercially available computer chess program, Chess 4. 0, was released. Since then, computer chess has made significant progress, and today, the best computer chess programs can easily defeat even the most skilled human players.  This progress has been driven by advances in hardware and software, as well as improvements in AI techniques such as search algorithms, game tree pruning, and machine learning. In 1997, IBM's Deep Blue computer famously defeated chess world champion Garry Kasparov in a highly publicized match.  This event marked a significant milestone in computer chess and demonstrated that computers could outperform human players in certain areas. ",1
1000,"Chess has been an important testbed for the development of artificial intelligence (AI) since the earliest days of computing.  The game's complexity and strategic depth make it an ideal domain for exploring the limits of machine intelligence and testing new AI algorithms. One of the key challenges in developing AI systems for chess is the enormous search space of possible moves.  In the game of chess, there are approximately 10^120 possible games, which is many orders of magnitude larger than the number of atoms in the observable universe.  To overcome this challenge, AI researchers have developed a range of techniques, such as alpha-beta pruning, minimax search, and Monte Carlo tree search, to efficiently explore the search space and find good moves. Another challenge in developing AI for chess is representing the game state in a way that is amenable to machine learning.  Historically, this has been done using hand-crafted features that encode various aspects of the game state, such as the relative values of the pieces and the mobility of the pieces.  However, in recent years, deep learning techniques have been used to automatically learn representations of the game state from raw board positions. ",1
1001,"A chipset is a set of electronic components on a computer's motherboard that manages the flow of data between the processor, memory, input/output devices, and other components.  The chipset provides the fundamental interface between the processor and the rest of the system, enabling communication and coordination between the various hardware components. The chipset is composed of two main components: the northbridge and the southbridge.  The northbridge is responsible for connecting the processor to the memory and high-speed input/output devices, such as the graphics card.  The southbridge connects to the slower input/output devices, such as USB and SATA controllers, as well as providing support for legacy devices. Chipsets are essential for the proper functioning of a computer system, as they ensure that data flows efficiently between the various components.  The chipset also plays a role in determining the overall performance of the system, as it can affect the speed and responsiveness of the processor, memory, and other components. ",1
1002,"In computing, the term ""last mile"" refers to the final leg of the telecommunications network that connects the end user to the wider network.  The last mile typically refers to the physical connection between a user's premises (such as a home or office) and the telecommunications provider's network infrastructure. The last mile is often considered the most challenging and expensive part of the telecommunications network to build and maintain, as it involves deploying physical infrastructure (such as cables or wireless towers) to reach individual homes and businesses.  In many cases, the cost of deploying and maintaining this infrastructure can be prohibitively high, particularly in rural or remote areas. The quality and reliability of the last mile connection can have a significant impact on the performance and usability of internet and telecommunications services.  For example, if the last mile connection is slow or unreliable, users may experience slow internet speeds, dropped calls, or other issues that impact their ability to access and use digital services. There are several technologies used to provide last mile connectivity, including wired technologies such as DSL, cable modems, and fiber optic cables, as well as wireless technologies such as cellular networks, satellite internet, and fixed wireless connections. ",1
1003,"In object-oriented programming (OOP), a class is a blueprint or template for creating objects.  It defines a set of properties (also known as attributes or fields) and methods that are common to all objects of a certain type. A class provides a way to encapsulate data and functionality, allowing for modular and organized programming.  It defines the behavior and properties of a certain type of object, but does not create any actual objects itself.  Instead, an object is created from a class using a process called instantiation. A class can be thought of as a template or a cookie cutter, while an object is an instance of that template or cookie cutter.  For example, a class called ""Person"" might define attributes such as name, age, and address, and methods such as ""walk"" and ""speak"".  An object created from the Person class would have specific values for those attributes (such as ""John"", 30, and ""123 Main St. ""), and would be able to perform the methods defined in the class. Classes can also inherit properties and methods from other classes, allowing for more complex relationships and structures.  This is known as inheritance and is a key feature of OOP. ",1
1004,"Client-server computing is a model of distributed computing where tasks are divided between clients and servers.  In this model, a client is a computer or device that requests data or services from a server, which is a computer or device that provides the data or services requested by the client. The client-server model is widely used in computer networks and the internet to provide a wide range of services, such as email, web browsing, file sharing, and database access.  The model allows multiple clients to access a centralized resource or service, which can be more efficient and cost-effective than having each client perform the same task independently. In a client-server architecture, the client typically sends a request to the server, which processes the request and returns the requested data or service to the client.  The server may also be responsible for managing data storage, security, and other aspects of the service being provided. ",1
1005,"Client-server computing offers several advantages over other computing models, including:Centralized control: Client-server architecture allows for a centralized management of data and resources, which can help to improve security and consistency of the data. Scalability: The client-server model is scalable, meaning that additional clients can be added to the system without the need for significant changes to the underlying infrastructure. Resource sharing: The server can act as a centralized resource pool, which allows clients to share resources such as storage, processing power, and memory. Improved performance: Client-server architecture can improve performance by allowing the server to handle complex and resource-intensive tasks, freeing up client resources for other tasks. Improved maintenance and support: Client-server architecture simplifies maintenance and support, as software and data can be centrally managed, updated, and backed up. Platform independence: The client-server model enables platform independence, which means that clients can access the server regardless of the operating system or hardware platform they are running on. ",1
1006,"Clock speed, also known as clock rate, refers to the frequency at which a computer's central processing unit (CPU) can execute instructions.  It is measured in Hertz (Hz), and typically expressed in Gigahertz (GHz) for modern CPUs. Clock speed represents the number of clock cycles per second that the CPU can execute.  Each clock cycle represents a single unit of work that the CPU can perform.  A higher clock speed means that the CPU can perform more work in a given amount of time, which can result in improved performance. However, clock speed is not the only factor that determines CPU performance.  Other factors, such as the number of cores, cache size, and instruction set, also play an important role in determining overall performance. ",1
1007,"COBOL (Common Business Oriented Language) is a high-level programming language designed for business applications.  It was first introduced in 1959 and is still widely used today for processing large amounts of data in applications such as banking, insurance, and government. COBOL was designed to be easily readable and understandable by business people, as well as programmers.  It features a rich set of data processing capabilities, including support for complex record structures and file processing. COBOL programs are typically compiled rather than interpreted, and they are often run on mainframe computers.  However, the language has been adapted to run on modern platforms, such as Windows and Unix. Despite its age, COBOL remains an important language in the business world due to its reliability and ability to handle large volumes of data.  It is estimated that billions of lines of COBOL code are still in use today. ",1
1008,"A codec (short for coder-decoder) is a technology used to compress and decompress digital media such as audio and video.  The purpose of a codec is to reduce the file size of digital media without significantly degrading its quality. There are two types of codecs: lossy and lossless.  Lossy codecs are designed to reduce file size by discarding some of the original data, resulting in a smaller file but a loss of quality.  Lossless codecs, on the other hand, compress the data without discarding any of it, resulting in a smaller file but no loss of quality. Codecs are used in a variety of applications, such as streaming media over the internet, video conferencing, and recording and playing back digital media.  Some examples of popular codecs include H. 264 for video and MP3 for audio. In order for digital media to be played back or edited, the appropriate codec must be installed on the computer or device.  Codecs can be proprietary, meaning they are owned by a particular company and require licensing, or open source, meaning they are freely available for use by anyone. ",1
1009,"Cognitive science is an interdisciplinary field of study that examines the nature of human thought, including perception, attention, memory, language, problem-solving, and decision-making.  It combines research from psychology, neuroscience, linguistics, philosophy, computer science, and anthropology to understand how the human mind processes information and how we interact with the world around us. Cognitive science seeks to explain how mental processes work at different levels, from the neural level to the behavioral level.  It investigates questions such as: How do we perceive and interpret sensory information? How do we acquire and use knowledge? How do we reason and make decisions? How do we use language to communicate?Cognitive science research often involves experiments and modeling, using techniques from various fields, such as brain imaging, computational modeling, and cognitive psychology.  It has important applications in areas such as education, artificial intelligence, and human-computer interaction. ",1
1010,"In computing, colors are typically represented using combinations of red, green, and blue (RGB) values.  Each pixel on a computer screen is made up of three small subpixels, one red, one green, and one blue, and the intensity of each subpixel determines the overall color of the pixel. The RGB color model is based on the additive color theory, which states that by adding different intensities of red, green, and blue light together, all colors can be created.  The values for each color component are typically represented as numbers ranging from 0 to 255, with 0 indicating no intensity and 255 indicating full intensity. In addition to the RGB color model, there are other color models used in computing, such as CMYK (Cyan, Magenta, Yellow, Key/Black) used in printing, and the HSL (Hue, Saturation, Lightness) and HSV (Hue, Saturation, Value) color models used in image processing and graphic design. Color is an important aspect of user interfaces and web design, as it can affect the mood and tone of a website or application.  Different colors are associated with different emotions and can be used to create contrast, highlight important information, and create a sense of hierarchy and organization. ",1
1011,"RGB stands for Red, Green, and Blue, which are the primary colors of light.  In the RGB color model, colors are created by combining different intensities of red, green, and blue light.  The model is based on the additive color theory, which states that by adding different intensities of light together, all colors can be created. In the RGB color model, each color component is typically represented as an integer value ranging from 0 to 255, where 0 indicates no intensity and 255 indicates maximum intensity.  By combining different values of red, green, and blue, a total of 16. 7 million colors can be created. ",1
1012,"CMYK stands for Cyan, Magenta, Yellow, and Key (Black), which are the primary colors used in printing.  In the CMYK color model, colors are created by combining different intensities of cyan, magenta, yellow, and black ink. The CMYK color model is based on the subtractive color theory, which states that by subtracting different colors from white, all colors can be created.  When all four colors are combined at their full intensity, they absorb all the light and produce black. In the CMYK color model, each color component is typically represented as a percentage value ranging from 0% to 100%, where 0% indicates no ink and 100% indicates full ink coverage.  By combining different percentages of cyan, magenta, yellow, and black, a wide range of colors can be created. ",1
1013,"Palettes can refer to different things depending on the context, but generally speaking, a palette is a collection or range of colors that are used together in a particular context or design. In art, a palette typically refers to the flat surface that a painter uses to mix and arrange colors before applying them to a canvas.  The term ""palette"" is also used to describe the range of colors that an artist uses in a particular work or style. In design and computer graphics, a palette can refer to a set of colors that are pre-defined for a specific project or application.  Designers and developers use palettes to ensure that the colors used in a project are consistent and visually appealing.  Palettes can be created manually or generated automatically by software tools. ",1
1014,"Probability is a measure of the likelihood that a specific event will occur.  It is a mathematical concept that is used to describe the uncertainty or randomness of an event.  Probability is expressed as a number between 0 and 1, where 0 represents an impossible event and 1 represents a certain event. The probability of an event occurring can be calculated by dividing the number of favorable outcomes by the total number of possible outcomes.  For example, if a coin is flipped, the probability of getting heads is 1/2 or 0. 5, since there are two possible outcomes (heads or tails) and one of them is favorable. Probability theory is used in a wide range of fields, including statistics, economics, physics, engineering, and computer science.  It is used to model and analyze complex systems that involve randomness or uncertainty, such as weather patterns, stock prices, and game theory. ",1
1015,"A compiler is a software program that translates source code written in a programming language into machine code, which is a language that computers can understand and execute.  The process of converting source code into machine code is called compilation. The compiler takes the source code, analyzes its syntax and structure, and translates it into an equivalent program in machine code.  The machine code is then executed by the computer's processor, performing the operations specified in the source code. Compilers are essential tools for software development, as they allow programmers to write code in high-level programming languages and then translate that code into machine code that can be executed on a variety of computer systems.  This enables developers to write code that is more abstract and expressive, without having to worry about the specifics of the underlying hardware. Compilers are also used to optimize code for performance, by generating machine code that is more efficient and faster than code written by humans.  Additionally, compilers can detect errors and bugs in the source code during the compilation process, helping to identify and fix issues before the program is run. There are many different compilers available for different programming languages, and some languages even have multiple compilers that can be used depending on the specific needs of the programmer. ",1
1016,"The compilation process is the process by which a compiler translates source code written in a programming language into machine code that can be executed by a computer.  The process typically involves several stages:Lexical Analysis: The compiler reads the source code and identifies the tokens (keywords, identifiers, operators, etc. ) in the code. Syntax Analysis: The compiler analyzes the structure of the code to ensure that it conforms to the syntax rules of the programming language.  This stage involves creating an abstract syntax tree (AST) that represents the structure of the code. Semantic Analysis: The compiler performs checks to ensure that the code is semantically correct, such as checking that variables are declared before they are used, and that functions are called with the correct number and type of arguments. Code Generation: The compiler generates machine code that can be executed by the computer.  This involves translating the AST into a lower-level representation of the code, such as assembly language or machine code. Optimization: The compiler may perform various optimizations on the generated code to improve its performance or reduce its size.  This may involve removing redundant code, reordering instructions, or replacing code with more efficient alternatives. Once the compilation process is complete, the resulting machine code can be executed by the computer's processor.  The process of compilation is typically performed by a dedicated software program called a compiler, which is specific to the programming language being used. ",1
1017,"Code generation is the process of converting the high-level source code written in a programming language into low-level machine code that can be executed by a computer.  The code generation process is typically the final stage of the compilation process, following lexical analysis, syntax analysis, and semantic analysis. The process of code generation can be divided into several steps:Intermediate Representation: The compiler typically converts the source code into an intermediate representation (IR) that is easier to analyze and optimize than the original source code. Instruction Selection: The compiler selects the appropriate machine instructions to implement each operation in the IR.  This involves mapping high-level operations to the corresponding low-level instructions, such as adding two numbers or loading data from memory. Register Allocation: The compiler assigns registers to hold intermediate values and computations.  This involves minimizing the number of register accesses and maximizing the reuse of registers to improve performance. Instruction Scheduling: The compiler reorders the instructions to optimize performance, taking into account factors such as data dependencies and instruction latencies. Code Emission: The compiler generates the final machine code by emitting the selected instructions and associated data. Optimization: The compiler may perform additional optimizations on the generated code to improve its performance or reduce its size.  This may involve removing redundant code, reordering instructions, or replacing code with more efficient alternatives. ",1
